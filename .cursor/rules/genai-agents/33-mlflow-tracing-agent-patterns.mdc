---
description: MLflow Tracing, ResponsesAgent, Streaming, Multi-Agent Orchestration, and OBO Authentication patterns
globs: src/agents/**/*.py
alwaysApply: false
---
# MLflow 3.0 Tracing & Agent Implementation Patterns

## Pattern Recognition
Comprehensive patterns for production agent implementation covering tracing, ResponsesAgent interface, streaming responses, multi-agent orchestration with Genie, on-behalf-of authentication, and visualization hints. Based on production implementation of Databricks Health Monitor Agent.

---

## ðŸ”´ CRITICAL: ResponsesAgent is MANDATORY for AI Playground

**Without `ResponsesAgent`, your agent will NOT work in AI Playground, Agent Evaluation, or Mosaic AI features.**

Reference: [Microsoft Docs - Model Signatures](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/create-agent#understand-model-signatures-to-ensure-compatibility-with-azure-databricks-features)

---

## Pattern 1: ResponsesAgent Implementation

### Why ResponsesAgent?

| Feature | ResponsesAgent | Legacy ChatAgent/PythonModel |
|---|---|---|
| Automatic signature inference | âœ… Yes | âŒ Manual required |
| AI Playground compatible | âœ… Yes | âš ï¸ Depends on signature |
| Streaming support | âœ… Built-in | âŒ Manual |
| Tool calling | âœ… Built-in | âŒ Manual |
| Multi-agent support | âœ… Built-in | âŒ Manual |

### ResponsesAgent Template

```python
import mlflow
import uuid
from mlflow.pyfunc import ResponsesAgent
from mlflow.types.responses import (
    ResponsesAgentRequest,
    ResponsesAgentResponse,
    ResponsesAgentStreamEvent,
    ResponsesAgentStreamEventDelta,
    ResponsesAgentMessageContentDelta
)

class HealthMonitorAgent(ResponsesAgent):
    """
    Production agent implementing ResponsesAgent with streaming.
    
    Reference: https://mlflow.org/docs/latest/genai/serving/responses-agent
    """
    
    def __init__(self):
        super().__init__()
        # Initialize components
        self.llm_endpoint = "databricks-claude-sonnet-4-5"
        self._graph = None  # Lazy-loaded LangGraph
    
    @mlflow.trace(name="health_monitor_agent", span_type="AGENT")
    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:
        """
        Non-streaming prediction.
        
        Input format:  {"input": [{"role": "user", "content": "..."}]}
        Output format: ResponsesAgentResponse
        """
        # Extract query from request.input (NOT messages!)
        input_messages = [msg.model_dump() for msg in request.input]
        query = input_messages[-1].get("content", "")
        
        # Extract custom inputs
        custom_inputs = request.custom_inputs or {}
        user_id = custom_inputs.get("user_id", "unknown")
        
        # Process query
        response_text = self._process(query, custom_inputs)
        
        # Return ResponsesAgentResponse
        return ResponsesAgentResponse(
            output=[self.create_text_output_item(
                text=response_text,
                id=str(uuid.uuid4())
            )],
            custom_outputs={
                "source": "agent",
                "domains_queried": ["cost", "reliability"],
                "user_id": user_id
            }
        )
    
    def _process(self, query: str, custom_inputs: dict) -> str:
        """Core processing logic."""
        # Your agent logic here
        return f"Processed: {query}"


# ============================================================================
# LOGGING (NO Manual Signature!)
# ============================================================================

def log_agent():
    agent = HealthMonitorAgent()
    
    # CRITICAL: Set model for MLflow
    mlflow.models.set_model(agent)
    
    # Input example (ResponsesAgent format - uses 'input', not 'messages')
    input_example = {
        "input": [{"role": "user", "content": "What is the status?"}],
        "custom_inputs": {"user_id": "test_user"}
    }
    
    with mlflow.start_run(run_name="log_agent"):
        # ================================================================
        # CRITICAL: DO NOT pass signature parameter!
        # ResponsesAgent automatically infers the correct signature.
        # Manual signatures WILL BREAK AI Playground compatibility.
        # ================================================================
        logged_model = mlflow.pyfunc.log_model(
            artifact_path="agent",
            python_model=agent,
            input_example=input_example,
            # signature=...  # âŒ NEVER include this!
            registered_model_name="health_monitor_agent",
            pip_requirements=[
                "mlflow>=3.0.0",
                "databricks-sdk>=0.28.0",
                "langchain>=0.3.0",
                "langgraph>=0.2.0",
            ],
        )
        
        print(f"âœ“ Agent logged: {logged_model.model_uri}")
    
    return logged_model
```

---

## Pattern 2: Streaming Responses

### Streaming Implementation

```python
from typing import Generator

class HealthMonitorAgent(ResponsesAgent):
    """Agent with streaming support."""
    
    def predict_stream(
        self,
        request: ResponsesAgentRequest
    ) -> Generator[ResponsesAgentStreamEvent, None, None]:
        """
        Stream ResponsesAgent events (delta pattern).
        
        Yields ResponsesAgentStreamEvent objects:
        - type="output_item.delta" for text chunks
        - type="output_item.done" for completion
        """
        input_messages = [msg.model_dump() for msg in request.input]
        query = input_messages[-1].get("content", "")
        
        # Generate unique item ID
        item_id = str(uuid.uuid4())
        
        # Stream text chunks
        for chunk in self._process_streaming(query):
            yield ResponsesAgentStreamEvent(
                type="output_item.delta",
                delta=ResponsesAgentStreamEventDelta(
                    type="message_delta",
                    delta=ResponsesAgentMessageContentDelta(
                        type="text",
                        text=chunk
                    )
                ),
                item_id=item_id
            )
        
        # Final done event (REQUIRED)
        yield ResponsesAgentStreamEvent(
            type="output_item.done",
            item_id=item_id
        )
    
    def _process_streaming(self, query: str) -> Generator[str, None, None]:
        """Generate response chunks."""
        # Example: Stream from LLM
        for chunk in self.llm.stream(query):
            if hasattr(chunk, 'content') and chunk.content:
                yield chunk.content
    
    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:
        """
        Non-streaming prediction (delegates to streaming).
        
        Code reuse pattern: predict() collects all chunks from predict_stream().
        """
        chunks = []
        item_id = None
        
        for event in self.predict_stream(request):
            if event.type == "output_item.delta":
                if event.delta and event.delta.delta:
                    chunks.append(event.delta.delta.text)
            elif event.type == "output_item.done":
                item_id = event.item_id
        
        full_text = "".join(chunks)
        
        return ResponsesAgentResponse(
            output=[self.create_text_output_item(
                text=full_text,
                id=item_id or str(uuid.uuid4())
            )]
        )
```

---

## Pattern 3: MLflow Tracing

### Automatic Tracing with Decorators

```python
import mlflow

@mlflow.trace(name="orchestrator_node", span_type="AGENT")
def orchestrator_node(state: dict) -> dict:
    """
    LangGraph node with automatic tracing.
    
    All inputs/outputs automatically logged to trace.
    """
    query = state["messages"][-1].content
    
    # Classify intent
    domains = classify_intent(query)
    
    # Route to domain workers
    results = route_to_workers(query, domains)
    
    return {"messages": state["messages"] + [results]}


@mlflow.trace(name="genie_query", span_type="TOOL")
def query_genie(domain: str, query: str) -> str:
    """
    Tool call with automatic tracing.
    
    Span type TOOL indicates external service call.
    """
    genie = get_genie_space(domain)
    
    try:
        result = genie.invoke(query)
        return result
    except Exception as e:
        # Log error to trace
        mlflow.update_current_trace(
            attributes={"error": str(e), "domain": domain}
        )
        raise
```

### Manual Span Creation (Fine-Grained Control)

```python
def complex_processing(query: str) -> dict:
    """Complex operation with nested spans."""
    
    with mlflow.start_span(name="complex_processing", span_type="AGENT") as span:
        span.set_inputs({"query": query})
        
        # Step 1: Intent classification
        with mlflow.start_span(name="classify_intent", span_type="CLASSIFIER") as intent_span:
            intent_span.set_inputs({"query": query})
            domains = classify(query)
            intent_span.set_outputs({"domains": domains})
            intent_span.set_attributes({"confidence": 0.95})
        
        # Step 2: Query Genie
        results = {}
        for domain in domains:
            with mlflow.start_span(name=f"genie_{domain}", span_type="TOOL") as genie_span:
                genie_span.set_inputs({"domain": domain, "query": query})
                result = query_genie(domain, query)
                genie_span.set_outputs({"result": result})
                results[domain] = result
        
        # Step 3: Synthesize
        with mlflow.start_span(name="synthesize", span_type="AGENT") as synth_span:
            synth_span.set_inputs({"results": results})
            final_response = synthesize(results)
            synth_span.set_outputs({"response": final_response})
        
        span.set_outputs({"response": final_response})
        span.set_attributes({"num_domains": len(domains)})
    
    return final_response
```

### Trace Storage Configuration

**File:** `src/agents/setup/configure_trace_storage.py`

```python
# Databricks notebook source
"""
Configure Unity Catalog trace storage for agent experiment.

Reference: https://docs.databricks.com/aws/en/mlflow3/genai/tracing/storage
"""

import mlflow
from databricks.sdk import WorkspaceClient

# Parameters
catalog = dbutils.widgets.get("catalog")
agent_schema = dbutils.widgets.get("agent_schema")

EXPERIMENT_PATH = "/Shared/health_monitor_agent_evaluation"
TRACE_TABLE_NAME = f"{catalog}.{agent_schema}.agent_traces"

# Set experiment
mlflow.set_experiment(EXPERIMENT_PATH)

# Get experiment ID
client = mlflow.MlflowClient()
experiment = client.get_experiment_by_name(EXPERIMENT_PATH)
experiment_id = experiment.experiment_id

# Configure trace storage
w = WorkspaceClient()

w.experiments.set_experiment_tag(
    experiment_id=experiment_id,
    key="mlflow.tracing.destination.catalog",
    value=catalog
)

w.experiments.set_experiment_tag(
    experiment_id=experiment_id,
    key="mlflow.tracing.destination.schema",
    value=agent_schema
)

w.experiments.set_experiment_tag(
    experiment_id=experiment_id,
    key="mlflow.tracing.destination.table",
    value="agent_traces"
)

print(f"âœ“ Trace storage configured: {TRACE_TABLE_NAME}")
```

---

## Pattern 4: Multi-Agent Orchestration with Genie

### Multi-Domain Architecture

```
User Query
    â†“
Orchestrator (Intent Classification)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cost   â”‚Security â”‚Perf.    â”‚Reliab.  â”‚Quality  â”‚ â† Domain Workers
â”‚  Agent  â”‚ Agent   â”‚Agent    â”‚Agent    â”‚Agent    â”‚   (parallel)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚         â”‚         â”‚         â”‚         â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
                  Synthesizer (Response Composition)
                        â†“
                  Final Response
```

### LangGraph Implementation

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, List

class AgentState(TypedDict):
    """State shared across nodes."""
    messages: List[dict]
    domains: List[str]
    domain_results: dict
    final_response: str


def build_multi_agent_graph() -> StateGraph:
    """Build LangGraph workflow for multi-agent orchestration."""
    
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("orchestrator", orchestrator_node)
    workflow.add_node("cost_worker", lambda s: cost_worker(s))
    workflow.add_node("security_worker", lambda s: security_worker(s))
    workflow.add_node("performance_worker", lambda s: performance_worker(s))
    workflow.add_node("reliability_worker", lambda s: reliability_worker(s))
    workflow.add_node("quality_worker", lambda s: quality_worker(s))
    workflow.add_node("synthesizer", synthesizer_node)
    
    # Entry point
    workflow.set_entry_point("orchestrator")
    
    # Conditional routing based on domains
    workflow.add_conditional_edges(
        "orchestrator",
        route_to_workers,  # Returns list of worker nodes to execute
        {
            "cost": "cost_worker",
            "security": "security_worker",
            "performance": "performance_worker",
            "reliability": "reliability_worker",
            "quality": "quality_worker",
        }
    )
    
    # All workers converge to synthesizer
    for worker in ["cost_worker", "security_worker", "performance_worker", 
                   "reliability_worker", "quality_worker"]:
        workflow.add_edge(worker, "synthesizer")
    
    # Synthesizer to end
    workflow.add_edge("synthesizer", END)
    
    return workflow


@mlflow.trace(name="orchestrator_node", span_type="AGENT")
def orchestrator_node(state: AgentState) -> AgentState:
    """Route query to relevant domain workers."""
    query = state["messages"][-1]["content"]
    
    # Classify intent
    domains = classify_intent(query)
    
    state["domains"] = domains
    state["domain_results"] = {}
    
    return state


def route_to_workers(state: AgentState) -> List[str]:
    """Conditional routing logic."""
    return [f"{domain}_worker" for domain in state["domains"]]


@mlflow.trace(name="cost_worker", span_type="AGENT")
def cost_worker(state: AgentState) -> AgentState:
    """Cost domain specialist."""
    query = state["messages"][-1]["content"]
    
    # Query Cost Genie
    genie_result = query_genie("cost", query)
    
    state["domain_results"]["cost"] = genie_result
    return state


@mlflow.trace(name="synthesizer_node", span_type="AGENT")
def synthesizer_node(state: AgentState) -> AgentState:
    """Synthesize multi-domain results."""
    query = state["messages"][-1]["content"]
    domain_results = state["domain_results"]
    
    # Synthesize response
    synthesized = synthesize_response(query, domain_results)
    
    state["final_response"] = synthesized
    state["messages"].append({"role": "assistant", "content": synthesized})
    
    return state
```

### Genie Tool Integration

```python
from databricks.agents import GenieAgent

def query_genie(domain: str, query: str) -> str:
    """
    Query Genie Space for real data.
    
    NO LLM FALLBACK - return explicit error if Genie fails.
    """
    with mlflow.start_span(name=f"genie_{domain}", span_type="TOOL") as span:
        span.set_inputs({"domain": domain, "query": query})
        
        try:
            # Get Genie Space ID
            genie_space_id = GENIE_SPACES.get(domain)
            if not genie_space_id:
                error_msg = f"Genie Space not configured for domain: {domain}"
                span.set_attributes({"error": error_msg})
                return f"## Error\n\n{error_msg}"
            
            # Query Genie
            genie = GenieAgent(space_id=genie_space_id)
            result = genie.invoke(query)
            
            span.set_outputs({"result": result})
            span.set_attributes({"source": "genie", "space_id": genie_space_id})
            
            return result
            
        except Exception as e:
            # ================================================================
            # CRITICAL: NO LLM FALLBACK!
            # Return explicit error instead of hallucinating fake data.
            # ================================================================
            error_msg = f"""## Genie Query Failed

**Domain:** {domain}
**Query:** {query}
**Error:** {str(e)}

I was unable to retrieve real data from the Databricks Genie Space.

**Note:** I will NOT generate fake data. All responses must come from real system tables via Genie."""
            
            span.set_attributes({"error": str(e), "fallback": "none"})
            span.set_outputs({"source": "error"})
            
            return error_msg
```

---

## Pattern 4: Adding Context to Traces (Tags & Metadata)

### Tags vs Metadata

**Reference:** [Microsoft Docs - Attach Custom Tags and Metadata](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/tracing/attach-tags/)

| Property | Mutability | Use For | Example |
|---|---|---|---|
| **Metadata** | Immutable (write-once) | Fixed information captured during execution | User ID, session ID, model version, environment |
| **Tags** | Mutable (can update) | Dynamic information that may change | User feedback, review status, quality assessments |

### Standard Metadata Fields

**Reference:** [Microsoft Docs - Add Context to Traces](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/tracing/add-context-to-traces)

MLflow provides standardized metadata fields that enable automatic filtering and grouping in the UI:

```python
import mlflow

mlflow.update_current_trace(
    metadata={
        # ========== USER & SESSION TRACKING ==========
        "mlflow.trace.user": user_id,           # Associate with specific user
        "mlflow.trace.session": session_id,     # Group multi-turn conversations
        
        # ========== DEPLOYMENT CONTEXT ==========
        "mlflow.source.type": "PRODUCTION",     # Environment (PRODUCTION, STAGING, DEV)
        "mlflow.source.name": "health-monitor-agent",  # Application name
        "mlflow.modelId": "v2.1.0",             # Application version
        
        # ========== GIT CONTEXT (auto-populated if in repo) ==========
        "mlflow.source.git.commit": commit_hash,   # Git commit hash
        "mlflow.source.git.branch": branch_name,   # Git branch
        "mlflow.source.git.repoURL": repo_url,     # Git repository URL
        
        # ========== REQUEST TRACKING ==========
        "client_request_id": request_id,        # Link to client request
        
        # ========== CUSTOM METADATA ==========
        "deployment_region": "us-west-2",       # Custom metadata
        "feature_flags": "new_routing,caching", # Application-specific
    }
)
```

### Tagging Pattern (Mutable Context)

```python
mlflow.update_current_trace(
    tags={
        # ========== QUERY CLASSIFICATION ==========
        "query_category": "cost_analysis",
        "domains": "cost,reliability",
        "confidence": "0.95",
        
        # ========== EXECUTION DETAILS ==========
        "streaming": "true",
        "genie_used": "true",
        "cache_hit": "false",
        
        # ========== QUALITY ASSESSMENT ==========
        "review_status": "approved",
        "data_quality": "high",
        
        # ========== BUSINESS CONTEXT ==========
        "customer_tier": "enterprise",
        "use_case": "cost_optimization",
    }
)
```

### Complete Context Pattern (Production Agent)

```python
@mlflow.trace(name="health_monitor_predict", span_type="AGENT")
def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:
    """Predict with comprehensive trace context."""
    
    # ========== EXTRACT CONTEXT FROM REQUEST ==========
    
    input_messages = [msg.model_dump() for msg in request.input]
    query = input_messages[-1].get("content", "")
    custom_inputs = request.custom_inputs or {}
    
    # User context
    user_id = custom_inputs.get("user_id", "unknown")
    session_id = custom_inputs.get("session_id", str(uuid.uuid4()))
    thread_id = custom_inputs.get("thread_id", str(uuid.uuid4()))
    
    # Request context
    client_request_id = custom_inputs.get("request_id", str(uuid.uuid4()))
    
    # Deployment context
    app_environment = os.environ.get("APP_ENVIRONMENT", "development")
    app_version = os.environ.get("APP_VERSION", "dev")
    deployment_region = os.environ.get("DEPLOYMENT_REGION", "unknown")
    endpoint_name = os.environ.get("ENDPOINT_NAME", "health-monitor-agent")
    
    # ========== UPDATE TRACE WITH METADATA (Immutable) ==========
    
    mlflow.update_current_trace(
        metadata={
            # User & Session (standard fields for UI filtering)
            "mlflow.trace.user": user_id,
            "mlflow.trace.session": session_id,
            
            # Deployment context
            "mlflow.source.type": app_environment.upper(),  # PRODUCTION, STAGING, DEV
            "mlflow.modelId": app_version,
            "client_request_id": client_request_id,
            
            # Custom application metadata
            "deployment_region": deployment_region,
            "endpoint_name": endpoint_name,
            "query_length": str(len(query)),
            "has_custom_inputs": str(bool(custom_inputs)),
        },
        tags={
            # Execution details (mutable - can update later)
            "user_id": user_id,         # Also in metadata, but tags are filterable
            "thread_id": thread_id,     # For conversation tracking
            "session_id": session_id,   # For multi-turn analysis
            "query_length": str(len(query)),
            "streaming": "pending",     # Can update to "true" later
        }
    )
    
    # ========== PROCESS QUERY ==========
    
    response_text, domains = self._process(query, custom_inputs)
    
    # ========== UPDATE TRACE WITH EXECUTION RESULTS (Tags) ==========
    
    mlflow.update_current_trace(tags={
        "domains_queried": ",".join(domains),
        "streaming": "false",  # Updated from "pending"
        "response_generated": "true",
    })
    
    # Return response
    return ResponsesAgentResponse(
        output=[self.create_text_output_item(text=response_text, id=str(uuid.uuid4()))],
        custom_outputs={
            "thread_id": thread_id,         # Return for client tracking
            "session_id": session_id,
            "domains_queried": domains,
        }
    )
```

### Safe Trace Update Helper

**Problem:** `mlflow.update_current_trace()` raises warnings during evaluation when no active trace exists.

**Solution:** Check for active trace before updating.

```python
def _safe_update_trace(metadata: dict = None, tags: dict = None) -> bool:
    """
    Safely update the current MLflow trace without triggering warnings.
    
    During evaluation testing, there may not be an active trace context.
    This helper checks for an active trace before attempting updates.
    
    Returns:
        True if update succeeded, False if no active trace.
    """
    try:
        # Check if there's an active trace
        current_span = mlflow.get_current_active_span()
        if current_span is None:
            return False
        
        # Safe to update
        mlflow.update_current_trace(metadata=metadata, tags=tags)
        return True
    except Exception:
        return False


# Usage in agent
_safe_update_trace(
    metadata={"mlflow.trace.user": user_id},
    tags={"query_category": "cost"}
)
```

### Updating Tags on Finished Traces

**After trace is logged, only tags can be updated (not metadata).**

**Reference:** [Microsoft Docs - Setting Tags on Finished Traces](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/tracing/attach-tags/#setting-tags-on-a-finished-trace)

```python
import mlflow

# Execute traced function
@mlflow.trace
def process_query(query):
    return f"Processed: {query}"

result = process_query("Why did costs spike?")

# ========== GET TRACE ID ==========
trace_id = mlflow.get_last_active_trace_id()

# ========== SET/UPDATE TAGS ON FINISHED TRACE ==========

# Add review status after human review
mlflow.set_trace_tag(
    trace_id=trace_id,
    key="review_status",
    value="approved"
)

# Add quality assessment after evaluation
mlflow.set_trace_tag(
    trace_id=trace_id,
    key="quality_score",
    value="0.85"
)

# ========== DELETE TAG ==========

mlflow.delete_trace_tag(
    trace_id=trace_id,
    key="quality_score"  # Remove tag
)
```

### Context Tracking for Multi-Turn Conversations

```python
class ConversationAgent(ResponsesAgent):
    """Agent with session and conversation tracking."""
    
    @mlflow.trace(name="predict", span_type="AGENT")
    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:
        """Predict with conversation context."""
        
        # Extract conversation identifiers
        custom_inputs = request.custom_inputs or {}
        user_id = custom_inputs.get("user_id", "anonymous")
        session_id = custom_inputs.get("session_id")
        thread_id = custom_inputs.get("thread_id")
        
        # Generate IDs if not provided
        if not session_id:
            session_id = str(uuid.uuid4())
        if not thread_id:
            thread_id = str(uuid.uuid4())
        
        # âœ… CRITICAL: Use standard metadata fields for UI filtering
        mlflow.update_current_trace(
            metadata={
                "mlflow.trace.user": user_id,       # Standard user field
                "mlflow.trace.session": session_id, # Standard session field
                "turn_number": str(custom_inputs.get("turn_number", 1)),
            },
            tags={
                "thread_id": thread_id,
                "conversation_type": "multi_turn",
                "user_tier": custom_inputs.get("user_tier", "standard"),
            }
        )
        
        # Load conversation history using thread_id
        history = self._get_conversation_history(thread_id)
        
        # Process with context
        response = self._process_with_history(query, history)
        
        # Return with IDs for client tracking
        return ResponsesAgentResponse(
            output=[self.create_text_output_item(text=response, id=str(uuid.uuid4()))],
            custom_outputs={
                "session_id": session_id,
                "thread_id": thread_id,
                "turn_number": len(history) + 1,
            }
        )
```

### Environment-Specific Context

```python
import os

def add_deployment_context():
    """Add deployment environment context to trace."""
    
    # Extract from environment variables (not hardcoded)
    mlflow.update_current_trace(
        metadata={
            # Override automatic detection
            "mlflow.source.type": os.getenv("APP_ENVIRONMENT", "DEVELOPMENT"),
            "mlflow.source.name": os.getenv("APP_NAME", "health-monitor-agent"),
            "mlflow.modelId": os.getenv("APP_VERSION", "dev"),
            
            # Custom deployment metadata
            "deployment_id": os.getenv("DEPLOYMENT_ID", "unknown"),
            "deployment_region": os.getenv("AWS_REGION", "us-west-2"),
            "k8s_pod_name": os.getenv("HOSTNAME", "unknown"),
        },
        tags={
            # Feature flags (can change without redeployment)
            "feature_new_routing": os.getenv("FEATURE_NEW_ROUTING", "false"),
            "feature_caching": os.getenv("FEATURE_CACHING", "true"),
        }
    )
```

### Query Pattern for Trace Context

```python
import mlflow

# ========== SEARCH TRACES BY USER ==========

user_traces = mlflow.search_traces(
    filter_string="metadata.mlflow.trace.user = 'user@example.com'",
    max_results=100
)

# ========== SEARCH TRACES BY SESSION ==========

session_traces = mlflow.search_traces(
    filter_string="metadata.mlflow.trace.session = 'session_123'",
    order_by=["timestamp ASC"]  # Chronological order
)

# ========== SEARCH TRACES BY ENVIRONMENT ==========

prod_traces = mlflow.search_traces(
    filter_string="metadata.mlflow.source.type = 'PRODUCTION'",
    max_results=1000
)

# ========== SEARCH BY TAG ==========

reviewed_traces = mlflow.search_traces(
    filter_string="tags.review_status = 'approved'",
    max_results=50
)

# ========== COMPLEX FILTER ==========

filtered_traces = mlflow.search_traces(
    filter_string="""
        metadata.mlflow.trace.user = 'user@example.com'
        AND tags.domains LIKE '%cost%'
        AND metadata.mlflow.source.type = 'PRODUCTION'
    """,
    max_results=100
)
```

---

## Pattern 5: On-Behalf-Of (OBO) Authentication

**OBO enables agents to query data on behalf of the calling user, respecting per-user permissions.**

âš ï¸ **CRITICAL:** OBO **only works in Model Serving**. Attempting OBO in notebooks/jobs/evaluation produces invalid credentials and permission errors.

### Context Detection (MANDATORY)

**Always detect execution context before attempting OBO:**

```python
def _get_authenticated_client(self):
    """
    Get WorkspaceClient with appropriate auth strategy.
    
    CRITICAL: OBO only works in Model Serving context.
    Outside Model Serving, use default auth (user credentials or service principal).
    
    Production Learning: Jan 27, 2026
    Problem: Agent evaluation failed with "You need 'Can View' permission" errors
    Root Cause: OBO attempted outside Model Serving â†’ invalid credentials
    Solution: Detect environment before attempting OBO
    """
    from databricks.sdk import WorkspaceClient
    import os
    
    # ================================================================
    # STEP 1: Detect if we're running in Model Serving environment
    # ================================================================
    # Model Serving sets specific environment variables:
    # - IS_IN_DB_MODEL_SERVING_ENV=true (Databricks Model Serving)
    # - DATABRICKS_SERVING_ENDPOINT (endpoint name)
    # - MLFLOW_DEPLOYMENT_FLAVOR_NAME=databricks (MLflow deployment)
    # ================================================================
    is_model_serving = (
        os.environ.get("IS_IN_DB_MODEL_SERVING_ENV") == "true" or
        os.environ.get("DATABRICKS_SERVING_ENDPOINT") is not None or
        os.environ.get("MLFLOW_DEPLOYMENT_FLAVOR_NAME") == "databricks"
    )
    
    # ================================================================
    # STEP 2: Use OBO only in Model Serving, default auth otherwise
    # ================================================================
    if is_model_serving:
        # We're in Model Serving - attempt OBO authentication
        try:
            from databricks_ai_bridge import ModelServingUserCredentials
            client = WorkspaceClient(credentials_strategy=ModelServingUserCredentials())
            print(f"âœ“ Using on-behalf-of-user auth (Model Serving)")
            return client
        except ImportError:
            # databricks-ai-bridge not installed
            print(f"âš  databricks-ai-bridge not available")
            print(f"  Falling back to default auth (may not respect user permissions)")
            return WorkspaceClient()
        except Exception as auth_e:
            # OBO setup failed
            print(f"âš  OBO auth failed: {type(auth_e).__name__}: {auth_e}")
            print(f"  Falling back to default auth")
            return WorkspaceClient()
    else:
        # Not in Model Serving - use default auth (notebooks, jobs, evaluation)
        # This uses the current user's credentials or service principal
        print(f"â†’ Using default workspace auth (evaluation/notebook mode)")
        return WorkspaceClient()
```

### Why Context Detection Is Critical

| Environment | OBO Attempted | Result |
|-------------|---------------|--------|
| **Notebooks** | âŒ Without detection | Permission errors (invalid credentials) |
| **Notebooks** | âœ… With detection | Works (uses your credentials) |
| **Jobs** | âŒ Without detection | Permission errors (invalid credentials) |
| **Jobs** | âœ… With detection | Works (uses job owner/SP credentials) |
| **Evaluation** | âŒ Without detection | Permission errors (invalid credentials) |
| **Evaluation** | âœ… With detection | Works (uses runner credentials) |
| **Model Serving** | âœ… With OBO | Works (uses end-user credentials) |

### Model Serving Configuration

**Enable OBO in serving endpoint YAML:**

```yaml
# File: resources/agents/agent_serving_endpoint.yml

resources:
  model_serving_endpoints:
    health_monitor_agent:
      name: health-monitor-agent
      config:
        served_entities:
          - name: current
            entity_name: ${var.catalog}.${var.agent_schema}.health_monitor_agent
            entity_version: "${var.agent_version}"
            workload_size: Small
            scale_to_zero_enabled: true
            
            # ================================================================
            # CRITICAL: Enable on-behalf-of authentication
            # This allows the agent to access data as the calling user.
            # Reference: https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html
            # ================================================================
            environment_vars:
              # Enable OBO for Genie and SDK
              DATABRICKS_USE_IDENTITY_PASSTHROUGH: "true"
              
              # Pass Genie Space IDs
              COST_GENIE_SPACE_ID: ${var.cost_genie_space_id}
              SECURITY_GENIE_SPACE_ID: ${var.security_genie_space_id}
              PERFORMANCE_GENIE_SPACE_ID: ${var.performance_genie_space_id}
              RELIABILITY_GENIE_SPACE_ID: ${var.reliability_genie_space_id}
              QUALITY_GENIE_SPACE_ID: ${var.quality_genie_space_id}
              UNIFIED_GENIE_SPACE_ID: ${var.unified_genie_space_id}
        
        # Traffic configuration
        traffic_config:
          routes:
            - served_model_name: current
              traffic_percentage: 100
```

### Agent Implementation with OBO

```python
class HealthMonitorAgent(ResponsesAgent):
    """Agent with context-aware OBO authentication support."""
    
    @mlflow.trace(name="predict", span_type="AGENT")
    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:
        """
        Execute agent with context-appropriate authentication.
        
        In Model Serving:
          - Uses OBO (end-user credentials)
          - Context contains user_id from calling user
        
        In Evaluation/Notebooks:
          - Uses default auth (your credentials)
          - Works with your permissions
        """
        # Extract query
        input_messages = [msg.model_dump() for msg in request.input]
        query = input_messages[-1].get("content", "")
        
        # Get authenticated client (context-aware)
        client = self._get_authenticated_client()
        
        # Extract user identity (available in Model Serving)
        user_id = getattr(request, 'context', {}).get('user_id', 'unknown')
        
        # Log user for audit trail
        mlflow.update_current_trace(tags={"user_id": user_id})
        
        # Process query (uses client with appropriate auth)
        result = self._process(client, query, user_id=user_id)
        
        return ResponsesAgentResponse(
            output=[self.create_text_output_item(text=result, id=str(uuid.uuid4()))],
            custom_outputs={"user_id": user_id}
        )
```

### Querying with OBO (Production)

```python
from databricks.sdk import WorkspaceClient

# Query agent endpoint (OBO automatically applied)
w = WorkspaceClient()

response = w.serving_endpoints.query(
    name="health-monitor-agent",
    inputs={
        "input": [{"role": "user", "content": "Show me failed jobs"}]
    }
)

# Agent queries data on behalf of calling user
# User sees only data they have permission to access
```

### Common OBO Errors and Fixes

#### Error 1: Permission Denied Outside Model Serving

**Symptom:**
```
âœ— Genie query failed: You need "Can View" permission to perform this action.
Config: host=https://..., auth_type=runtime
```

**Root Cause:** OBO attempted in notebook/job/evaluation (invalid credentials)

**Fix:** Add context detection (see pattern above)

#### Error 2: databricks-ai-bridge Import Error

**Symptom:**
```
ImportError: No module named 'databricks_ai_bridge'
```

**Root Cause:** Package not installed

**Fix:** Install package (graceful fallback already handled in pattern):
```bash
%pip install databricks-ai-bridge
dbutils.library.restartPython()
```

#### Error 3: OBO Works in Serving but Fails in Eval

**Symptom:**
```
# Model Serving: âœ“ Works
# Evaluation: âœ— Permission errors
```

**Root Cause:** Evaluation doesn't have Model Serving environment variables

**Expected Behavior:** This is correct! Evaluation uses your credentials, not OBO.

**Validation:**
```python
import os

# In evaluation/notebook - should be False
is_model_serving = os.environ.get("IS_IN_DB_MODEL_SERVING_ENV") == "true"
print(f"Model Serving: {is_model_serving}")  # False

# In Model Serving - should be True
is_model_serving = os.environ.get("IS_IN_DB_MODEL_SERVING_ENV") == "true"
print(f"Model Serving: {is_model_serving}")  # True
```

### OBO Dependencies

**Required Package:**
```
databricks-ai-bridge>=0.1.0  # For ModelServingUserCredentials
```

**Add to model logging:**
```python
mlflow.pyfunc.log_model(
    artifact_path="agent",
    python_model=agent,
    pip_requirements=[
        "mlflow>=3.0.0",
        "databricks-sdk>=0.30.0",
        "databricks-ai-bridge>=0.1.0",  # OBO support
    ],
    ...
)
```

### Automatic Authentication Passthrough for Evaluation/Notebooks (CRITICAL)

âš ï¸ **CRITICAL:** OBO context detection alone is NOT sufficient. Even with default `WorkspaceClient()`, evaluation will FAIL if Genie Spaces are not declared as resources!

**Problem (Jan 27, 2026):** After implementing OBO context detection, evaluation still failed with:
```
You need "Can View" permission to perform this action. Config: host=..., auth_type=runtime
```

**Root Cause:** MLflow agents have TWO separate authentication mechanisms:

| Mechanism | Context | How It Works |
|-----------|---------|--------------|
| **OBO (On-Behalf-Of-User)** | Model Serving | Uses end-user credentials via `UserAuthPolicy` scopes |
| **Automatic Auth Passthrough** | Evaluation/Notebooks | Databricks creates SERVICE PRINCIPAL with access to declared `resources` |

Without declaring `DatabricksGenieSpace` resources, the service principal has NO Genie access!

**Official Documentation Quote:**
> "Remember to log all downstream dependent resources, too. For example, if you log a Genie Space, you must also log its tables, SQL Warehouses, and Unity Catalog functions."

**Solution: Declare ALL Resources in SystemAuthPolicy**

```python
from mlflow.models.auth_policy import AuthPolicy, SystemAuthPolicy, UserAuthPolicy
from mlflow.models.resources import (
    DatabricksGenieSpace,
    DatabricksSQLWarehouse,
    DatabricksServingEndpoint,
    DatabricksLakebase,
)

def get_mlflow_resources():
    """
    Get ALL resources for Automatic Authentication Passthrough.
    
    CRITICAL: Include Genie Spaces AND SQL Warehouse!
    Without these, evaluation/notebook contexts cannot access Genie.
    
    Minimum MLflow versions:
    - DatabricksGenieSpace: 2.17.1+
    - DatabricksSQLWarehouse: 2.16.1+
    """
    resources = []
    
    # LLM endpoint
    resources.append(DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT))
    
    # Memory storage (if using Lakebase)
    resources.append(DatabricksLakebase(database_instance_name=LAKEBASE_INSTANCE))
    
    # ================================================================
    # CRITICAL: Add ALL Genie Spaces
    # ================================================================
    # Service principal needs "Can Run" on each Genie Space
    # ================================================================
    for domain, space_id in GENIE_SPACES.items():
        if space_id:
            resources.append(DatabricksGenieSpace(genie_space_id=space_id))
            print(f"âœ“ Added DatabricksGenieSpace: {domain} ({space_id})")
    
    # ================================================================
    # CRITICAL: Add SQL Warehouse for Genie query execution
    # ================================================================
    # Service principal needs "CAN USE" on the warehouse
    # ================================================================
    resources.append(DatabricksSQLWarehouse(warehouse_id=WAREHOUSE_ID))
    print(f"âœ“ Added DatabricksSQLWarehouse: {WAREHOUSE_ID}")
    
    return resources


def get_auth_policy():
    """
    Get combined AuthPolicy with BOTH system and user authentication.
    
    BOTH are required for complete coverage:
    - SystemAuthPolicy: For evaluation/notebook contexts (service principal)
    - UserAuthPolicy: For Model Serving contexts (end-user credentials via OBO)
    """
    # ================================================================
    # System policy: ALL resources for automatic auth passthrough
    # ================================================================
    # This creates a service principal with access to these resources.
    # Used in evaluation/notebook contexts where OBO doesn't work.
    # ================================================================
    system_resources = get_mlflow_resources()
    system_policy = SystemAuthPolicy(resources=system_resources)
    
    # ================================================================
    # User policy: API scopes for OBO in Model Serving
    # ================================================================
    # These scopes allow the agent to use end-user credentials
    # for Genie queries in production.
    # ================================================================
    user_policy = UserAuthPolicy(api_scopes=[
        "dashboards.genie",           # Genie Space access
        "sql.warehouses",             # SQL Warehouse access
        "sql.statement-execution",    # Execute SQL statements
        "serving.serving-endpoints",  # LLM endpoint calls
    ])
    
    return AuthPolicy(
        system_auth_policy=system_policy,
        user_auth_policy=user_policy
    )


# Log model with auth_policy (NOT just resources!)
mlflow.pyfunc.log_model(
    artifact_path="agent",
    python_model=agent,
    auth_policy=get_auth_policy(),  # Includes BOTH system and user policies
    pip_requirements=[...],
)
```

**Authentication Flow Summary:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Agent Authentication Flow                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Model Logging (mlflow.pyfunc.log_model)                        â”‚
â”‚  â”œâ”€â”€ auth_policy.system_auth_policy.resources                   â”‚
â”‚  â”‚   â””â”€â”€ Creates SERVICE PRINCIPAL with access to resources     â”‚
â”‚  â””â”€â”€ auth_policy.user_auth_policy.api_scopes                    â”‚
â”‚      â””â”€â”€ Declares API scopes for OBO                            â”‚
â”‚                                                                 â”‚
â”‚  Runtime Context Detection (_get_authenticated_client)          â”‚
â”‚  â”œâ”€â”€ Model Serving Environment?                                 â”‚
â”‚  â”‚   â”œâ”€â”€ YES â†’ Use OBO (end-user credentials via scopes)        â”‚
â”‚  â”‚   â””â”€â”€ NO  â†’ Use default auth (service principal via resources)â”‚
â”‚  â””â”€â”€ Result: Correct auth for BOTH contexts                     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Required Resources for Genie Agents:**

| Resource Type | Class | Permission Needed | Min MLflow |
|--------------|-------|-------------------|------------|
| Genie Space | `DatabricksGenieSpace` | Can Run | 2.17.1 |
| SQL Warehouse | `DatabricksSQLWarehouse` | CAN USE | 2.16.1 |
| LLM Endpoint | `DatabricksServingEndpoint` | Can Query | 2.13.1 |
| Lakebase | `DatabricksLakebase` | databricks_superuser | 3.3.2 |

### Validation Checklist

**Before Deploying with OBO/Automatic Auth Passthrough:**

**Resource Declaration (for Evaluation/Notebooks):**
- [ ] ALL Genie Spaces declared via `DatabricksGenieSpace(genie_space_id=...)`
- [ ] SQL Warehouse declared via `DatabricksSQLWarehouse(warehouse_id=...)`
- [ ] LLM endpoint declared via `DatabricksServingEndpoint(endpoint_name=...)`
- [ ] Lakebase (if used) declared via `DatabricksLakebase(database_instance_name=...)`
- [ ] Resources included in `SystemAuthPolicy(resources=[...])`
- [ ] `auth_policy` parameter passed to `mlflow.pyfunc.log_model()`

**OBO Configuration (for Model Serving):**
- [ ] Context detection implemented in auth client initialization
- [ ] Environment variable checks include all three variables
- [ ] Graceful fallback to default auth if OBO fails
- [ ] Logging shows which auth mode is active
- [ ] databricks-ai-bridge in pip_requirements
- [ ] API scopes declared in `UserAuthPolicy(api_scopes=[...])`
- [ ] Serving endpoint has `DATABRICKS_USE_IDENTITY_PASSTHROUGH: "true"`

**Testing:**
- [ ] Tested in notebook (should use default auth via service principal)
- [ ] Tested in Model Serving (should use OBO with end-user credentials)
- [ ] Genie queries succeed in evaluation context
- [ ] Genie queries succeed in Model Serving context
- [ ] User audit logging includes user_id in Model Serving
- [ ] Error messages don't expose credentials

### Production Learnings

**Jan 27, 2026 - Part 1: OBO Context Detection Fix**

**Problem:** Agent evaluation failing with permission errors despite user having proper permissions

**Root Cause:** Code attempted OBO if `databricks-ai-bridge` was installed, regardless of execution context. OBO produces invalid credentials outside Model Serving.

**Solution:** Added environment variable checks to detect Model Serving context:
- Check `IS_IN_DB_MODEL_SERVING_ENV == "true"`
- Check `DATABRICKS_SERVING_ENDPOINT is not None`
- Check `MLFLOW_DEPLOYMENT_FLAVOR_NAME == "databricks"`

**Reference:** See `docs/deployment/OBO_AUTH_FIX.md` for complete details

---

**Jan 27, 2026 - Part 2: Genie Resource Declaration Fix**

**Problem:** After implementing OBO context detection, evaluation STILL failed with:
```
You need "Can View" permission to perform this action. Config: host=..., auth_type=runtime
```

**Root Cause:** MLflow agents have TWO authentication mechanisms:
1. **OBO** - for Model Serving (uses `UserAuthPolicy` scopes)
2. **Automatic Auth Passthrough** - for evaluation/notebooks (uses `SystemAuthPolicy` resources)

The code had OBO configured via `user_auth_policy`, but `system_auth_policy.resources` did NOT include Genie Spaces! Without declaring `DatabricksGenieSpace` resources, the service principal created by Databricks has NO Genie access.

**Solution (3 parts):**
1. Add `DatabricksGenieSpace(genie_space_id=...)` for ALL Genie Spaces
2. Add `DatabricksSQLWarehouse(warehouse_id=...)` for query execution
3. Include ALL resources in `SystemAuthPolicy.resources`

**Impact:**
- âœ… Evaluation now works with service principal credentials
- âœ… Model Serving continues to work with OBO
- âœ… Same code works across all environments
- âœ… No runtime permission errors in any context

**Key Learning:** MLflow agent authentication has TWO SEPARATE mechanisms that work in different contexts:
- `user_auth_policy` â†’ OBO in Model Serving
- `system_auth_policy` â†’ Automatic passthrough in evaluation/notebooks

**BOTH must be configured.** Missing either one causes 100% failures in that context.

**Reference:** [Official Databricks Agent Authentication Docs](https://docs.databricks.com/aws/en/generative-ai/agent-framework/agent-authentication)

---

## Pattern 6: Visualization Hints (AI/BI Integration)

### Generating Visualization Metadata

```python
def generate_visualization_hints(domain: str, data: dict) -> dict:
    """
    Generate visualization hints for AI/BI dashboards.
    
    Returns metadata about chart type, axes, and formatting.
    """
    hints = {
        "visualization_type": None,
        "x_axis": None,
        "y_axis": None,
        "color_by": None,
        "sort_by": None,
        "format": {}
    }
    
    # Domain-specific hints
    if domain == "cost":
        if "time_series" in data:
            hints["visualization_type"] = "line_chart"
            hints["x_axis"] = "date"
            hints["y_axis"] = "cost_usd"
            hints["format"]["y_axis"] = {"type": "currency", "currency": "USD"}
        
        elif "top_n" in data:
            hints["visualization_type"] = "bar_chart"
            hints["x_axis"] = "job_name"
            hints["y_axis"] = "cost_usd"
            hints["sort_by"] = {"field": "cost_usd", "order": "desc"}
            hints["format"]["y_axis"] = {"type": "currency", "currency": "USD"}
    
    elif domain == "reliability":
        hints["visualization_type"] = "table"
        hints["columns"] = ["job_name", "failure_count", "last_failure_time"]
        hints["format"]["failure_count"] = {"type": "number"}
        hints["format"]["last_failure_time"] = {"type": "datetime"}
    
    return hints


# In agent response
response_text, visualization_hints = process_query(query)

return ResponsesAgentResponse(
    output=[self.create_text_output_item(text=response_text, id=str(uuid.uuid4()))],
    custom_outputs={
        "visualization_hints": visualization_hints,  # âœ… For AI/BI dashboards
        "domain": domain,
    }
)
```

---

## Common Mistakes to Avoid

### âŒ DON'T: Use `messages` Instead of `input`

```python
# BAD: Legacy ChatAgent format
input_example = {
    "messages": [{"role": "user", "content": "..."}]  # âŒ Wrong!
}
```

### âœ… DO: Use ResponsesAgent Format

```python
# GOOD: ResponsesAgent format
input_example = {
    "input": [{"role": "user", "content": "..."}]  # âœ… Correct!
}
```

### âŒ DON'T: Return Dict from predict()

```python
# BAD: Dict output
def predict(self, request):
    return {"messages": [...]}  # âŒ Wrong!
```

### âœ… DO: Return ResponsesAgentResponse

```python
# GOOD: ResponsesAgentResponse
def predict(self, request):
    return ResponsesAgentResponse(
        output=[self.create_text_output_item(...)]
    )
```

### âŒ DON'T: Pass Manual Signature

```python
# BAD: Manual signature breaks AI Playground
mlflow.pyfunc.log_model(
    python_model=agent,
    signature=my_custom_signature,  # âŒ Breaks auto-inference!
)
```

### âœ… DO: Let MLflow Infer Signature

```python
# GOOD: Auto-inference
mlflow.pyfunc.log_model(
    python_model=agent,
    # NO signature parameter!
)
```

### âŒ DON'T: Hardcode Deployment Context

```python
# BAD: Hardcoded environment
mlflow.update_current_trace(
    metadata={"mlflow.source.type": "PRODUCTION"}  # âŒ Hardcoded!
)
```

### âœ… DO: Use Environment Variables

```python
# GOOD: From environment
mlflow.update_current_trace(
    metadata={
        "mlflow.source.type": os.getenv("APP_ENVIRONMENT", "DEV")  # âœ… Dynamic!
    }
)
```

### âŒ DON'T: Use Custom Fields for Standard Context

```python
# BAD: Custom field names for standard concepts
mlflow.update_current_trace(
    metadata={
        "user": "user123",          # âŒ Wrong field name!
        "conversation": "session1"  # âŒ Wrong field name!
    }
)
```

### âœ… DO: Use Standard MLflow Fields

```python
# GOOD: Standard field names (enables UI filtering)
mlflow.update_current_trace(
    metadata={
        "mlflow.trace.user": "user123",       # âœ… Standard!
        "mlflow.trace.session": "session1"    # âœ… Standard!
    }
)
```

### âŒ DON'T: Try to Update Metadata on Finished Traces

```python
# BAD: Metadata is immutable after logging
trace_id = mlflow.get_last_active_trace_id()

mlflow.update_current_trace(
    metadata={"new_field": "value"}  # âŒ Won't work on finished trace!
)
```

### âœ… DO: Use Tags for Post-Logging Updates

```python
# GOOD: Tags are mutable
trace_id = mlflow.get_last_active_trace_id()

mlflow.set_trace_tag(
    trace_id=trace_id,
    key="review_status",
    value="approved"  # âœ… Works!
)
```

### âŒ DON'T: Update Trace Without Checking for Active Trace

```python
# BAD: Causes warnings during evaluation
def my_function():
    mlflow.update_current_trace(metadata={"key": "value"})  # âŒ May not have active trace!
```

### âœ… DO: Safe Trace Update

```python
# GOOD: Check for active trace first
def my_function():
    try:
        current_span = mlflow.get_current_active_span()
        if current_span:
            mlflow.update_current_trace(metadata={"key": "value"})  # âœ… Safe!
    except:
        pass  # No active trace, skip
```

---

## Validation Checklist

Before deploying agent:

### ResponsesAgent
- [ ] Agent inherits from `mlflow.pyfunc.ResponsesAgent`
- [ ] `predict()` accepts `request` parameter
- [ ] `predict()` returns `ResponsesAgentResponse`
- [ ] Input example uses `input` key (not `messages`)
- [ ] No `signature` parameter in `log_model()`

### Streaming
- [ ] `predict_stream()` implemented
- [ ] Yields `ResponsesAgentStreamEvent` objects
- [ ] Final `output_item.done` event sent
- [ ] `predict()` delegates to `predict_stream()` (code reuse)

### Tracing
- [ ] `@mlflow.trace` decorator on key functions
- [ ] Span types specified (`AGENT`, `TOOL`, `LLM`, etc.)
- [ ] Trace storage configured in Unity Catalog
- [ ] Traces visible in MLflow Experiment UI

### Trace Context (NEW!)
- [ ] Standard metadata fields used (`mlflow.trace.user`, `mlflow.trace.session`)
- [ ] Deployment context from environment variables (not hardcoded)
- [ ] Safe trace update helper for evaluation compatibility
- [ ] User ID and session ID returned in `custom_outputs`
- [ ] Tags used for mutable context, metadata for immutable
- [ ] Environment type set (`PRODUCTION`, `STAGING`, `DEV`)

### Multi-Agent
- [ ] LangGraph workflow defined
- [ ] Conditional routing based on intent
- [ ] Domain workers execute in parallel
- [ ] Synthesizer combines results
- [ ] No LLM fallback for Genie failures

### OBO Authentication
- [ ] `DATABRICKS_USE_IDENTITY_PASSTHROUGH=true` in serving config
- [ ] User identity logged to traces
- [ ] Genie queries use caller's permissions

### Visualization
- [ ] Visualization hints in `custom_outputs`
- [ ] Chart type and axes specified
- [ ] Format metadata included

---

## References

### Official Documentation - Core Patterns
- [ResponsesAgent](https://mlflow.org/docs/latest/genai/serving/responses-agent) - **Primary reference**
- [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html)
- [Unity Catalog Traces](https://docs.databricks.com/aws/en/mlflow3/genai/tracing/storage)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [Model Serving OBO](https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html)

### Official Documentation - Trace Context (NEW!)
- [Add Context to Traces](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/tracing/add-context-to-traces) - **Primary reference for metadata/tags**
- [Attach Custom Tags and Metadata](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/tracing/attach-tags/) - Tags vs metadata
- [Standard Metadata Fields](https://mlflow.org/docs/latest/genai/tracing/track-environments-context/#reserved-standard-tags) - MLflow conventions
- [Search Traces Programmatically](https://mlflow.org/docs/latest/genai/tracing/search-traces/) - Query patterns

### Related Rules
- [30-mlflow-genai-evaluation.mdc](30-mlflow-genai-evaluation.mdc) - Evaluation patterns
- [31-lakebase-memory-patterns.mdc](31-lakebase-memory-patterns.mdc) - Memory patterns
- [32-prompt-registry-patterns.mdc](32-prompt-registry-patterns.mdc) - Prompt management
- [35-production-monitoring-patterns.mdc](35-production-monitoring-patterns.mdc) - Production monitoring

### Implementation Reference
- `src/agents/setup/log_agent_model.py` - Complete ResponsesAgent implementation with trace context
- `src/agents/orchestrator/graph.py` - LangGraph multi-agent workflow
- `src/agents/setup/configure_trace_storage.py` - Trace configuration
- `resources/agents/agent_serving_endpoint.yml` - OBO serving config
