---
description: MLflow 3.0 GenAI patterns for tracing, evaluation, prompt registry, and agent logging in Databricks
globs: src/agents/**/*.py, src/ml/**/*.py
alwaysApply: false
---
# MLflow 3.0 GenAI Patterns

## Overview

This rule defines patterns for using MLflow 3.0 GenAI features in Databricks applications including tracing, evaluation with LLM judges, prompt registry, and agent logging.

---

## âš ï¸ CRITICAL: ResponsesAgent is MANDATORY for AI Playground Compatibility

**Databricks recommends `ResponsesAgent` over `ChatAgent` for all new agents.**

Without proper model signatures, your agent will **NOT work** in AI Playground, Agent Evaluation, or Mosaic AI features.

Reference: [Microsoft Docs - Model Signatures](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/create-agent#understand-model-signatures-to-ensure-compatibility-with-azure-databricks-features)

---

## ðŸ”´ Model Signatures: The Foundation of AI Playground Compatibility

### Why Model Signatures Matter

> "Azure Databricks uses **MLflow Model Signatures** to define agents' input and output schema. Product features like the **AI Playground assume that your agent has one of a set of supported model signatures**."
> â€” [Microsoft Docs](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/create-agent)

### The Golden Rule

> "If you follow the **recommended approach to authoring agents**, MLflow will **automatically infer a signature** for your agent that is compatible with Azure Databricks product features, with **no additional work required** on your part."
> â€” [Microsoft Docs](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/create-agent)

### What Happens Without Proper Signatures

| Issue | Impact |
|---|---|
| Manual signature with wrong schema | âŒ AI Playground won't load |
| `PythonModel` instead of `ResponsesAgent` | âŒ No signature inference |
| `messages` input instead of `input` | âŒ Request format mismatch |
| Legacy dict output instead of `ResponsesAgentResponse` | âŒ Response parsing fails |

---

## âœ… ResponsesAgent Pattern (MANDATORY for New Agents)

### Why ResponsesAgent?

Per [MLflow Docs](https://mlflow.org/docs/latest/genai/serving/responses-agent) and [Databricks Docs](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/author-agent):

| Feature | ResponsesAgent | Legacy ChatAgent/PythonModel |
|---|---|---|
| **Automatic signature inference** | âœ… Yes | âŒ Manual required |
| **AI Playground compatible** | âœ… Yes | âš ï¸ Depends on signature |
| **Streaming support** | âœ… Built-in | âŒ Manual |
| **Tool calling** | âœ… Built-in | âŒ Manual |
| **Multi-agent support** | âœ… Built-in | âŒ Manual |
| **OpenAI API compatible** | âœ… Responses API | âš ï¸ Chat Completions only |

### ResponsesAgent Implementation

```python
import mlflow
import uuid
from mlflow.pyfunc import ResponsesAgent
from mlflow.types.responses import ResponsesAgentRequest, ResponsesAgentResponse

class MyAgent(ResponsesAgent):
    """
    Agent implementing ResponsesAgent for Databricks AI Playground compatibility.
    
    Reference: https://mlflow.org/docs/latest/genai/serving/responses-agent
    Reference: https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/author-agent
    """
    
    def __init__(self):
        super().__init__()
        # Initialize your agent components
        self.llm_endpoint = "databricks-claude-3-7-sonnet"
    
    @mlflow.trace(name="my_agent", span_type="AGENT")
    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:
        """
        Handle inference requests using ResponsesAgent interface.
        
        Input format:  {"input": [{"role": "user", "content": "..."}]}
        Output format: ResponsesAgentResponse with output items
        """
        # Extract query from input (ResponsesAgent uses 'input', not 'messages')
        input_messages = [msg.model_dump() for msg in request.input]
        query = input_messages[-1].get("content", "")
        
        # Process query
        response_text = self._process(query)
        
        # Return ResponsesAgentResponse using helper method
        return ResponsesAgentResponse(
            output=[self.create_text_output_item(
                text=response_text,
                id=str(uuid.uuid4())
            )],
            custom_outputs={"source": "my_agent"}  # Optional metadata
        )
    
    def _process(self, query: str) -> str:
        # Your agent logic here
        return f"Processed: {query}"
```

### Logging ResponsesAgent (NO Manual Signature!)

```python
import mlflow

def log_agent():
    agent = MyAgent()
    
    # Set model for MLflow
    mlflow.models.set_model(agent)
    
    # Input example using ResponsesAgent format (input, NOT messages)
    input_example = {
        "input": [{"role": "user", "content": "What is the status?"}]
    }
    
    with mlflow.start_run():
        # ================================================================
        # CRITICAL: DO NOT pass signature parameter!
        # ResponsesAgent automatically infers the correct signature.
        # Manual signatures WILL BREAK AI Playground compatibility.
        # ================================================================
        logged_model = mlflow.pyfunc.log_model(
            artifact_path="agent",
            python_model=agent,
            input_example=input_example,
            # signature=...  # âŒ NEVER include this!
            registered_model_name="my_agent",
            pip_requirements=[
                "mlflow>=3.0.0",
                "databricks-sdk>=0.28.0",
            ],
        )
    
    return logged_model
```

### ResponsesAgent Input/Output Format

**Input Format (`request.input`):**
```python
{
    "input": [
        {"role": "user", "content": "Why did costs spike yesterday?"}
    ],
    "custom_inputs": {  # Optional
        "user_id": "user123",
        "session_id": "session456"
    }
}
```

**Output Format (`ResponsesAgentResponse`):**
```python
ResponsesAgentResponse(
    output=[
        {
            "type": "message",
            "id": "msg_123",
            "role": "assistant",
            "content": [
                {"type": "output_text", "text": "The cost spike was due to..."}
            ]
        }
    ],
    custom_outputs={"domain": "cost", "source": "genie"}
)
```

### Helper Methods for ResponsesAgent

```python
# Create text output item
self.create_text_output_item(text="Response text", id="msg_123")

# Create function call item (for tool calling)
self.create_function_call_item(
    id="fc_1",
    call_id="call_1",
    name="get_weather",
    arguments='{"city": "Boston"}'
)

# Create function call output item
self.create_function_call_output_item(
    call_id="call_1",
    output="72Â°F, Sunny"
)
```

---

## ðŸ”´ CRITICAL: No LLM Fallback for Genie/Tool Calls (Prevents Hallucination)

### The Problem

When an agent uses **Genie Spaces** or other **data retrieval tools**, a common anti-pattern is to fall back to an LLM when the tool fails. **This causes hallucination of fake data that looks real.**

### âŒ DANGEROUS: LLM Fallback Pattern (NEVER DO THIS)

```python
def predict(self, request):
    # Try Genie
    try:
        result = genie.invoke(query)
        return result
    except Exception:
        pass  # Genie failed
    
    # âŒ FATAL: LLM fallback generates FAKE DATA!
    llm_response = llm.invoke(f"Answer this question: {query}")
    return llm_response  # âŒ HALLUCINATED DATA!
```

**What happens:**
1. User asks "Which jobs failed today?"
2. Genie fails to initialize (missing package, auth error, etc.)
3. LLM "answers" with plausible-sounding but **completely fabricated** job names
4. User sees a professional-looking table of **fake failures**
5. User takes action on **non-existent problems**

### âœ… CORRECT: Explicit Error with No Data Fabrication

```python
def predict(self, request):
    genie = self._get_genie(domain)
    
    if genie:
        try:
            result = genie.invoke(query)
            return ResponsesAgentResponse(
                output=[self.create_text_output_item(text=result, id=str(uuid.uuid4()))],
                custom_outputs={"source": "genie"}
            )
        except Exception as e:
            # âœ… Return clear error - DO NOT hallucinate
            error_msg = f"""## Genie Query Failed

**Domain:** {domain}
**Query:** {query}
**Error:** {str(e)}

I was unable to retrieve real data from the Databricks Genie Space.

**Note:** I will NOT generate fake data. All responses must come from real system tables via Genie."""
            
            return ResponsesAgentResponse(
                output=[self.create_text_output_item(text=error_msg, id=str(uuid.uuid4()))],
                custom_outputs={"source": "error", "error": str(e)}
            )
    else:
        # âœ… Genie not available - explicit error
        error_msg = f"""## Genie Space Not Available

**Domain:** {domain}

The Genie Space is not currently available. Possible causes:
- `databricks-agents` package not installed
- Genie Space ID not configured
- Service principal lacks access

**Note:** I will NOT generate fake data."""
        
        return ResponsesAgentResponse(
            output=[self.create_text_output_item(text=error_msg, id=str(uuid.uuid4()))],
            custom_outputs={"source": "error", "error": "genie_not_available"}
        )
```

### When LLM Fallback IS Acceptable

| Scenario | LLM Fallback OK? | Reason |
|---|---|---|
| **Data queries** (jobs, costs, metrics) | âŒ **NEVER** | LLM will hallucinate fake data |
| **Explanation/interpretation** of real data | âœ… Yes | LLM explains, doesn't invent |
| **General questions** (not data-dependent) | âœ… Yes | No risk of fake data |
| **Classification/routing** | âœ… Yes | No data fabrication |

### The Golden Rule

> **If your agent uses Genie or any data retrieval tool, NEVER fall back to LLM for data responses. Return an explicit error instead.**

### Error Message Best Practices

A good error message should include:
1. **What failed** - Domain, query, error details
2. **Why it matters** - User understands the limitation
3. **What they can do** - Actionable next steps
4. **Explicit no-hallucination statement** - "I will NOT generate fake data"

### Trace Visibility

When implementing this pattern, ensure errors are visible in traces:

```python
with mlflow.start_span(name=f"genie_{domain}", span_type="TOOL") as tool_span:
    tool_span.set_inputs({"domain": domain, "query": query})
    
    if not genie:
        tool_span.set_attributes({"error": "genie_not_available", "fallback": "none"})
        tool_span.set_outputs({"source": "error"})
        return error_response
    
    try:
        result = genie.invoke(query)
        tool_span.set_outputs({"source": "genie", "response": result})
        return success_response
    except Exception as e:
        tool_span.set_attributes({"error": str(e), "fallback": "none"})
        tool_span.set_outputs({"source": "error"})
        return error_response
```

### Checklist: Genie/Tool Integration

- [ ] **NO LLM fallback** for data queries
- [ ] Clear error messages when tools fail
- [ ] "I will NOT generate fake data" statement in errors
- [ ] Errors logged to trace spans
- [ ] `custom_outputs.source` indicates "genie" or "error" (never "llm" for data)

---

## âŒ Common Mistakes That Break AI Playground

### Mistake 1: Using PythonModel Instead of ResponsesAgent

```python
# âŒ WRONG: PythonModel doesn't auto-infer compatible signature
class MyAgent(mlflow.pyfunc.PythonModel):
    def predict(self, context, model_input):
        return {"messages": [...]}  # Wrong format!

# âœ… CORRECT: ResponsesAgent auto-infers signature
class MyAgent(mlflow.pyfunc.ResponsesAgent):
    def predict(self, request):
        return ResponsesAgentResponse(output=[...])
```

### Mistake 2: Manual Signature Definition

```python
# âŒ WRONG: Manual signature breaks auto-inference
from mlflow.models.signature import ModelSignature
from mlflow.types.schema import Schema, ColSpec

signature = ModelSignature(
    inputs=Schema([ColSpec("string", "messages")]),
    outputs=Schema([ColSpec("string", "messages")])
)

mlflow.pyfunc.log_model(
    python_model=agent,
    signature=signature,  # âŒ This breaks AI Playground!
    ...
)

# âœ… CORRECT: Let MLflow infer signature
mlflow.pyfunc.log_model(
    python_model=agent,  # ResponsesAgent
    # NO signature parameter!
    ...
)
```

### Mistake 3: Using `messages` Instead of `input`

```python
# âŒ WRONG: ChatAgent/legacy format
input_example = {
    "messages": [{"role": "user", "content": "..."}]
}

# âœ… CORRECT: ResponsesAgent format
input_example = {
    "input": [{"role": "user", "content": "..."}]
}
```

### Mistake 4: Returning Dict Instead of ResponsesAgentResponse

```python
# âŒ WRONG: Dict output
def predict(self, request):
    return {
        "messages": [{"role": "assistant", "content": "..."}]
    }

# âœ… CORRECT: ResponsesAgentResponse
def predict(self, request):
    return ResponsesAgentResponse(
        output=[self.create_text_output_item(text="...", id="msg_1")]
    )
```

---

## ðŸ“‹ ResponsesAgent Checklist

Before deploying any agent:

### Model Structure
- [ ] Agent class inherits from `mlflow.pyfunc.ResponsesAgent`
- [ ] `predict` method accepts `request` parameter (not `context, model_input`)
- [ ] `predict` returns `ResponsesAgentResponse` (not dict)
- [ ] `@mlflow.trace` decorator on predict method

### Input/Output Format
- [ ] Input example uses `input` key (not `messages`)
- [ ] Input messages are `[{"role": "user", "content": "..."}]`
- [ ] Output uses `create_text_output_item()` helper
- [ ] Custom metadata in `custom_outputs` (not `metadata`)

### Logging
- [ ] `mlflow.models.set_model(agent)` called before logging
- [ ] **NO `signature` parameter** in `log_model()`
- [ ] Input example provided with correct format
- [ ] `pip_requirements` includes `mlflow>=3.0.0`

### Testing
- [ ] Agent works in Databricks notebook
- [ ] Agent loads in AI Playground
- [ ] Tracing appears in MLflow Experiment
- [ ] Model version shows in Unity Catalog

---

## Tracing Patterns

### Enable Automatic Tracing

Always enable autolog at the module level for LangChain applications:

```python
# At the TOP of your main module
import mlflow

mlflow.langchain.autolog(
    log_models=True,
    log_input_examples=True,
    log_model_signatures=True,
    log_inputs=True
)
```

### Manual Tracing with Decorators

Use `@mlflow.trace` for custom functions:

```python
import mlflow

@mlflow.trace(name="my_function", span_type="AGENT")
def my_agent_function(query: str) -> dict:
    """Function is automatically traced."""
    result = process(query)
    return result
```

**Span Types:**
- `AGENT`: Agent-level operations
- `LLM`: LLM calls
- `TOOL`: Tool invocations
- `RETRIEVER`: RAG/memory retrieval
- `CLASSIFIER`: Classification operations
- `MEMORY`: Memory operations

### Manual Span Creation

For fine-grained control:

```python
import mlflow

def complex_operation(data):
    with mlflow.start_span(name="outer_operation") as span:
        span.set_inputs({"data": data})
        
        with mlflow.start_span(name="inner_step", span_type="LLM") as inner:
            inner.set_inputs({"prompt": "..."})
            result = llm.invoke(...)
            inner.set_outputs({"response": result})
        
        span.set_outputs({"result": result})
        span.set_attributes({"custom_metric": 0.95})
        
    return result
```

### Trace Tagging

Always tag traces for filtering:

```python
mlflow.update_current_trace(tags={
    "user_id": user_id,
    "session_id": session_id,
    "environment": os.environ.get("ENVIRONMENT", "dev"),
    "domains": ",".join(domains),
    "confidence": str(confidence)
})
```

## Evaluation Patterns

### Built-in Scorers

```python
from mlflow.genai.scorers import Relevance, Safety, Correctness, GuidelinesAdherence

results = mlflow.genai.evaluate(
    model=agent,
    data=evaluation_data,
    scorers=[
        Relevance(),
        Safety(),
        Correctness(),
        GuidelinesAdherence(guidelines=[
            "Include time context",
            "Format costs as USD",
            "Cite sources"
        ])
    ]
)
```

### Custom LLM Judges

Use the `@scorer` decorator:

```python
from mlflow.genai import scorer, Score

@scorer
def domain_accuracy_judge(inputs: dict, outputs: dict, expectations: dict = None) -> Score:
    """Custom judge for domain-specific accuracy."""
    from langchain_databricks import ChatDatabricks
    
    llm = ChatDatabricks(endpoint="databricks-dbrx-instruct", temperature=0)
    
    prompt = f"""Evaluate the response accuracy (0-1):
Query: {inputs.get('query')}
Response: {outputs.get('response')}

Return JSON: {{"score": <float>, "rationale": "<reason>"}}"""
    
    import json
    result = json.loads(llm.invoke(prompt).content)
    
    return Score(
        value=result["score"],
        rationale=result["rationale"]
    )
```

### Production Monitoring

Use `mlflow.genai.assess()` for real-time assessment:

```python
assessment = mlflow.genai.assess(
    inputs={"query": query},
    outputs={"response": response},
    scorers=[Relevance(), Safety()]
)

if assessment.scores["relevance"] < 0.6:
    trigger_quality_alert()
```

## Prompt Registry Patterns

### Log Prompts

```python
import mlflow.genai

mlflow.genai.log_prompt(
    prompt="""You are a helpful assistant.
    
User context: {user_context}
Query: {query}""",
    artifact_path="prompts/assistant",
    registered_model_name="my_app_assistant_prompt"
)
```

### Load Prompts by Alias

```python
# Load production prompt
prompt = mlflow.genai.load_prompt(
    "prompts:/my_app_assistant_prompt/production"
)

# Load by version
prompt_v1 = mlflow.genai.load_prompt(
    "prompts:/my_app_assistant_prompt/1"
)
```

### Set Aliases

```python
from mlflow import MlflowClient

client = MlflowClient()

client.set_registered_model_alias(
    name="my_app_assistant_prompt",
    alias="production",
    version="2"
)
```

## Agent Logging Patterns

### âš ï¸ Use ResponsesAgent (Not ChatAgent)

**ChatAgent is legacy.** For new agents, always use `ResponsesAgent`.

See the **ResponsesAgent Pattern** section above for the recommended implementation.

### Legacy ChatAgent (Deprecated)

Only use if you have existing agents that cannot be migrated:

```python
# âš ï¸ LEGACY - Use ResponsesAgent for new agents
from mlflow.pyfunc import ChatAgent
from mlflow.types.agent import ChatAgentMessage, ChatAgentResponse

class MyLegacyAgent(ChatAgent):
    def __init__(self):
        self.model = create_model()
    
    @mlflow.trace(name="agent_predict", span_type="AGENT")
    def predict(
        self,
        context,
        messages: list[ChatAgentMessage],
        params: dict = None
    ) -> ChatAgentResponse:
        query = messages[-1].content
        result = self.model.invoke(query)
        
        return ChatAgentResponse(
            messages=[ChatAgentMessage(role="assistant", content=result)],
            metadata={"confidence": 0.9}
        )
```

### Log ResponsesAgent to Registry

```python
import mlflow

agent = MyResponsesAgent()

# CRITICAL: Set model before logging
mlflow.models.set_model(agent)

# Input example in ResponsesAgent format
input_example = {
    "input": [{"role": "user", "content": "What is the status?"}]
}

with mlflow.start_run():
    mlflow.log_params({"agent_type": "multi_agent"})
    
    # ================================================================
    # DO NOT pass signature parameter - auto-inferred!
    # ================================================================
    mlflow.pyfunc.log_model(
        artifact_path="agent",
        python_model=agent,
        input_example=input_example,
        # signature=...  # âŒ NEVER include!
        registered_model_name="my_agent",
        pip_requirements=[
            "mlflow>=3.0.0",
            "databricks-sdk>=0.28.0",
        ]
    )
```

### Promote to Production

```python
from mlflow import MlflowClient

client = MlflowClient()

# Get latest version
versions = client.search_model_versions(f"name='my_agent'")
latest = max(versions, key=lambda v: int(v.version))

# Set production alias
client.set_registered_model_alias(
    name="my_agent",
    alias="production",
    version=latest.version
)
```

## Common Mistakes

### Tracing

```python
# WRONG: Missing span type
@mlflow.trace(name="my_func")  # No span_type

# CORRECT: Include span type
@mlflow.trace(name="my_func", span_type="AGENT")
```

```python
# WRONG: Not setting inputs/outputs
with mlflow.start_span("operation"):
    result = process()

# CORRECT: Set inputs and outputs
with mlflow.start_span("operation") as span:
    span.set_inputs({"data": data})
    result = process()
    span.set_outputs({"result": result})
```

### Evaluation

```python
# WRONG: Custom judge without Score return type
@scorer
def my_judge(inputs, outputs):
    return 0.9  # Just a float

# CORRECT: Return Score object
@scorer
def my_judge(inputs, outputs) -> Score:
    return Score(value=0.9, rationale="Good response")
```

### Prompt Registry

```python
# WRONG: Loading by name only
prompt = mlflow.genai.load_prompt("my_prompt")

# CORRECT: Use full URI with alias
prompt = mlflow.genai.load_prompt("prompts:/my_prompt/production")
```

### Agent Logging

```python
# WRONG: Forgetting set_model before log_model
with mlflow.start_run():
    mlflow.langchain.log_model(agent, "agent")  # May fail

# CORRECT: Set model first
mlflow.models.set_model(agent)
with mlflow.start_run():
    mlflow.langchain.log_model(agent.graph, "agent")
```

## Validation Checklist

### ðŸ”´ ResponsesAgent & Model Signatures (CRITICAL)
- [ ] Agent class inherits from `mlflow.pyfunc.ResponsesAgent`
- [ ] `predict` method accepts single `request` parameter
- [ ] `predict` returns `ResponsesAgentResponse` object
- [ ] Input example uses `input` key (NOT `messages`)
- [ ] **NO `signature` parameter** in `log_model()` call
- [ ] Agent loads successfully in AI Playground

### Tracing
- [ ] `mlflow.langchain.autolog()` enabled at module level
- [ ] All custom functions decorated with `@mlflow.trace`
- [ ] Span types specified (AGENT, LLM, TOOL, etc.)
- [ ] Inputs and outputs set for manual spans
- [ ] Traces tagged with user_id, session_id, environment

### Evaluation
- [ ] Built-in scorers used where appropriate
- [ ] Custom judges return `Score` or `Feedback` objects
- [ ] Evaluation metrics logged to MLflow
- [ ] Production monitoring with `assess()` implemented

### Prompts
- [ ] All prompts logged to registry
- [ ] Production alias set for deployment
- [ ] Prompts loaded by alias in production code

### Agent Logging
- [ ] `ResponsesAgent` interface implemented (not ChatAgent)
- [ ] `set_model()` called before `log_model()`
- [ ] Model registered with proper name
- [ ] Aliases set for dev/staging/production

## References

### Model Signatures & AI Playground Compatibility
- [**Model Signatures for Databricks Features (CRITICAL)**](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/create-agent#understand-model-signatures-to-ensure-compatibility-with-azure-databricks-features)
- [ResponsesAgent for Model Serving](https://mlflow.org/docs/latest/genai/serving/responses-agent)
- [Author AI Agents in Code](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/author-agent)
- [MLflow Model Signatures](https://mlflow.org/docs/latest/ml/model/signatures/)
- [Legacy Agent Schema](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/agent-legacy-schema)

### Core MLflow GenAI
- [MLflow GenAI Concepts](https://docs.databricks.com/aws/en/mlflow3/genai/concepts/)
- [MLflow Tracing](https://docs.databricks.com/aws/en/mlflow3/genai/tracing/)
- [MLflow Scorers](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/concepts/scorers)
- [Prompt Registry](https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/)
- [Log Agent](https://docs.databricks.com/aws/en/generative-ai/agent-framework/log-agent)

---

## Version History

- **v3.1** (Jan 7, 2026) - Added NO LLM FALLBACK pattern for Genie/Tool calls
  - Added critical section on preventing hallucination when tools fail
  - Documented anti-pattern: LLM fallback generates fake data
  - Added correct pattern: explicit error messages without data fabrication
  - Added decision table for when LLM fallback is/isn't acceptable
  - Added error message best practices and trace visibility patterns
  - Added Genie/Tool integration checklist
  - **Key Learning:** LLM fallback for data queries = HALLUCINATED fake data that looks real
  - **Root Cause:** Agent showed "Job Failures Analysis" with fake job names when Genie failed

- **v3.0** (Jan 7, 2026) - Added ResponsesAgent as MANDATORY pattern
  - Added comprehensive section on MLflow Model Signatures importance
  - Added ResponsesAgent implementation pattern with code examples
  - Added common mistakes section for AI Playground compatibility
  - Added ResponsesAgent-specific checklist
  - Updated Agent Logging section to deprecate ChatAgent
  - Added Microsoft Docs references for model signatures
  - **Key Learning:** Manual signatures or PythonModel breaks AI Playground
  
- **v2.0** (Previous) - Custom scorers and Feedback patterns
- **v1.0** (Initial) - Basic tracing and evaluation patterns
