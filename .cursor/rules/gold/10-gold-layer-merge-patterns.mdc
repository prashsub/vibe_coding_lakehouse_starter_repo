---
description: Gold layer merge patterns including column mapping, schema evolution, and common pitfalls to avoid
globs: ["src/company_gold/*.py"]
alwaysApply: true
---

# Gold Layer MERGE Patterns

## Core Principle: Schema-Aware Transformations

Gold layer merge operations read from Silver and must handle:
1. Column name differences
2. Data type transformations
3. Business logic calculations
4. SCD Type 2 tracking

---

## üéØ Critical Rule: Extract Schema, Don't Generate

### Problem: Hallucinated Column Names Lead to Failures

**‚ùå Common Error Pattern:**
```python
# Developer generates column list from memory
updates_df = silver_df.select(
    "store_number",
    "store_name",
    "store_address",  # ‚ùå Doesn't exist! Silver has "address" not "store_address"
    "company_rcn"     # ‚ùå Gold expects "company_retail_control_number"
)
# Result: Column 'store_address' does not exist error
```

**Root Cause:** Generating column names from assumptions instead of extracting from actual schemas.

---

### Solution: Always Extract from Source of Truth

#### Source of Truth Hierarchy

| Priority | Source | Use For | Method |
|---|---|---|---|
| **1. Gold YAML** | `gold_layer_design/yaml/{domain}/{table}.yaml` | Target schema, column names, types, constraints | Parse YAML |
| **2. Silver Schema** | `DESCRIBE TABLE` or `df.columns` | Source available columns | Query metadata |
| **3. System Tables** | `system.information_schema.columns` | Verify table existence | SQL query |

---

### Extraction Pattern 1: Get Gold Table Schema

```python
import yaml
from pathlib import Path
from typing import Dict, List

def get_gold_schema(domain: str, table_name: str) -> Dict:
    """
    Extract Gold table schema from YAML (single source of truth).
    
    Returns:
        {
            'table_name': str,
            'columns': [{'name': str, 'type': str, 'nullable': bool, 'comment': str}],
            'primary_key': List[str],
            'foreign_keys': List[dict]
        }
    """
    yaml_file = Path(f"gold_layer_design/yaml/{domain}/{table_name}.yaml")
    
    if not yaml_file.exists():
        raise FileNotFoundError(
            f"Gold YAML not found: {yaml_file}\n"
            f"Cannot proceed without schema definition."
        )
    
    with open(yaml_file) as f:
        schema = yaml.safe_load(f)
    
    return schema

# ‚úÖ CORRECT: Extract actual Gold schema
gold_schema = get_gold_schema("billing", "fact_usage")
gold_columns = [col['name'] for col in gold_schema['columns']]
print(f"Gold table expects {len(gold_columns)} columns: {gold_columns}")

# ‚ùå WRONG: Hardcode column list
gold_columns = ["usage_date", "workspace_id", "sku", "dbus_used"]  # Incomplete!
```

---

### Extraction Pattern 2: Get Silver Table Columns

```python
def get_silver_columns(spark, catalog: str, silver_schema: str, table_name: str) -> List[str]:
    """
    Extract actual Silver table columns from metadata.
    
    Returns:
        List of column names available in Silver table
    """
    silver_table = f"{catalog}.{silver_schema}.{table_name}"
    
    try:
        silver_df = spark.table(silver_table)
        return silver_df.columns
    except Exception as e:
        raise ValueError(
            f"Silver table {silver_table} does not exist or is not accessible.\n"
            f"Error: {str(e)}"
        )

# ‚úÖ CORRECT: Get actual Silver columns
silver_columns = get_silver_columns(spark, catalog, silver_schema, "silver_usage")
print(f"Silver table has {len(silver_columns)} columns: {silver_columns}")

# ‚ùå WRONG: Assume Silver columns
silver_columns = ["date", "workspace_id", "sku", "usage"]  # Might be wrong!
```

---

### Extraction Pattern 3: Build Column Mapping

```python
def build_column_mapping(
    spark,
    catalog: str,
    silver_schema: str,
    silver_table: str,
    domain: str,
    gold_table: str
) -> Dict:
    """
    Build Silver ‚Üí Gold column mapping from actual schemas.
    
    Returns:
        {
            'direct_mappings': {silver_col: gold_col},  # Same name in both
            'renamed_mappings': {silver_col: gold_col},  # Different names (needs withColumn)
            'silver_only': [cols],  # In Silver but not Gold
            'gold_only': [cols]      # In Gold but not Silver (need to derive)
        }
    """
    # Extract actual schemas
    silver_columns = set(get_silver_columns(spark, catalog, silver_schema, silver_table))
    gold_schema = get_gold_schema(domain, gold_table)
    gold_columns = {col['name']: col for col in gold_schema['columns']}
    
    # Find direct matches (same name in both)
    direct_mappings = {}
    for col in silver_columns:
        if col in gold_columns:
            direct_mappings[col] = col
    
    # Find columns that need renaming (manual mapping required)
    # These must be explicitly defined based on business knowledge
    renamed_mappings = {}
    
    # Check for common naming patterns
    for silver_col in silver_columns:
        for gold_col in gold_columns:
            # Example: company_rcn ‚Üí company_retail_control_number
            if silver_col in gold_col or gold_col in silver_col:
                if silver_col not in direct_mappings:
                    print(f"‚ö†Ô∏è  Potential mapping: {silver_col} ‚Üí {gold_col}?")
    
    # Unmapped columns
    silver_only = silver_columns - set(direct_mappings.keys())
    gold_only = set(gold_columns.keys()) - set(direct_mappings.values())
    
    return {
        'direct_mappings': direct_mappings,
        'renamed_mappings': renamed_mappings,
        'silver_only': list(silver_only),
        'gold_only': list(gold_only)
    }

# ‚úÖ CORRECT: Build mapping from actual schemas
mapping = build_column_mapping(
    spark, catalog, silver_schema, "silver_usage",
    "billing", "fact_usage"
)

print("Direct mappings:", mapping['direct_mappings'])
print("‚ö†Ô∏è  Need manual mapping for:", mapping['silver_only'])
print("‚ö†Ô∏è  Need to derive:", mapping['gold_only'])
```

---

### Extraction Pattern 4: Validate Before Merge

```python
def validate_merge_readiness(
    spark,
    catalog: str,
    silver_schema: str,
    silver_table: str,
    domain: str,
    gold_table: str,
    manual_mappings: Dict[str, str] = None
) -> bool:
    """
    Validate that merge can proceed without column errors.
    
    Returns:
        True if all Gold columns can be satisfied from Silver
    """
    mapping = build_column_mapping(
        spark, catalog, silver_schema, silver_table,
        domain, gold_table
    )
    
    manual_mappings = manual_mappings or {}
    
    # Check if all Gold columns are accounted for
    all_mappings = {**mapping['direct_mappings'], **manual_mappings}
    gold_schema = get_gold_schema(domain, gold_table)
    gold_columns = {col['name'] for col in gold_schema['columns']}
    
    mapped_gold_columns = set(all_mappings.values())
    unmapped_gold = gold_columns - mapped_gold_columns
    
    # Exclude derived columns (calculated in code, not from Silver)
    derived_columns = {
        'record_created_timestamp',
        'record_updated_timestamp',
        'effective_from',
        'effective_to',
        'is_current'
    }
    unmapped_gold = unmapped_gold - derived_columns
    
    if unmapped_gold:
        print("‚ùå VALIDATION FAILED")
        print(f"Unmapped Gold columns: {unmapped_gold}")
        print("These columns exist in Gold YAML but no Silver source found.")
        print("Action: Add manual mapping or derive in code.")
        return False
    
    print("‚úÖ VALIDATION PASSED")
    print(f"All {len(gold_columns)} Gold columns accounted for.")
    return True

# ‚úÖ CORRECT: Validate before writing merge code
manual_mappings = {
    "company_rcn": "company_retail_control_number"
}

if validate_merge_readiness(
    spark, catalog, silver_schema, "silver_store_dim",
    "retail", "dim_store",
    manual_mappings
):
    # Safe to proceed with merge
    merge_dim_store(spark, catalog, silver_schema, gold_schema)
else:
    raise ValueError("Schema validation failed. Cannot proceed with merge.")
```

---

### Complete Extraction-Based Merge Example

```python
def merge_with_schema_extraction(
    spark,
    catalog: str,
    silver_schema: str,
    silver_table: str,
    domain: str,
    gold_table: str,
    manual_mappings: Dict[str, str] = None
):
    """
    Merge using extracted schemas to prevent column errors.
    """
    # ‚úÖ Step 1: Extract schemas
    gold_schema = get_gold_schema(domain, gold_table)
    silver_columns = get_silver_columns(spark, catalog, silver_schema, silver_table)
    
    # ‚úÖ Step 2: Build column mapping
    mapping = build_column_mapping(
        spark, catalog, silver_schema, silver_table,
        domain, gold_table
    )
    
    # ‚úÖ Step 3: Apply manual mappings
    manual_mappings = manual_mappings or {}
    all_mappings = {**mapping['direct_mappings'], **manual_mappings}
    
    # ‚úÖ Step 4: Validate
    if not validate_merge_readiness(
        spark, catalog, silver_schema, silver_table,
        domain, gold_table, manual_mappings
    ):
        raise ValueError("Schema validation failed")
    
    # ‚úÖ Step 5: Build transformation using extracted mappings
    silver_df = spark.table(f"{catalog}.{silver_schema}.{silver_table}")
    
    # Apply mappings
    updates_df = silver_df
    
    # Rename columns that need it
    for silver_col, gold_col in manual_mappings.items():
        updates_df = updates_df.withColumn(gold_col, col(silver_col))
    
    # Add derived columns
    updates_df = (
        updates_df
        .withColumn("record_created_timestamp", current_timestamp())
        .withColumn("record_updated_timestamp", current_timestamp())
    )
    
    # ‚úÖ Step 6: Select only Gold columns (in correct order from YAML)
    gold_column_names = [c['name'] for c in gold_schema['columns']]
    updates_df = updates_df.select(*gold_column_names)
    
    # ‚úÖ Step 7: Execute merge
    gold_table_fqn = f"{catalog}.{gold_schema['schema']}.{gold_table}"
    delta_gold = DeltaTable.forName(spark, gold_table_fqn)
    
    primary_key = gold_schema.get('primary_key', [])
    if not primary_key:
        raise ValueError(f"No primary key defined in Gold YAML for {gold_table}")
    
    merge_condition = " AND ".join([
        f"target.{pk} = source.{pk}" for pk in primary_key
    ])
    
    delta_gold.alias("target").merge(
        updates_df.alias("source"),
        merge_condition
    ).whenMatchedUpdateAll(
    ).whenNotMatchedInsertAll(
    ).execute()
    
    record_count = updates_df.count()
    print(f"‚úÖ Merged {record_count} records into {gold_table}")
    print(f"   Primary key: {primary_key}")
    print(f"   Columns: {len(gold_column_names)}")

# ‚úÖ CORRECT: Use extraction-based merge
merge_with_schema_extraction(
    spark, catalog, silver_schema, "silver_store_dim",
    "retail", "dim_store",
    manual_mappings={"company_rcn": "company_retail_control_number"}
)

# ‚ùå WRONG: Hardcode column transformations
updates_df = silver_df.select(
    "store_number",
    "store_name",
    col("company_rcn").alias("company_retail_control_number")
    # Incomplete! Missing 20+ other columns from Gold schema
)
```

---

## Column Name Mapping Pattern

### Problem: Column Names Differ Between Layers

**Example:** Silver has `company_rcn`, but Gold expects `company_retail_control_number`

### ‚ùå DON'T: Reference non-existent columns
```python
updates_df = (
    silver_df
    .select(
        "store_number",
        "company_retail_control_number",  # ‚ùå This column doesn't exist in Silver!
        # ...
    )
)
```

### ‚úÖ DO: Map columns explicitly with withColumn
```python
updates_df = (
    silver_df
    # Map Silver column to Gold column name
    .withColumn("company_retail_control_number", col("company_rcn"))
    .select(
        "store_number",
        "company_retail_control_number",  # ‚úÖ Now this exists
        # ...
    )
)
```

## Variable Naming Conflicts

### Problem: Import Conflicts with Local Variables

**Critical Rule:** NEVER name local variables the same as imported PySpark functions.

### ‚ùå DON'T: Shadow imported functions
```python
from pyspark.sql.functions import count

def merge_data():
    # Later in the code...
    count = updates_df.count()  # ‚ùå Shadows the imported 'count' function!
    
    # This will fail:
    df.agg(count("*"))  # Error: 'int' object is not callable
```

### ‚úÖ DO: Use descriptive variable names
```python
from pyspark.sql.functions import count

def merge_data():
    record_count = updates_df.count()  # ‚úÖ No conflict
    
    # This works:
    df.agg(count("*"))  # ‚úÖ Uses the imported function
```

### Common PySpark Functions to Avoid as Variable Names
- `count` ‚Üí use `record_count`, `row_count`, `num_records`
- `sum` ‚Üí use `total`, `sum_value`, `aggregated_sum`
- `min` ‚Üí use `min_value`, `minimum`
- `max` ‚Üí use `max_value`, `maximum`
- `round` ‚Üí use `rounded_value`, `result`
- `filter` ‚Üí use `filtered_df`, `subset`

## Merge Operation Patterns

### SCD Type 1 (Overwrite)

**Use for:** Dimension tables where history doesn't matter

```python
def merge_dim_product(spark: SparkSession, catalog: str, silver_schema: str, gold_schema: str):
    """Merge dim_product from Silver to Gold (SCD Type 1)."""
    
    silver_table = f"{catalog}.{silver_schema}.silver_product_dim"
    gold_table = f"{catalog}.{gold_schema}.dim_product"
    
    silver_df = spark.table(silver_table)
    
    # Prepare updates with column mappings
    updates_df = (
        silver_df
        .withColumn("product_key", col("upc_code"))  # Business key
        .withColumn("record_updated_timestamp", current_timestamp())
        .select(
            "product_key", "upc_code", "product_description",
            # ... other columns
            "record_updated_timestamp"
        )
    )
    
    delta_gold = DeltaTable.forName(spark, gold_table)
    
    # SCD Type 1: Update all fields when matched
    delta_gold.alias("target").merge(
        updates_df.alias("source"),
        "target.product_key = source.product_key"
    ).whenMatchedUpdateAll(
    ).whenNotMatchedInsertAll(
    ).execute()
    
    record_count = updates_df.count()  # ‚úÖ Good variable name
    print(f"‚úì Merged {record_count} records into dim_product")
```

### SCD Type 2 (Historical Tracking)

**Use for:** Dimension tables where you need to track changes over time

```python
def merge_dim_store(spark: SparkSession, catalog: str, silver_schema: str, gold_schema: str):
    """Merge dim_store from Silver to Gold (SCD Type 2)."""
    
    silver_table = f"{catalog}.{silver_schema}.silver_store_dim"
    gold_table = f"{catalog}.{gold_schema}.dim_store"
    
    silver_df = spark.table(silver_table)
    
    updates_df = (
        silver_df
        # Generate surrogate key
        .withColumn("store_key", md5(concat_ws("||", col("store_id"), col("processed_timestamp"))))
        
        # SCD Type 2 columns
        .withColumn("effective_from", col("processed_timestamp"))
        .withColumn("effective_to", lit(None).cast("timestamp"))
        .withColumn("is_current", lit(True))
        
        # Derived columns
        .withColumn("store_status", 
                   when(col("close_date").isNotNull(), "Closed").otherwise("Active"))
        
        # Column mappings
        .withColumn("company_retail_control_number", col("company_rcn"))
        
        # Timestamps
        .withColumn("record_created_timestamp", current_timestamp())
        .withColumn("record_updated_timestamp", current_timestamp())
        
        .select(
            "store_key", "store_number", "store_name",
            "company_retail_control_number",  # Mapped column
            "effective_from", "effective_to", "is_current",
            # ... other columns
        )
    )
    
    delta_gold = DeltaTable.forName(spark, gold_table)
    
    # SCD Type 2: Only update timestamp for current records
    delta_gold.alias("target").merge(
        updates_df.alias("source"),
        "target.store_number = source.store_number AND target.is_current = true"
    ).whenMatchedUpdate(set={
        "record_updated_timestamp": "source.record_updated_timestamp"
    }).whenNotMatchedInsertAll(
    ).execute()
    
    record_count = updates_df.count()
    print(f"‚úì Merged {record_count} records into dim_store")
```

### Fact Table Aggregation

**Use for:** Pre-aggregated fact tables from transactional Silver data

```python
def merge_fact_sales_daily(spark: SparkSession, catalog: str, silver_schema: str, gold_schema: str):
    """Merge fact_sales_daily from Silver to Gold."""
    
    silver_table = f"{catalog}.{silver_schema}.silver_transactions"
    gold_table = f"{catalog}.{gold_schema}.fact_sales_daily"
    
    transactions = spark.table(silver_table)
    
    # Aggregate daily sales
    daily_sales = (
        transactions
        .groupBy("store_number", "upc_code", "transaction_date")
        .agg(
            spark_sum(when(col("quantity_sold") > 0, col("final_sales_price")).otherwise(0)).alias("gross_revenue"),
            spark_sum(col("final_sales_price")).alias("net_revenue"),
            spark_sum(when(col("quantity_sold") > 0, col("quantity_sold")).otherwise(0)).alias("units_sold"),
            count("*").alias("transaction_count"),  # ‚úÖ PySpark function
            # ... more aggregations
        )
        .withColumn("record_created_timestamp", current_timestamp())
        .withColumn("record_updated_timestamp", current_timestamp())
    )
    
    delta_gold = DeltaTable.forName(spark, gold_table)
    
    delta_gold.alias("target").merge(
        daily_sales.alias("source"),
        """target.store_number = source.store_number 
           AND target.upc_code = source.upc_code 
           AND target.transaction_date = source.transaction_date"""
    ).whenMatchedUpdate(set={
        "net_revenue": "source.net_revenue",
        "units_sold": "source.units_sold",
        "transaction_count": "source.transaction_count",
        "record_updated_timestamp": "source.record_updated_timestamp"
    }).whenNotMatchedInsertAll(
    ).execute()
    
    record_count = daily_sales.count()  # ‚úÖ Good variable name
    print(f"‚úì Merged {record_count} records into fact_sales_daily")
```

## Schema Evolution Handling

### Data Type Changes

```python
# Example: INT to BIGINT migration
.withColumn("quantity_sold", col("quantity_sold").cast("bigint"))

# Example: DECIMAL to DOUBLE migration
.withColumn("price", col("price").cast("double"))
```

### Adding Derived Columns

```python
# Always calculate in the merge script, not in the target table
.withColumn("total_discount",
           coalesce(col("multi_unit_discount"), lit(0)) +
           coalesce(col("coupon_discount"), lit(0)) +
           coalesce(col("loyalty_discount"), lit(0)))
```

## Error Handling Pattern

```python
def main():
    """Main entry point for Gold layer MERGE operations."""
    
    catalog, silver_schema, gold_schema = get_parameters()
    
    spark = SparkSession.builder.appName("Gold Layer MERGE").getOrCreate()
    
    try:
        # Merge dimensions
        merge_dim_store(spark, catalog, silver_schema, gold_schema)
        merge_dim_product(spark, catalog, silver_schema, gold_schema)
        merge_dim_date(spark, catalog, silver_schema, gold_schema)
        
        # Merge facts
        merge_fact_sales_daily(spark, catalog, silver_schema, gold_schema)
        merge_fact_inventory_snapshot(spark, catalog, silver_schema, gold_schema)
        
        print("\n" + "=" * 80)
        print("‚úì Gold layer MERGE completed successfully!")
        print("=" * 80)
        
    except Exception as e:
        print(f"\n‚ùå Error during Gold layer MERGE: {str(e)}")
        raise
        
    finally:
        spark.stop()
```

## Validation Checklist

Before deploying Gold merge scripts:

### Schema Extraction (MANDATORY)
- [ ] ‚úÖ Gold schema extracted from YAML (not hardcoded)
- [ ] ‚úÖ Silver columns extracted from metadata (not assumed)
- [ ] ‚úÖ Column mapping built from actual schemas (not generated)
- [ ] ‚úÖ Validation script run before deployment
- [ ] ‚úÖ All Gold columns accounted for (mapped or derived)
- [ ] ‚úÖ No hardcoded column lists
- [ ] ‚úÖ No assumed column names
- [ ] ‚úÖ Gold YAML exists and is complete

### Code Quality
- [ ] All Silver column references exist
- [ ] Column mappings are explicit (using `.withColumn()`)
- [ ] No variable names shadow PySpark functions
- [ ] MERGE conditions use correct join keys
- [ ] SCD Type 2 includes `is_current` filter
- [ ] Timestamps are added (`record_created_timestamp`, `record_updated_timestamp`)
- [ ] Error handling with try/except
- [ ] Meaningful print statements for monitoring

### Pre-Deployment Validation Script

**ALWAYS run this before deploying merge code:**

```python
# scripts/validate_merge_schemas.py
import yaml
from pathlib import Path
from pyspark.sql import SparkSession

def validate_all_merges(spark, catalog: str, silver_schema: str):
    """
    Validate all Gold merge operations can proceed without errors.
    """
    validation_configs = [
        # (silver_table, domain, gold_table, manual_mappings)
        ("silver_store_dim", "retail", "dim_store", {"company_rcn": "company_retail_control_number"}),
        ("silver_product_dim", "retail", "dim_product", {}),
        ("silver_transactions", "retail", "fact_sales_daily", {}),
        # Add all merge operations here
    ]
    
    passed = 0
    failed = 0
    
    for silver_table, domain, gold_table, manual_mappings in validation_configs:
        print(f"\n{'='*80}")
        print(f"Validating: {silver_table} ‚Üí {gold_table}")
        print('='*80)
        
        try:
            if validate_merge_readiness(
                spark, catalog, silver_schema, silver_table,
                domain, gold_table, manual_mappings
            ):
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"‚ùå ERROR: {str(e)}")
            failed += 1
    
    print(f"\n{'='*80}")
    print(f"VALIDATION SUMMARY")
    print('='*80)
    print(f"Passed: {passed}")
    print(f"Failed: {failed}")
    
    if failed > 0:
        raise RuntimeError(
            f"{failed} merge validation(s) failed. "
            f"Fix schemas before deployment."
        )
    
    print("‚úÖ All merge validations passed!")

if __name__ == "__main__":
    spark = SparkSession.builder.appName("Validate Merges").getOrCreate()
    
    # Get parameters
    catalog = dbutils.widgets.get("catalog")
    silver_schema = dbutils.widgets.get("silver_schema")
    
    validate_all_merges(spark, catalog, silver_schema)
```

**Run before every deployment:**
```bash
databricks bundle run validate_merge_schemas_job
```

## Common Errors and Solutions

### Error: `Column 'X' does not exist`
**Root Cause:** Generated/assumed column name doesn't exist in Silver.

**‚ùå Wrong Approach:**
```python
# Guessing column names
updates_df = silver_df.select("store_address")  # Doesn't exist!
```

**‚úÖ Correct Solution:**
```python
# Extract actual Silver columns
silver_columns = get_silver_columns(spark, catalog, silver_schema, "silver_store_dim")
print(f"Available columns: {silver_columns}")

# Use only columns that exist
if "address" in silver_columns:
    updates_df = silver_df.withColumn("store_address", col("address"))
```

### Error: `'int' object is not callable` 
**Root Cause:** Variable name shadows a PySpark function.

**‚ùå Wrong Approach:**
```python
from pyspark.sql.functions import count
count = updates_df.count()  # Shadows import!
```

**‚úÖ Correct Solution:**
```python
from pyspark.sql.functions import count
record_count = updates_df.count()  # No conflict
```

### Error: `Cartesian product detected`
**Root Cause:** MERGE condition missing or uses wrong keys.

**‚ùå Wrong Approach:**
```python
# Guessing primary key
merge_condition = "target.id = source.id"  # Wrong key!
```

**‚úÖ Correct Solution:**
```python
# Extract primary key from Gold YAML
gold_schema = get_gold_schema(domain, gold_table)
primary_key = gold_schema['primary_key']
merge_condition = " AND ".join([
    f"target.{pk} = source.{pk}" for pk in primary_key
])
```

### Error: Schema mismatch during MERGE
**Root Cause:** Column types don't match between Silver and Gold.

**‚ùå Wrong Approach:**
```python
# Assuming types match
updates_df = silver_df.select("quantity")  # INT in Silver, BIGINT in Gold!
```

**‚úÖ Correct Solution:**
```python
# Extract expected types from Gold YAML
gold_schema = get_gold_schema(domain, gold_table)
quantity_col = next(c for c in gold_schema['columns'] if c['name'] == 'quantity')
expected_type = quantity_col['type']

# Cast to correct type
updates_df = silver_df.withColumn("quantity", col("quantity").cast(expected_type))
```

---

## Common Extraction Mistakes

### Mistake 1: Generating Column Lists

**‚ùå WRONG:**
```python
# Hardcoding column list from memory
columns = [
    "store_number", "store_name", "address",
    "city", "state", "zip_code"
]
updates_df = silver_df.select(*columns)
```

**Problem:** 
- Might miss columns (incomplete)
- Might include non-existent columns (error)
- No validation

**‚úÖ CORRECT:**
```python
# Extract from Gold YAML
gold_schema = get_gold_schema("retail", "dim_store")
gold_columns = [col['name'] for col in gold_schema['columns']]

# Validate Silver has these columns
silver_columns = set(get_silver_columns(spark, catalog, silver_schema, "silver_store_dim"))
missing = set(gold_columns) - silver_columns
if missing:
    print(f"‚ö†Ô∏è  Need to derive: {missing}")

# Select available columns
available = [c for c in gold_columns if c in silver_columns]
updates_df = silver_df.select(*available)
```

### Mistake 2: Assuming Column Name Mappings

**‚ùå WRONG:**
```python
# Assuming Silver uses short names
updates_df = (
    silver_df
    .withColumn("company_retail_control_number", col("rcn"))  # What if it's "company_rcn"?
    .withColumn("store_address", col("addr"))  # What if it's "address"?
)
```

**‚úÖ CORRECT:**
```python
# Build mapping from actual schemas
mapping = build_column_mapping(
    spark, catalog, silver_schema, "silver_store_dim",
    "retail", "dim_store"
)

# Review suggested mappings
print("Suggested mappings:")
for silver_col in mapping['silver_only']:
    for gold_col in mapping['gold_only']:
        if silver_col in gold_col or gold_col in silver_col:
            print(f"  {silver_col} ‚Üí {gold_col}?")

# Define explicit manual mappings
manual_mappings = {
    "company_rcn": "company_retail_control_number",
    "address": "store_address"
}

# Apply mappings
updates_df = silver_df
for silver_col, gold_col in manual_mappings.items():
    updates_df = updates_df.withColumn(gold_col, col(silver_col))
```

### Mistake 3: Not Validating Before Deployment

**‚ùå WRONG:**
```python
# Write merge code without validation
def merge_dim_store(...):
    updates_df = silver_df.select("store_number", "store_name", ...)
    # Deploys ‚Üí fails in production with column errors
```

**‚úÖ CORRECT:**
```python
# Validate first
if not validate_merge_readiness(
    spark, catalog, silver_schema, "silver_store_dim",
    "retail", "dim_store",
    manual_mappings
):
    raise ValueError("Schema validation failed. Fix before deploying.")

# Then write merge code
def merge_dim_store(...):
    # Safe to proceed
```

### Mistake 4: Using Old/Stale Schema Information

**‚ùå WRONG:**
```python
# Code written 2 weeks ago, schemas changed since then
columns = ["col1", "col2", "col3"]  # Stale!
```

**‚úÖ CORRECT:**
```python
# Always extract from current YAML/metadata
gold_schema = get_gold_schema(domain, gold_table)  # Latest schema
silver_columns = get_silver_columns(spark, ...)     # Current Silver state
```

---

## Recovery Pattern: When Schemas Change

**Scenario:** Gold YAML updated with new columns. Merge code breaks.

### Step 1: Detect Schema Drift
```python
# Run validation to detect drift
validate_merge_readiness(
    spark, catalog, silver_schema, silver_table,
    domain, gold_table, manual_mappings
)
# Output: "‚ùå Unmapped Gold columns: ['new_column_1', 'new_column_2']"
```

### Step 2: Update Manual Mappings
```python
# Add mappings for new columns
manual_mappings = {
    # Existing mappings
    "company_rcn": "company_retail_control_number",
    
    # New mappings for new columns
    "new_col": "new_column_1"
}

# Or mark as derived
derived_columns = {
    'new_column_2'  # Will calculate in code
}
```

### Step 3: Re-validate
```python
validate_merge_readiness(
    spark, catalog, silver_schema, silver_table,
    domain, gold_table, manual_mappings
)
# Output: "‚úÖ VALIDATION PASSED"
```

### Step 4: Update Merge Code
```python
# Add derivation logic for new calculated columns
updates_df = (
    updates_df
    .withColumn("new_column_2", 
                # Calculation logic here
                col("existing_col") * 100)
)
```

## References
- [Delta Lake MERGE](https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge)
- [SCD Type 2 Patterns](https://www.databricks.com/blog/2022/08/22/dimensional-modeling-delta-lake-how-to-manage-slowly-changing-dimensions.html)
