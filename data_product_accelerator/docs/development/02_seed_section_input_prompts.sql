-- =============================================================================
-- SEED DATA: SECTION INPUT PROMPTS
-- =============================================================================
-- order_number determines display order on Configuration UI (ascending)
-- how_to_apply and expected_output are user-facing content shown after prompt generation
-- Updated for 21-step workflow (Steps 1 & 2 are handled elsewhere: usecase selection & project setup)
--
-- TEMPLATE VARIABLES (automatically substituted at runtime):
--   {industry_name}           - Formatted industry name
--   {use_case_title}          - Formatted use case title
--   {use_case_description}    - Full use case description
--   {workspace_url}           - From Workshop Parameters config
--   {lakebase_instance_name}  - From Workshop Parameters config (Lakebase instance name)
--   {lakebase_host_name}      - From Workshop Parameters config (Lakebase host DNS)
--   {default_warehouse}       - From Workshop Parameters config
--   {prd_document}            - PRD from previous step
--   {table_metadata}          - Table metadata from previous step
--
-- Foundation (Steps 1-3)
--   Step 1 = Pick Use Case (handled by usecase_selection UI)
--   Step 2 = Set Up Project (handled by project_setup UI)
--   Step 3 = PRD Generation
--
-- Chapter 1: Databricks App (Steps 4-5)
--   Step 4 = UI Design
--   Step 5 = Deploy App
--
-- Chapter 2: Lakebase (Steps 6-8)
--   Step 6 = Setup Lakebase
--   Step 7 = Wire UI to Lakebase
--   Step 8 = Deploy and Test
--
-- Chapter 3: Lakehouse (Steps 9-14)
--   Step 9 = Sync from Lakebase (only shows when Lakebase ch2 is visible)
--   Step 10 = Table Metadata
--   Step 11 = Gold Layer Design
--   Step 12 = Bronze Layer Creation
--   Step 13 = Silver Layer
--   Step 14 = Gold Pipeline
--
-- Chapter 4: Data Intelligence (Steps 15-19)
--   Step 15 = Use-Case Plan
--   Step 16 = AI/BI Dashboard
--   Step 17 = Genie Space
--   Step 18 = Build Agent
--   Step 19 = Wire UI to Agent
--
-- Refinement (Steps 20-21)
--   Step 20 = Iterate & Enhance
--   Step 21 = Redeploy & Test
-- =============================================================================

-- Product Requirements Document (PRD)
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, version, is_active, inserted_at, updated_at, created_by)
VALUES
(1, 'prd_generation',
'Generate a prompt that I can copy into my AI coding assistant (Cursor/Copilot) to create a simple Product Requirements Document (PRD).

The generated prompt MUST include these instructions at the very beginning:

```
## IMPORTANT - READ FIRST
Your ONLY task is to create a PRD document. Do NOT:
- Generate any code or scripts
- Create any implementation files
- Start building the application
- Define table structures, schemas, or database designs
- Create table names or data models
- Define API endpoints, routes, or API specifications
- Include implementation-specific logic or technical details
- Do anything other than creating the PRD

You MUST:
- Create ONLY the PRD document
- Save it to: docs/design_prd.md
- STOP after saving the PRD - do nothing else
```

After those instructions, the prompt should ask for a simple, focused PRD for a {industry_name} application focused on {use_case_title}.

## Use Case Context to Include
{use_case_description}

## Application Context to Include
- **Industry**: {industry_name}
- **Use Case**: {use_case_title}
- Use a neutral, professional product name and generic terminology
- Web first, but include mobile considerations if applicable

## PRD Focus Guidelines
**Keep it simple** - Focus on providing enough details to generate a clear, readable PRD without over-engineering.

**Important Constraints:**
- Do NOT include table definitions, table names, or database schema designs - these will come in later steps
- Do NOT include API definitions, endpoints, or implementation-specific logic
- Only focus on **High Value workflows**
- Document **Happy Path only** - skip edge cases and error handling details for now
- Prioritize clarity over completeness

## PRD Structure to Request
The generated prompt should ask for a PRD with these sections:

1. **Summary** - Product vision, problem statement, target personas (2-3 max), goals + non-goals
2. **Scope** - MVP scope only, clear out of scope items
3. **User Journeys** - High-value end-to-end flows (Happy Path only) for primary personas
4. **Functional Requirements** - Key requirements with simple acceptance criteria
5. **Non-Functional Requirements** - Basic performance, security, accessibility notes
6. **High-Level Data Entities** - Entity names and relationships only (NO table definitions or schemas)
7. **Release Plan** - Simple milestones from MVP to GA

The prompt MUST end with:
```
Save this PRD to: docs/design_prd.md
STOP after saving. Do not generate any code, tables, APIs, or proceed with other tasks.
```',
'You are generating a prompt that users will copy into their AI coding assistant.

Your output should be a complete, ready-to-use prompt that when pasted into Cursor or Copilot will:
1. Create ONLY a simple Product Requirements Document
2. Save it to docs/design_prd.md
3. NOT generate any code, scripts, table definitions, or API specifications

CRITICAL: Your generated prompt MUST start with clear instructions telling the AI to ONLY create the PRD document and save it to docs/design_prd.md, and to NOT do anything else. Focus on High Value workflows with Happy Path only.

The prompt should be focused and specific to {use_case_title}, incorporating the use case context provided.

**OUTPUT FORMAT RULES:**
- Output the prompt directly as plain markdown text - do NOT wrap the entire output in code blocks or backticks
- Use proper markdown formatting: ## for headers, - for bullet points, **text** for bold
- For code blocks within your output (like file paths or specific instructions to include verbatim), use triple backticks on their own lines
- Do NOT use single backticks for multi-line content
- The output should render properly when displayed as markdown',
'Product Requirements Document (PRD)',
'Generate a simple PRD that defines what the application does and its key high-value features',
3,
'## Prerequisite

**Run this in your cloned Template Repository** (see Prerequisites in Step 0). These prompts assume you are working in that codebase with a coding assistant (Cursor or Copilot) enabled.

---

## Steps to Apply

1. **Copy the generated prompt** using the copy button
2. **Paste it into your AI coding assistant** (Cursor or VS Code with Copilot)
3. **Let the AI generate the PRD** - it will create a `docs/design_prd.md` file
4. **Review the generated PRD** carefully
   - Validate assumptions
   - Ensure all user personas are accurately represented
5. **DO NOT proceed to the next step** until you are satisfied with the PRD

**IMPORTANT:** This step ONLY generates the PRD document. No code or scripts should be created.

---

## After Generation

1. **Customize user personas** based on your actual target users
2. **Prioritize features** using MoSCoW method
3. **Refine acceptance criteria** - ensure all Given/When/Then scenarios are testable
4. **Get stakeholder sign-off** before proceeding to design',
'## Expected PRD Deliverables

### Document Sections
- **Summary** - Product vision, problem statement, personas, goals
- **Scope** - MVP vs V1/V2 with MoSCoW prioritization
- **User Journeys** - End-to-end flows with success/failure paths
- **Functional Requirements** - Numbered requirements with acceptance criteria
- **Non-Functional Requirements** - Performance, security, accessibility
- **Data & System Design** - Entity model, APIs, integrations
- **Risks & Dependencies** - What could go wrong
- **Release Plan** - Milestones from MVP to GA
- **Requirements Coverage Checklist** - Validation of completeness

### Quality Metrics
- Every requirement has acceptance criteria
- All user journeys have success AND failure paths
- Edge cases and error states are documented
- Analytics events are specified for key actions',
1, TRUE, current_timestamp(), current_timestamp(), current_user());

-- Figma UI Design
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(2, 'figma_ui_design',
'Generate the **actual UI mockups** for a simple, happy-path implementation using the Product Requirements Document as the source of truth.

## Product Requirements Document (PRD)

Use the following PRD to understand user personas, key user journeys, and core features:

{prd_document}

---

## Design Requirements

Create a **simple UI design** that includes:
- Key screens/pages for primary user personas
- Core components for functional requirements (Happy Path only)
- Simple data display layouts for main entities
- Basic navigation and interactions for high-value user journeys

In addition, document the design with:
- Key screens/pages
- Core components
- Basic navigation and interactions (happy path only)

**Keep it simple** ‚Äî focus on essential screens and flows, not edge cases.',
'This prompt is returned as-is for you to paste into Figma AI. No LLM processing is applied.',
'Figma UI Design',
'Design a simple, clean user interface using Figma AI',
4,
'## Steps to Apply

1. **First, get your PRD ready:**
   - Copy the PRD content generated from the previous step (Step 3)
   - Or open `@docs/design_prd.md` and copy its contents

2. **Open Figma and create a new design file**

3. **Upload/Attach the PRD to Figma:**
   - In Figma AI, you can paste the PRD content directly
   - Or use Galileo AI and provide the PRD as context

4. **Copy the generated prompt** using the copy button above

5. **Paste into Figma AI or Galileo AI** to generate the design

6. **Review and iterate** on the generated components

7. **Export the design** when ready:
   - Export as images/assets for reference
   - Or use Figma''''s code export features

8. **Move to code implementation:**
   - Open Cursor or VS Code with the project
   - Use the exported Figma designs as visual reference
   - Implement the UI components in code

**Note:** The prompt includes `{prd_document}` which will be replaced with your actual PRD content.',
'## Expected Output

Simple UI design mockups that match the PRD:
- Key screens for primary user flows
- Basic component designs
- Clean, minimal layouts
- Ready for implementation in Cursor/Copilot',
TRUE,
1, TRUE, current_timestamp(), current_timestamp(), current_user());

-- UI Design - Build Locally
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(3, 'cursor_copilot_ui_design',
'## Your Task

You are a full-stack developer building a web application. Your goal is to **generate UI and backend APIs** from the PRD and **test locally**.

**Workspace:** `{workspace_url}`

**Working directory:** Run all app commands and create/edit app files under the `apps_lakebase/` folder. Design docs (PRD, UI design) remain in the parent `docs/` folder at repo root.

---

### Step 1: Authenticate and Set Up Variables

```bash
# Authenticate to Databricks
databricks auth login --host {workspace_url}

# Derive app name from your username
FIRSTNAME=$(databricks current-user me --output json | jq -r ''.userName'' | cut -d''@'' -f1 | cut -d''.'' -f1)
LASTINITIAL=$(databricks current-user me --output json | jq -r ''.userName'' | cut -d''@'' -f1 | cut -d''.'' -f2 | cut -c1)
USERNAME="${FIRSTNAME}_${LASTINITIAL}"
APP_NAME="${USERNAME}_vibe_coding_app"
EMAIL=$(databricks current-user me --output json | jq -r ''.userName'')
echo "App: $APP_NAME | Email: $EMAIL"
```

---

### Step 2: Update Configuration Files

Update `apps_lakebase/app.yaml` with your app name:

```yaml
name: <APP_NAME>  # Replace with your APP_NAME from Step 1
```

Update `apps_lakebase/databricks.yml` with your app name in the resources section:

```yaml
resources:
  apps:
    vibe_coding_workshop:
      name: <APP_NAME>  # Replace with your APP_NAME from Step 1
```

**Important:** These files must have matching app names for deployment to work correctly. Both files are under `apps_lakebase/`.

---

### Step 3: Read the PRD

Review `@docs/design_prd.md` (parent folder) to understand:
- User personas and their needs
- Key user journeys (Happy Path only)
- Core features and requirements

---

### Step 4: Generate the UI

Create a **working web UI** with:
- Key pages/views for primary user personas
- Components for core features (Happy Path only)
- Simple data display and forms for high-value user journeys
- Follow the existing project''s framework, styling, and patterns

**Frontend code location:** `@apps_lakebase/src/components/`

---

### Step 5: Create Backend APIs

Add API endpoints in `@apps_lakebase/src/backend/api/routes.py` for each UI feature:
- The UI must call these APIs - no hardcoded/mock data in components
- Use placeholder data in API responses for now (Lakebase wiring comes later)

**Code Pattern:**

```python
# Backend API (routes.py)
@router.get("/api/items")
async def get_items():
    return {"data": [...]}
```

```typescript
// Frontend component - calls backend API
useEffect(() => {
  fetch(''/api/items'').then(res => res.json()).then(data => setItems(data.data));
}, []);
```

---

### Step 6: Create UI Design Document

Save a design overview to `@docs/ui_design.md` (parent folder at repo root) describing:
- Key screens/pages
- Core components
- Basic navigation flow

---

### Step 7: Verify app.py Serves Frontend

From the `apps_lakebase/` folder, ensure `app.py` serves the built frontend. It should have routes to serve `/` and `/assets`.

---

### Step 8: Test Locally

**Test the app locally before deployment (run from `apps_lakebase/`):**

1. Build the frontend: `npm install && npm run build`
2. Start the backend server: `python app.py`
3. Open `http://localhost:8000` in your browser
4. Verify:
   - The UI loads correctly
   - Navigation works
   - Backend API endpoints respond
   - No console errors

**Only proceed to deployment (next step) after local testing passes.**

---

### Summary

Your job is complete when:
- Configuration files (apps_lakebase/app.yaml, apps_lakebase/databricks.yml) are updated with your app name
- A working web UI is generated under apps_lakebase/
- Backend APIs are created in apps_lakebase/src/backend/api/routes.py
- `@docs/ui_design.md` is created (parent docs folder)
- Local testing passes at `http://localhost:8000` (run from apps_lakebase/)',
'You are a full-stack developer who builds web applications with React frontends and Python backends.

Your approach:
1. Read the PRD to understand user needs and journeys
2. Build functional UI components with clean code
3. Create backend API endpoints for data flow
4. Test locally before deployment

Development principles:
- Keep it simple - focus on Happy Path flows first
- UI must call backend APIs - no hardcoded/mock data
- Reuse existing components where possible
- Follow the project''s existing patterns and conventions

This prompt is returned as-is for direct use in Cursor/Copilot. No LLM processing.',
'UI Design - Build Locally',
'Build UI and backend APIs, test locally before deployment',
4,
'## How to Use

1. **Copy the generated prompt**
2. **Paste into Cursor or Copilot**
3. The code assistant will:
   - Read your PRD
   - Generate UI components and backend APIs
   - Create ui_design.md document
   - Test locally at localhost:8000

**Note:** This step focuses on local development. Deployment to Databricks happens in the next step.',
'## Expected Output

- Working web UI components under apps_lakebase/src/
- Backend API endpoints in apps_lakebase/src/backend/api/routes.py
- UI components calling backend APIs (not static HTML)
- UI design document at `@docs/ui_design.md` (parent folder)
- App running locally at `http://localhost:8000` (from apps_lakebase/)',
TRUE,
1, TRUE, current_timestamp(), current_timestamp(), current_user());

-- Deploy and Test
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(4, 'workspace_setup_deploy',
'## Your Task

Deploy the locally-tested web application to Databricks Apps and run comprehensive end-to-end testing.

**Workspace:** `{workspace_url}`

**Working directory:** All app paths and commands use the `apps_lakebase/` folder.

> **Prerequisite:** Complete Step 7 (Wire UI to Lakebase) first. Local testing must pass before deployment.

---

## Deployment Constraints
- Databricks App names must use only lowercase letters, numbers, and dashes (no underscores).
  Use hyphens: `my-app-name` not `my_app_name`.
- The Databricks Apps runtime auto-runs `npm install` and `npm run build` when it
  finds a `package.json`. Ensure `databricks.yml` sync config includes both `dist/**`
  AND `src/**` so the platform build succeeds.

---

### Step 1: Verify Local Build

Ensure the frontend is built and local testing has passed (from `apps_lakebase/` or check path):
```bash
ls apps_lakebase/dist/index.html || (cd apps_lakebase && npm run build)
```

---

### Step 2: Deploy Application

1. **Review the `apps_lakebase/scripts/` folder** in this project to find a deployment script (e.g., `deploy.sh`)
2. **Read the script** to understand its usage and options
3. **Run the deployment script** from `apps_lakebase/` (e.g. `cd apps_lakebase && ./scripts/deploy.sh`) with appropriate parameters for your target environment
4. **If errors occur:** Fix them and retry the deployment
5. **If no deployment script exists:** Search Databricks documentation for `databricks apps` CLI commands and deploy manually

**Note:** If the app already exists, use sync and redeploy instead of stop/start - it''s faster.

---

### Step 3: Get App URL and Verify UI

Get the deployed app URL and visit it in a browser:
```bash
APP_NAME="<your-app-name>"
databricks apps get $APP_NAME --output json | jq -r ''.url''
```

You should see the web UI, not JSON. Verify:
- The UI loads correctly
- Navigation works
- ConnectionStatus indicator shows "Live Data" or "Mock Data"

---

### Step 4: Test All Backend APIs

Test the health endpoint and all data endpoints:
```bash
APP_URL=$(databricks apps get $APP_NAME --output json | jq -r ''.url'')

# Test health endpoint
curl -s "$APP_URL/api/health/lakebase" | jq .

# Test each data endpoint used by your UI pages
# Replace with your actual API endpoints:
curl -s "$APP_URL/api/listings" | jq .
curl -s "$APP_URL/api/bookings" | jq .
# ... add all endpoints that fetch from Lakebase
```

**Verify each response includes:**
- `"source": "live"` (not "mock") when Lakebase is connected
- Actual data from your Lakebase tables

---

### Step 5: Check Logs for Lakebase Connections

```bash
databricks apps logs $APP_NAME --tail 100 | grep -i lakebase
```

You should see INFO logs showing:
- Connection attempts to Lakebase
- Queries being executed for each page/endpoint
- Success messages with row counts

---

### Step 6: Fix Errors (up to 3 iterations)

If any errors occur:

1. **Check the logs:**
   ```bash
   databricks apps logs $APP_NAME --tail 100
   ```

2. **Common errors and fixes:**
   - "No module named ''psycopg2''" ‚Üí Add to requirements.txt and rebuild
   - "token''s identity did not match" ‚Üí Set LAKEBASE_USER to service principal ID
   - "role does not exist" ‚Üí Run add-lakebase-role command
   - "Could not import module" ‚Üí Check apps_lakebase/app.yaml command matches file structure

3. **Fix the issue, rebuild, and redeploy (from apps_lakebase/):**
   ```bash
   cd apps_lakebase && npm run build
   # Run deploy script again from apps_lakebase/
   ```

4. **Repeat up to 3 times.** If errors persist, report them for manual investigation.

---

### If the Workspace App Limit Is Reached

If deployment fails because the workspace has hit its app limit, do NOT rename your app. Instead, free up a slot by removing the oldest stopped app:

1. Find stopped apps sorted by oldest first:
   ```bash
   databricks apps list -o json | jq -r ''[.[] | select(.compute_status.state == "STOPPED")] | sort_by(.update_time) | .[0] | .name''
   ```
2. Delete it and wait for cleanup to complete:
   ```bash
   databricks apps delete <name-from-above>
   sleep 10
   ```
3. Retry the deployment.

If the limit error persists, repeat with the next oldest stopped app -- but **stop after 3 total attempts** (increase the wait to 20s, then 40s between retries). If it still fails after 3 tries, stop and report the issue for manual workspace cleanup. Never delete apps in RUNNING state.

---

### Summary

Your job is complete when:
- Databricks App is deployed and running
- Web UI is accessible at the app URL
- ConnectionStatus shows "Live Data" (connected to Lakebase)
- All API endpoints return live data from Lakebase
- No errors in the app logs',
'You are deploying a web application to Databricks Apps and running comprehensive testing.

Your approach:
1. Use existing deployment scripts when available
2. Deploy to Databricks Apps
3. Verify the deployment by testing the app URL
4. Test all API endpoints for live data
5. Check logs for Lakebase connectivity
6. Debug and fix any deployment errors

CLI Best Practices:
- Check the `apps_lakebase/scripts/` folder for existing deploy scripts before writing ad-hoc commands
- Run CLI commands outside the IDE sandbox to avoid SSL/TLS certificate errors

This prompt is returned as-is for direct use in Cursor/Copilot. No LLM processing.',
'Deploy and Test',
'Deploy the application to Databricks and run full end-to-end testing',
7,
'## Prerequisite

Complete Step 7 (Wire UI to Lakebase) first. Local testing must pass.

---

## Steps to Apply

1. Copy the generated prompt
2. Paste into Cursor or Copilot
3. The code assistant will:
   - Deploy the app to Databricks
   - Get the app URL and verify UI loads
   - Test all backend APIs for live data
   - Check logs for Lakebase connectivity
   - Fix any errors and retry',
'## Expected Deliverables

- Databricks App deployed and running
- Web UI accessible at the app URL
- ConnectionStatus shows "Live Data"
- All API endpoints return live data from Lakebase
- No errors in the app logs',
TRUE,
1, TRUE, current_timestamp(), current_timestamp(), current_user());

-- Step 9: Table Metadata & Data Dictionary (Bronze Layer) - bypass_llm=TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(5, 'bronze_table_metadata',
'Extract table schema metadata from Databricks and save as a CSV data dictionary.

This will:

- **Query information_schema.columns** ‚Äî extract all table and column metadata from the **{chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema}** source
- **Convert results to CSV** ‚Äî transform the JSON API response into a structured CSV file using Python
- **Save as data_product_accelerator/context/{chapter_3_lakehouse_schema}_Schema.csv** ‚Äî create the data dictionary that drives the entire Design-First Pipeline (all subsequent steps reference this CSV)

**Source:** `{chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema}` (configured in the source panel above ‚Äî auto-set from Step 9 or editable via Edit)

Copy and paste this prompt to the AI:

```
Run this SQL query and save results to CSV:

Query: SELECT * FROM {chapter_3_lakehouse_catalog}.information_schema.columns WHERE table_schema = ''{chapter_3_lakehouse_schema}'' ORDER BY table_name, ordinal_position

Output: data_product_accelerator/context/{chapter_3_lakehouse_schema}_Schema.csv

---

Technical reference (for AI execution):

1. Get warehouse ID:
   databricks warehouses list --output json | jq ''.[0].id''

2. Execute SQL via Statement Execution API:
   databricks api post /api/2.0/sql/statements --json ''{
     "warehouse_id": "<WAREHOUSE_ID>",
     "statement": "<SQL_QUERY>",
     "wait_timeout": "50s",
     "format": "JSON_ARRAY"
   }'' > /tmp/sql_result.json

3. Convert JSON to CSV with Python:
   python3 << ''EOF''
   import json, csv
   with open(''/tmp/sql_result.json'', ''r'') as f:
       result = json.load(f)
   if result.get(''status'', {}).get(''state'') != ''SUCCEEDED'':
       print(f"Query failed: {result.get(''status'')}")
       exit(1)
   columns = [col[''name''] for col in result[''manifest''][''schema''][''columns'']]
   data = result[''result''][''data_array'']
   with open(''<OUTPUT_FILE>'', ''w'', newline='''') as f:
       writer = csv.writer(f)
       writer.writerow(columns)
       writer.writerows(data)
   print(f"Saved {len(data)} rows to <OUTPUT_FILE>")
   EOF

Known warehouse ID: <YOUR_WAREHOUSE_ID> (get via: databricks warehouses list --output json | jq ''.[0].id'')

Common queries:
- Schema info: SELECT * FROM <catalog>.information_schema.columns WHERE table_schema = ''<schema>'' ORDER BY table_name, ordinal_position
- Table list: SELECT * FROM <catalog>.information_schema.tables WHERE table_schema = ''<schema>''
- Sample data: SELECT * FROM <catalog>.<schema>.<table> LIMIT 1000

Expected output (for schema query):
- Console: "Saved N rows to data_product_accelerator/context/{chapter_3_lakehouse_schema}_Schema.csv"
- CSV file with columns: table_catalog, table_schema, table_name, column_name, ordinal_position, is_nullable, data_type, comment, ...
```',
'',
'Table Metadata & Data Dictionary',
'Extract table schema metadata from Databricks and save as CSV for data dictionary reference',
8,
'## 1Ô∏è‚É£ How To Apply

Copy the prompt from the Prompt tab, start a new Agent chat in your IDE, paste it and press Enter.

**Prerequisite:** Run this in your cloned Template Repository (see Prerequisites in Step 0). Ensure Databricks CLI is authenticated.

**Steps:** Copy the prompt ‚Üí paste into Cursor or VS Code with Copilot ‚Üí AI executes SQL via Databricks CLI ‚Üí CSV saved to data_product_accelerator/context/{chapter_3_lakehouse_schema}_Schema.csv.

**Note:** The source catalog and schema are shown in the **Source** panel above this prompt. If you completed Step 9 (Register Lakebase in Unity Catalog), these are automatically set to your Lakebase UC catalog and user schema. You can edit or reset them using the Edit/Reset buttons.

---

## 2Ô∏è‚É£ What Are We Building?

This step extracts the **data dictionary** ‚Äî a CSV file containing every table, column, data type, and comment from the source schema. This CSV becomes the starting input for the entire Design-First Pipeline:

```
data_product_accelerator/context/{chapter_3_lakehouse_schema}_Schema.csv
  ‚Üí Gold Design (Step 11)  ‚Äî reads CSV to design dimensional model
  ‚Üí Bronze (Step 12)       ‚Äî uses schema to create tables
  ‚Üí Silver (Step 13)       ‚Äî uses schema for DQ expectations
  ‚Üí Gold Impl (Step 14)    ‚Äî uses YAML schemas derived from this CSV
```

---

## 3Ô∏è‚É£ Why Are We Building It This Way? (Databricks Best Practices)

| Practice | How It''s Used Here |
|----------|-------------------|
| **Unity Catalog `information_schema`** | Queries `information_schema.columns` ‚Äî the standard UC metadata catalog ‚Äî instead of proprietary DESCRIBE commands |
| **SQL Statement Execution API** | Uses the REST API (`/api/2.0/sql/statements`) for programmatic SQL execution ‚Äî the production-grade approach for CI/CD |
| **Data Dictionary as Governance Foundation** | The CSV captures table/column COMMENTs from UC, establishing metadata lineage from day one |
| **Serverless SQL Warehouse** | Executes against a SQL warehouse (not a cluster) for cost-efficient, instant-start queries |

---

## 4Ô∏è‚É£ What Happens Behind the Scenes?

This step does **not** invoke an Agent Skill ‚Äî it runs a direct SQL extraction via the Databricks CLI. Every subsequent skill references this CSV (or artifacts derived from it) to **extract** table names, column names, and data types ‚Äî never generating them from scratch. This is the "Extract, Don''t Generate" principle.',
'## Expected Deliverables

- data_product_accelerator/context/{chapter_3_lakehouse_schema}_Schema.csv file created
- Contains column metadata rows for all tables in {chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema}
- Includes: table_name, column_name, data_type, comment
- Ready for use as data dictionary reference
- **This CSV is the starting input for the entire Design-First Pipeline** (all subsequent steps reference it)',
true, 1, true, current_timestamp(), current_timestamp(), current_user());

-- Step 9: Gold Layer Design (PRD-aligned) - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(6, 'gold_layer_design',
'I have a customer schema at @data_product_accelerator/context/{chapter_3_lakehouse_schema}_Schema.csv.

Please design the Gold layer using @data_product_accelerator/skills/gold/00-gold-layer-design/SKILL.md

This skill will orchestrate the following end-to-end design workflow:

- **Parse the schema CSV** ‚Äî read the source schema file, classify each table as a dimension, fact, or bridge, and infer foreign key relationships from column names and comments
- **Design the dimensional model** ‚Äî identify dimensions (with SCD Type 1/2 decisions), fact tables (with explicit grain definitions), and measures, then assign tables to business domains
- **Create ERD diagrams** ‚Äî generate Mermaid Entity-Relationship Diagrams organized by table count (master ERD always, plus domain and summary ERDs for larger schemas)
- **Generate YAML schema files** ‚Äî produce one YAML file per Gold table with column definitions, PK/FK constraints, table properties, lineage metadata, and dual-purpose descriptions (human + LLM readable)
- **Document column-level lineage** ‚Äî trace every Gold column back through Silver to Bronze with transformation type (DIRECT_COPY, AGGREGATION, DERIVATION, etc.) in both CSV and Markdown formats
- **Create business documentation** ‚Äî write a Business Onboarding Guide with domain context, real-world scenarios, and role-based getting-started guides
- **Map source tables** ‚Äî produce a Source Table Mapping CSV documenting which source tables are included, excluded, or planned with rationale for each
- **Validate design consistency** ‚Äî cross-check YAML schemas, ERD diagrams, and lineage CSV to ensure all columns, relationships, and constraints are consistent

The orchestrator skill will automatically load its worker skills for merge patterns, deduplication, documentation standards, Mermaid ERDs, schema validation, grain validation, and YAML-driven setup.',
'',
'Gold Layer Design (PRD-aligned)',
'Design Gold layer using project skills with YAML definitions and Mermaid ERD',
9,
'## 1Ô∏è‚É£ How To Apply

Copy the prompt from the **Prompt** tab, start a **new Agent chat** in your IDE, paste it, and press Enter.

---

### Prerequisite

**Run this in your cloned Template Repository** (see Prerequisites in Step 0).

Ensure you have:
- ‚úÖ `data_product_accelerator/context/{chapter_3_lakehouse_schema}_Schema.csv` - Your source schema file (from Bronze/Silver)
- ‚úÖ `data_product_accelerator/skills/gold/00-gold-layer-design/SKILL.md` - The Gold layer design orchestrator skill

---

### Steps to Apply

1. **Start new Agent thread** ‚Äî Open Cursor and start a new Agent thread for clean context
2. **Copy and paste the prompt** ‚Äî Use the copy button, paste into Cursor; the AI will read your schema and the orchestrator skill (which automatically loads all worker skills)
3. **Review generated design** ‚Äî The AI creates `gold_layer_design/` with ERD diagrams, YAML schema files, and lineage documentation
4. **Validate the design** ‚Äî Check grain, SCD type, relationships, and lineage for each fact/dimension
5. **Get stakeholder sign-off** ‚Äî Share the ERD and design summary with business stakeholders before implementation

---

## 2Ô∏è‚É£ What Are We Building?

### What is the Gold Layer?

The Gold Layer is the **business-ready** analytics layer that transforms Silver data into dimensional models optimized for reporting, dashboards, and AI/ML consumption.

### Why Design Before Implementation?

| Principle | Benefit |
|-----------|---------|
| **Design First** | Catch errors before writing code |
| **YAML as Source of Truth** | Schema changes are reviewable diffs |
| **ERD Documentation** | Visual communication with stakeholders |
| **Documented Grain** | Prevents incorrect aggregations |
| **Lineage Tracking** | Know where every column comes from |

### Gold Layer Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           GOLD LAYER DESIGN                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                        DIMENSIONAL MODEL                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚îÇdim_store ‚îÇ    ‚îÇdim_product‚îÇ   ‚îÇdim_date  ‚îÇ    ‚îÇdim_host  ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ (SCD2)   ‚îÇ    ‚îÇ (SCD1)   ‚îÇ    ‚îÇ (Static) ‚îÇ    ‚îÇ (SCD2)   ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ        ‚îÇ               ‚îÇ               ‚îÇ               ‚îÇ          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                ‚îÇ                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                        ‚îÇ fact_bookings ‚îÇ                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                        ‚îÇ   (Daily)     ‚îÇ                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  Design Artifacts:                                                          ‚îÇ
‚îÇ  ‚Ä¢ ERD Diagrams (Mermaid)      ‚Ä¢ YAML Schema Files                         ‚îÇ
‚îÇ  ‚Ä¢ Column Lineage              ‚Ä¢ Business Documentation                     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 3Ô∏è‚É£ Why Are We Building It This Way? (Databricks Best Practices)

| Practice | How It''s Used Here |
|----------|-------------------|
| **YAML-Driven Dimensional Modeling** | Gold schemas defined as YAML files ‚Äî reviewable, version-controlled, machine-readable. No embedded DDL strings in Python. |
| **Star Schema with Surrogate Keys** | Dimensions use surrogate keys (BIGINT) as PRIMARY KEYs, not business keys. Facts reference surrogate PKs via FOREIGN KEY constraints. |
| **SCD Type 1 / Type 2 Classification** | Every dimension is classified: SCD1 (overwrite, e.g., `dim_destination`) or SCD2 (versioned with `is_current`/`valid_from`/`valid_to`, e.g., `dim_property`). |
| **Dual-Purpose COMMENTs** | Table and column COMMENTs serve both business users AND Genie/LLMs ‚Äî written to be human-readable and machine-parseable simultaneously. |
| **Mermaid ERDs for Documentation** | Entity-Relationship Diagrams use Mermaid syntax ‚Äî renderable in Databricks notebooks, GitHub, and any Markdown viewer. |
| **Column-Level Lineage** | Every Gold column traces back to its Silver source table and column with transformation type (DIRECT_COPY, AGGREGATION, DERIVATION). |
| **Grain Documentation** | Every fact table has an explicit grain statement (e.g., "One row per booking transaction") ‚Äî prevents incorrect aggregations and joins. |

---

## 4Ô∏è‚É£ What Happens Behind the Scenes?

This framework uses a **skills-first architecture** with an **orchestrator/worker pattern**:

1. You paste **one prompt** referencing the orchestrator: `@data_product_accelerator/skills/gold/00-gold-layer-design/SKILL.md`
2. The AI reads the orchestrator skill, which lists **mandatory dependencies** (worker skills + common skills)
3. The AI automatically loads each worker skill as needed during the workflow
4. You never need to reference individual worker skills ‚Äî the orchestrator handles it

### 9-Phase Workflow

| Phase | What Happens | Key Output |
|-------|-------------|------------|
| **Phase 0** | Parse schema CSV, classify tables (dim/fact/bridge), infer FKs | Table inventory |
| **Phase 1** | Gather project requirements (domain, use cases, stakeholders) | Project context |
| **Phase 2** | Design dimensional model (dimensions, facts, grain, SCD types) | Model blueprint |
| **Phase 3** | Create ERD diagrams using Mermaid syntax | `erd_master.md` + domain ERDs |
| **Phase 4** | Generate YAML schema files with lineage and descriptions | `yaml/{domain}/{table}.yaml` |
| **Phase 5** | Document column-level lineage (Bronze ‚Üí Silver ‚Üí Gold) | `COLUMN_LINEAGE.csv` |
| **Phase 6** | Write Business Onboarding Guide with real-world scenarios | `BUSINESS_ONBOARDING_GUIDE.md` |
| **Phase 7** | Map source tables with inclusion/exclusion rationale | `SOURCE_TABLE_MAPPING.csv` |
| **Phase 8** | Validate design consistency (YAML ‚Üî ERD ‚Üî Lineage) | Validation report |

### Orchestrator / Worker Pattern

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     ORCHESTRATOR / WORKER PATTERN                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  YOUR PROMPT                                                                ‚îÇ
‚îÇ  "@data_product_accelerator/skills/gold/00-gold-layer-design/SKILL.md"                              ‚îÇ
‚îÇ         ‚îÇ                                                                   ‚îÇ
‚îÇ         ‚ñº                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ  ‚îÇ  ORCHESTRATOR (00-gold-layer-design)‚îÇ                                   ‚îÇ
‚îÇ  ‚îÇ  Manages the full design workflow   ‚îÇ                                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                 ‚îÇ  automatically loads                                      ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                ‚îÇ
‚îÇ    ‚ñº            ‚ñº            ‚ñº            ‚ñº                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   + 3 more workers             ‚îÇ
‚îÇ  ‚îÇ 01-  ‚îÇ  ‚îÇ 02-  ‚îÇ   ‚îÇ 05-  ‚îÇ   ‚îÇ 07-  ‚îÇ                                 ‚îÇ
‚îÇ  ‚îÇGrain ‚îÇ  ‚îÇ Dims ‚îÇ   ‚îÇ ERD  ‚îÇ   ‚îÇValid ‚îÇ                                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                 ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  + Common Skills: naming-tagging-standards, databricks-expert-agent        ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Design Worker Skills (Loaded Automatically by Orchestrator)

| Worker Skill | Path | Purpose |
|-----------|------|---------|
| `01-grain-definition` | `data_product_accelerator/skills/gold/design-workers/01-*/SKILL.md` | Grain definition patterns for fact tables |
| `02-dimension-patterns` | `data_product_accelerator/skills/gold/design-workers/02-*/SKILL.md` | Dimension design (SCD1, SCD2, conformed) |
| `03-fact-table-patterns` | `data_product_accelerator/skills/gold/design-workers/03-*/SKILL.md` | Fact table design (transactional, periodic, accumulating) |
| `04-conformed-dimensions` | `data_product_accelerator/skills/gold/design-workers/04-*/SKILL.md` | Cross-domain conformed dimension patterns |
| `05-erd-diagrams` | `data_product_accelerator/skills/gold/design-workers/05-*/SKILL.md` | Mermaid ERD diagram syntax and organization |
| `06-table-documentation` | `data_product_accelerator/skills/gold/design-workers/06-*/SKILL.md` | Dual-purpose (business + technical) documentation standards |
| `07-design-validation` | `data_product_accelerator/skills/gold/design-workers/07-*/SKILL.md` | Design consistency validation (YAML, ERD, lineage cross-check) |',
'## Expected Deliverables

### üìÅ Generated Folder Structure

```
gold_layer_design/
‚îú‚îÄ‚îÄ README.md                           # Navigation hub
‚îú‚îÄ‚îÄ erd_master.md                       # Complete ERD (ALWAYS)
‚îú‚îÄ‚îÄ erd_summary.md                      # Domain overview (if 20+ tables)
‚îú‚îÄ‚îÄ erd/                                # Domain ERDs (if 9+ tables)
‚îÇ   ‚îú‚îÄ‚îÄ erd_booking.md
‚îÇ   ‚îú‚îÄ‚îÄ erd_property.md
‚îÇ   ‚îî‚îÄ‚îÄ erd_host.md
‚îú‚îÄ‚îÄ yaml/                               # YAML schemas by domain
‚îÇ   ‚îú‚îÄ‚îÄ booking/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dim_booking.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fact_booking_daily.yaml
‚îÇ   ‚îú‚îÄ‚îÄ property/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dim_property.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dim_destination.yaml
‚îÇ   ‚îî‚îÄ‚îÄ host/
‚îÇ       ‚îî‚îÄ‚îÄ dim_host.yaml
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ BUSINESS_ONBOARDING_GUIDE.md    # ‚≠ê Business context and stories
‚îú‚îÄ‚îÄ COLUMN_LINEAGE.csv                  # ‚≠ê Machine-readable lineage
‚îú‚îÄ‚îÄ COLUMN_LINEAGE.md                   # Human-readable lineage
‚îú‚îÄ‚îÄ SOURCE_TABLE_MAPPING.csv            # ‚≠ê Source table rationale
‚îú‚îÄ‚îÄ DESIGN_SUMMARY.md                   # Grain, SCD, decisions
‚îî‚îÄ‚îÄ DESIGN_GAP_ANALYSIS.md             # Coverage analysis
```

---

### üìä ERD Organization (Based on Table Count)

| Total Tables | ERD Strategy |
|--------------|--------------|
| **1-8 tables** | Master ERD only |
| **9-20 tables** | Master ERD + Domain ERDs |
| **20+ tables** | Master ERD + Summary ERD + Domain ERDs |

---

### üìù YAML Schema Example

```yaml
# gold_layer_design/yaml/booking/fact_booking_daily.yaml
table_name: fact_booking_daily
domain: booking
grain: "One row per property-date combination"

primary_key:
  columns: [''property_id'', ''check_in_date'']
  composite: true

foreign_keys:
  - columns: [''property_id'']
    references: dim_property(property_id)
  - columns: [''host_id'']
    references: dim_host(host_id)

columns:
  - name: property_id
    type: BIGINT
    nullable: false
    description: >
      Property identifier.
      Business: Links to property dimension.
      Technical: FK to dim_property.property_id.
    lineage:
      silver_table: silver_bookings
      silver_column: property_id
      transformation: "DIRECT_COPY"
```

---

### ‚úÖ Success Criteria Checklist

**ERD Artifacts:**
- [ ] Master ERD created with all tables
- [ ] Domain ERDs created (if 9+ tables)
- [ ] All relationships shown with cardinality

**YAML Schemas:**
- [ ] One YAML file per table
- [ ] Organized by domain folders
- [ ] Primary keys defined
- [ ] Foreign keys defined
- [ ] Column lineage documented

**Mandatory Documentation:**
- [ ] COLUMN_LINEAGE.csv created
- [ ] SOURCE_TABLE_MAPPING.csv created
- [ ] BUSINESS_ONBOARDING_GUIDE.md created
- [ ] DESIGN_SUMMARY.md created

**Validation:**
- [ ] Grain explicitly stated for each fact
- [ ] SCD type specified for each dimension
- [ ] All columns trace back to source
- [ ] Stakeholder sign-off obtained',
true, 1, true, current_timestamp(), current_timestamp(), current_user());

-- Step 10: Bronze Layer Creation (Approach C - Copy Sample Data) - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(7, 'bronze_layer_creation',
'Set up the Bronze layer using @data_product_accelerator/skills/bronze/00-bronze-layer-setup/SKILL.md with Approach C ‚Äî copy data from the existing source tables in the {chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema} schema.

This will involve the following steps:

- **Clone all source tables** from the {chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema} schema into your target catalog''s Bronze schema
- **Apply enterprise table properties** ‚Äî enable Change Data Feed (CDF), Liquid Clustering (CLUSTER BY AUTO), auto-optimize, and auto-compact on every table
- **Preserve source COMMENTs** ‚Äî carry over all column-level documentation from the source schema
- **Create Asset Bundle job** ‚Äî generate a repeatable, version-controlled deployment job (databricks.yml + clone script)
- **Deploy and run** ‚Äî validate, deploy the bundle, and execute the clone job to populate Bronze tables

IMPORTANT: Use the EXISTING catalog `{lakehouse_default_catalog}` -- do NOT create a new catalog. Create the Bronze schema `{user_schema_prefix}_bronze` and tables inside this catalog.

NOTE: Before creating the schema, check if `{lakehouse_default_catalog}.{user_schema_prefix}_bronze` already exists. If it does, DROP the schema with CASCADE and recreate it from scratch. These are user-specific schemas so dropping is safe.',
'',
'Bronze Layer Creation (Approach C)',
'Create Bronze layer by copying sample data from {chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema} with Asset Bundle structure',
10,
'## 1Ô∏è‚É£ How To Apply

Copy the prompt above, start a **new Agent chat** in Cursor, and paste it.

### Prerequisite

**Run this in your cloned Template Repository** (see Prerequisites in Step 0).

Ensure you have:
- ‚úÖ `data_product_accelerator/skills/bronze/00-bronze-layer-setup/SKILL.md` - The Bronze layer setup skill
- ‚úÖ Access to `{chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema}` catalog in your Databricks workspace
- ‚úÖ Permissions to create tables in your target catalog

### Steps to Apply

**Step 1:** Start a new Agent thread in Cursor
**Step 2:** Copy the prompt and paste it into Cursor
**Step 3:** Review generated code (Asset Bundle config, clone script, job definition)
**Step 4:** Validate: `databricks bundle validate -t dev`
**Step 5:** Deploy: `databricks bundle deploy -t dev`
**Step 6:** Run: `databricks bundle run -t dev bronze_clone_job`
**Step 7:** Verify in Databricks UI (SHOW TABLES, row counts, CDF enabled)

---

## 2Ô∏è‚É£ What Are We Building?

### What is the Bronze Layer?

The Bronze Layer is the **raw data landing zone** in the Medallion Architecture. It preserves source data exactly as received, enabling full traceability and reprocessing.

| Principle | Benefit |
|-----------|---------|
| **Raw Preservation** | Keep original data for audit and replay |
| **Change Data Feed** | Enable incremental processing downstream |
| **Schema Evolution** | Handle schema changes gracefully |
| **Single Source** | One place for all raw data ingestion |

### Bronze Layer in Medallion Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        MEDALLION ARCHITECTURE                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ  ‚îÇ   BRONZE    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   SILVER    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    GOLD     ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ   (Raw)     ‚îÇCDF ‚îÇ  (Cleaned)  ‚îÇCDF ‚îÇ  (Business) ‚îÇ                     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îÇ        ‚ñ≤                                                                    ‚îÇ
‚îÇ        ‚îÇ                                                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                             ‚îÇ
‚îÇ  ‚îÇ  SOURCE   ‚îÇ  ‚óÄ‚îÄ‚îÄ This step creates Bronze from source                   ‚îÇ
‚îÇ  ‚îÇ   DATA    ‚îÇ                                                              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                              ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  CDF = Change Data Feed (enables incremental processing)                    ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Three Approaches for Bronze Data

| Approach | When to Use | What Happens |
|----------|-------------|--------------|
| **A: Generate Fake Data** | Testing/demos before customer delivery | Create DDLs, populate with Faker library |
| **B: Use Existing Bronze** | Customer already has Bronze layer | Skip this step, connect directly |
| **C: Copy from External** | Sample data available (THIS WORKSHOP) | Clone tables from `{chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema}` |

**This Prompt Uses Approach C** ‚Äî we copy from `{chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema}` for real-world structure, immediate data availability, and focus on pipeline development.

### Bronze Clone Process

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        BRONZE CLONE PROCESS                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  SOURCE                           TARGET                                    ‚îÇ
‚îÇ  {chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema}     ‚Üí       {lakehouse_default_catalog}.{user_schema_prefix}_bronze           ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ  ‚îÇ amenities           ‚îÇ  CREATE  ‚îÇ amenities           ‚îÇ + CDF enabled    ‚îÇ
‚îÇ  ‚îÇ booking_updates     ‚îÇ  TABLE   ‚îÇ booking_updates     ‚îÇ + CLUSTER BY AUTO‚îÇ
‚îÇ  ‚îÇ bookings            ‚îÇ   AS     ‚îÇ bookings            ‚îÇ + Auto-optimize  ‚îÇ
‚îÇ  ‚îÇ clickstream         ‚îÇ SELECT   ‚îÇ clickstream         ‚îÇ + TBLPROPERTIES  ‚îÇ
‚îÇ  ‚îÇ countries           ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  ‚îÇ countries           ‚îÇ + COMMENTs       ‚îÇ
‚îÇ  ‚îÇ customer_support_.. ‚îÇ          ‚îÇ customer_support_.. ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ destinations        ‚îÇ          ‚îÇ destinations        ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ employees           ‚îÇ          ‚îÇ employees           ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ hosts               ‚îÇ          ‚îÇ hosts               ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ page_views          ‚îÇ          ‚îÇ page_views          ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ payments            ‚îÇ          ‚îÇ payments            ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ properties          ‚îÇ          ‚îÇ properties          ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ property_amenities  ‚îÇ          ‚îÇ property_amenities  ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ property_images     ‚îÇ          ‚îÇ property_images     ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ reviews             ‚îÇ          ‚îÇ reviews             ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ users               ‚îÇ          ‚îÇ users               ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ                                     (all tables)                      ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Table Properties (Best Practices Enabled)

| Property | Setting | Why It Matters |
|----------|---------|----------------|
| **Liquid Clustering** | ‚úÖ `CLUSTER BY AUTO` | Automatic data layout optimization |
| **Change Data Feed** | ‚úÖ `delta.enableChangeDataFeed = true` | Enables incremental Silver processing |
| **Auto Optimize** | ‚úÖ `delta.autoOptimize.optimizeWrite = true` | Automatic file compaction |
| **Auto Compact** | ‚úÖ `delta.autoOptimize.autoCompact = true` | Reduces small files |

### Tables Cloned from {chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema}

| Table | Description | Columns |
|-------|-------------|---------|
| `amenities` | Property amenities (Wi-Fi, Pool, etc.) | 4 |
| `booking_updates` | Change log for booking modifications | 11 |
| `bookings` | Guest booking records | 10 |
| `clickstream` | User click behavior events | 5 |
| `countries` | Country reference data | 3 |
| `customer_support_logs` | Support ticket records | 5 |
| `destinations` | Travel destinations | 6 |
| `employees` | Company employee records | 10 |
| `hosts` | Property host profiles | 9 |
| `page_views` | Website page view events | 7 |
| `payments` | Payment transaction records | 6 |
| `properties` | Vacation rental listings | 13 |
| `property_amenities` | Property-to-amenity mapping (junction) | 2 |
| `property_images` | Property photo references | 6 |
| `reviews` | Guest reviews | 9 |
| `users` | Platform users (guests) | 8 |

### Verification Queries

```sql
-- 1. List all Bronze tables
SHOW TABLES IN {lakehouse_default_catalog}.{user_schema_prefix}_bronze;

-- 2. Check row counts for each table
SELECT ''amenities'' as tbl, COUNT(*) as cnt FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.amenities
UNION ALL SELECT ''booking_updates'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.booking_updates
UNION ALL SELECT ''bookings'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.bookings
UNION ALL SELECT ''clickstream'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.clickstream
UNION ALL SELECT ''countries'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.countries
UNION ALL SELECT ''customer_support_logs'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.customer_support_logs
UNION ALL SELECT ''destinations'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.destinations
UNION ALL SELECT ''employees'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.employees
UNION ALL SELECT ''hosts'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.hosts
UNION ALL SELECT ''page_views'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.page_views
UNION ALL SELECT ''payments'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.payments
UNION ALL SELECT ''properties'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.properties
UNION ALL SELECT ''property_amenities'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.property_amenities
UNION ALL SELECT ''property_images'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.property_images
UNION ALL SELECT ''reviews'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.reviews
UNION ALL SELECT ''users'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.users;

-- 3. Verify CDF is enabled (check any table)
DESCRIBE EXTENDED {lakehouse_default_catalog}.{user_schema_prefix}_bronze.bookings;
-- Look for: delta.enableChangeDataFeed = true

-- 4. Preview sample data
SELECT * FROM {lakehouse_default_catalog}.{user_schema_prefix}_bronze.bookings LIMIT 5;
```

---

## 3Ô∏è‚É£ Why Are We Building It This Way? (Databricks Best Practices)

| Practice | How It''s Used Here |
|----------|-------------------|
| **Change Data Feed (CDF)** | `delta.enableChangeDataFeed = true` on every Bronze table ‚Äî enables Silver to read only changed rows instead of full table scans |
| **Liquid Clustering** | `CLUSTER BY AUTO` ‚Äî Databricks automatically chooses optimal clustering columns and reorganizes data layout over time |
| **Auto-Optimize** | `delta.autoOptimize.optimizeWrite = true` + `autoCompact = true` ‚Äî automatic small file compaction, no manual OPTIMIZE needed |
| **Unity Catalog Governance** | All tables registered in Unity Catalog with proper catalog.schema.table naming, enabling lineage, access control, and discovery |
| **Schema-on-Read with Evolution** | Bronze preserves raw source schema; downstream layers handle schema evolution gracefully |
| **Databricks Asset Bundles (DAB)** | Infrastructure as Code ‚Äî `databricks.yml` defines jobs, targets, and resources. Deploy with `databricks bundle deploy` for repeatable, CI/CD-ready deployments |
| **Serverless Jobs** | Jobs run on serverless compute ‚Äî no cluster management, instant startup, pay-per-use cost model |
| **Enterprise Naming Standards** | Tables follow `{schema}.{table_name}` convention; COMMENTs applied to tables and columns for data discovery |

---

## 4Ô∏è‚É£ What Happens Behind the Scenes?

When you paste the prompt, the AI reads `@data_product_accelerator/skills/bronze/00-bronze-layer-setup/SKILL.md` ‚Äî the **Bronze orchestrator skill**. Behind the scenes:

1. **Orchestrator reads approach** ‚Äî detects "Approach C" and activates the clone-from-source workflow
2. **Common skills auto-loaded** ‚Äî the orchestrator''s mandatory dependencies include:
   - `databricks-table-properties` ‚Äî ensures CDF, liquid clustering, auto-optimize are set
   - `databricks-asset-bundles` ‚Äî generates proper `databricks.yml` and job YAML
   - `naming-tagging-standards` ‚Äî applies enterprise naming conventions and COMMENTs
   - `schema-management-patterns` ‚Äî handles `CREATE SCHEMA IF NOT EXISTS`
   - `databricks-python-imports` ‚Äî handles shared code modules between notebooks
3. **Code generation** ‚Äî the skill produces clone scripts that read from `{chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema}` and write to your catalog with all best practices applied
4. **Deploy loop** ‚Äî if deployment fails, the `databricks-autonomous-operations` skill kicks in for self-healing (deploy ‚Üí poll ‚Üí diagnose ‚Üí fix ‚Üí redeploy)',
'## Expected Deliverables

### üìÅ Generated Asset Bundle Structure

```
project_root/
‚îú‚îÄ‚îÄ databricks.yml                      # Bundle configuration (updated)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ source_bronze/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ clone_samples.py            # Code to copy sample data
‚îî‚îÄ‚îÄ resources/
    ‚îî‚îÄ‚îÄ bronze/
        ‚îî‚îÄ‚îÄ bronze_clone_job.yml        # Job configuration
```

---

### ‚úÖ Success Criteria Checklist

**Bundle Deployment:**
- [ ] `databricks bundle validate` passes with no errors
- [ ] `databricks bundle deploy` completes successfully
- [ ] Job appears in Databricks Workflows UI

**Job Execution:**
- [ ] Bronze clone job runs without errors
- [ ] All tables cloned successfully
- [ ] Job completes in < 10 minutes

**Table Verification:**
- [ ] All tables visible in Unity Catalog
- [ ] Row counts match source tables
- [ ] CDF enabled on all tables
- [ ] Liquid clustering enabled
- [ ] Sample data looks correct',
true, 1, true, current_timestamp(), current_timestamp(), current_user());

-- Step 11: Silver Layer Pipelines (SDP with Centralized DQ Rules) - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(8, 'silver_layer_sdp',
'Set up the Silver layer using @data_product_accelerator/skills/silver/00-silver-layer-setup/SKILL.md

This will involve the following steps:

- **Generate SDP pipeline notebooks** ‚Äî create Spark Declarative Pipeline notebooks with incremental ingestion from Bronze using Change Data Feed (CDF)
- **Create centralized DQ rules table** ‚Äî build a configurable data quality rules table with expectations (null checks, range validation, referential integrity)
- **Create Asset Bundle** ‚Äî generate bundle configuration for both the DQ rules setup job and the SDP pipeline
- **Deploy and run in order** ‚Äî deploy the bundle, run the DQ rules setup job FIRST (creates the rules table), then run the SDP pipeline (reads rules from the table)

Ensure bundle is validated and deployed successfully, and silver layer jobs run with no errors.

Validate the results in the UI to ensure the DQ rules show up in centralized delta table, and that the silver layer pipeline runs successfully with Expectations being checked.

IMPORTANT: Use the EXISTING catalog `{lakehouse_default_catalog}` -- do NOT create a new catalog. Create the Silver schema `{user_schema_prefix}_silver` and all Silver tables inside this catalog.

NOTE: Before creating the schema, check if `{lakehouse_default_catalog}.{user_schema_prefix}_silver` already exists. If it does, DROP the schema with CASCADE and recreate it from scratch. These are user-specific schemas so dropping is safe.',
'',
'Silver Layer Pipelines (SDP)',
'Create Silver layer using Spark Declarative Pipelines with centralized data quality rules',
11,
'## 1Ô∏è‚É£ How To Apply

Copy the prompt above, start a **new Agent chat** in Cursor, and paste it. The AI will read the Silver setup skill and generate the implementation.

### Prerequisite

**Run this in your cloned Template Repository** (see Prerequisites in Step 0).

Ensure you have:
- ‚úÖ Bronze layer created and populated (Step 10 complete)
- ‚úÖ `data_product_accelerator/skills/silver/00-silver-layer-setup/SKILL.md` - The Silver layer setup skill (loads worker skills automatically)

### Steps to Apply

**Step 1: Generate Silver Layer Code**

1. **Start a new Agent thread** in Cursor
2. **Copy the prompt** using the copy button
3. **Paste it into Cursor** and let the AI generate:
   - SDP pipeline notebooks
   - Data quality rules configuration
   - Asset Bundle job definitions

**Step 2: Validate the Bundle**

```bash
# Validate bundle configuration
databricks bundle validate -t dev

# Expected: No errors, all resources validated
```

**Step 3: Deploy the Bundle**

```bash
# Deploy to Databricks workspace
databricks bundle deploy -t dev

# Expected: Pipeline and jobs created successfully
```

**Step 4: Run DQ Rules Setup Job FIRST ‚ö†Ô∏è**

**CRITICAL: You must create the DQ rules table before running the pipeline ‚Äî otherwise the pipeline fails with `Table or view not found: dq_rules`.**

```bash
# Run the DQ rules setup job (creates and populates dq_rules table)
databricks bundle run -t dev silver_dq_setup_job

# Verify the rules table was created:
# SELECT * FROM {lakehouse_default_catalog}.{user_schema_prefix}_silver.dq_rules
```

**Step 5: Run the Silver DLT Pipeline**

```bash
# NOW run the DLT pipeline (it reads rules from the dq_rules table)
databricks bundle run -t dev silver_dlt_pipeline

# Or trigger from Databricks UI:
# Workflows ‚Üí DLT Pipelines ‚Üí [dev] Silver Layer Pipeline ‚Üí Start
```

**Step 6: Validate Results in UI**

After pipeline completes, verify in Databricks UI:

1. **Check DQ Rules Table:**
   ```sql
   SELECT * FROM {lakehouse_default_catalog}.{user_schema_prefix}_silver.dq_rules;
   ```
   ‚úÖ Should show all configured quality rules

2. **Check Pipeline Event Log:**
   - Navigate to: Workflows ‚Üí DLT Pipelines ‚Üí Your Pipeline
   - Click "Data Quality" tab
   - ‚úÖ Should show Expectations being evaluated

3. **Check Silver Tables:**
   ```sql
   SHOW TABLES IN {lakehouse_default_catalog}.{user_schema_prefix}_silver;
   SELECT * FROM {lakehouse_default_catalog}.{user_schema_prefix}_silver.{table} LIMIT 10;
   ```
   ‚úÖ Should show cleaned, validated data

---

## 2Ô∏è‚É£ What Are We Building?

### What is the Silver Layer?

The Silver Layer transforms raw Bronze data into **cleaned, validated, and enriched** data ready for Gold layer consumption.

### Core Philosophy: Schema Cloning

Silver should **mirror the Bronze schema** with minimal changes ‚Äî same column names, same data types, same grain. The value-add is **data quality**, not transformation:

| ‚úÖ DO in Silver | ‚ùå DON''T do in Silver (save for Gold) |
|----------------|--------------------------------------|
| Apply DQ rules (null checks, range validation) | Aggregation (SUM, COUNT, GROUP BY) |
| Add derived flags (`is_return`, `is_out_of_stock`) | Join across tables |
| Add business keys (SHA256 hashes) | Complex business logic |
| Add `processed_timestamp` | Schema restructuring |
| Deduplicate records | Rename columns significantly |

**Why?** Silver is the validated copy of source data. Gold handles complex transformations. This keeps Silver focused on data quality and makes troubleshooting easier (column names match source).

### Why Spark Declarative Pipelines (SDP)?

| Feature | Benefit |
|---------|---------|
| **Incremental Ingestion** | Reads only changed data from Bronze using Change Data Feed (CDF) |
| **Built-in Quality Rules** | Expectations framework for data validation |
| **Serverless Compute** | Cost-efficient, auto-scaling execution |
| **Automatic Schema Evolution** | Handles schema changes gracefully |
| **Complete Lineage** | Full data lineage tracking in Unity Catalog |
| **Photon Engine** | Vectorized query execution for faster processing |

### Key Validation Points

| What to Check | Where | Expected Result |
|---------------|-------|-----------------|
| DQ Rules loaded | `dq_rules` table | Rules visible in Delta table |
| Expectations running | Pipeline event log | Pass/Warn/Fail counts shown |
| Data quality | Silver tables | Clean, standardized data |
| Incremental working | Pipeline metrics | Only new/changed rows processed |

---

## 3Ô∏è‚É£ Why Are We Building It This Way? (Databricks Best Practices)

| Practice | How It''s Used Here |
|----------|-------------------|
| **Spark Declarative Pipelines (SDP/DLT)** | Silver uses SDP for declarative, streaming-first pipelines ‚Äî define WHAT the data should look like, not HOW to process it |
| **Legacy `import dlt` API** | Uses `import dlt` (not modern `pyspark.pipelines`) because the DQ rules framework depends on `@dlt.expect_all_or_drop()` decorators. Will migrate when Databricks ports expectations to the modern API. |
| **CDF-Based Incremental Reads** | Silver reads from Bronze using Change Data Feed ‚Äî only processing new/changed rows, not full table scans |
| **Expectations Framework** | DLT Expectations with severity levels: `@dlt.expect_all()` (warn but keep), `@dlt.expect_all_or_drop()` (quarantine bad rows), `@dlt.expect_or_fail()` (halt pipeline ‚Äî avoided in favor of drop) |
| **Centralized DQ Rules in Delta Tables** | Quality rules stored in `dq_rules` Delta table ‚Äî updateable at runtime via SQL without code redeployment. PK constraint on `(table_name, rule_name)`. |
| **Quarantine Pattern** | Records failing critical DQ rules are routed to quarantine tables for investigation, not silently dropped |
| **Row Tracking** | `delta.enableRowTracking = true` on EVERY Silver table ‚Äî required for downstream Gold Materialized Views to use incremental refresh instead of expensive full recomputation |
| **Photon + ADVANCED Edition** | `photon: true` and `edition: ADVANCED` are non-negotiable in pipeline YAML ‚Äî Photon for vectorized execution, ADVANCED for expectations/CDC support |
| **Serverless DLT Compute** | `serverless: true` in pipeline YAML ‚Äî auto-scaling, no cluster configuration, no `clusters:` block |
| **Schema Cloning Philosophy** | Silver mirrors Bronze schema (same column names, same grain, no aggregation, no joins). Only adds: DQ rules, derived flags, business keys, `processed_timestamp`. Aggregation belongs in Gold. |
| **Unity Catalog Integration** | Silver tables are UC-managed, inheriting governance, lineage tracking, and access controls from Bronze |
| **Pure Python DQ Loader** | `dq_rules_loader.py` has NO notebook header ‚Äî it''s a pure Python module importable by DLT notebooks. Cache pattern uses `toPandas()` (not `.collect()`) for performance. |
| **2-Job Deployment Pattern** | Two separate resources: (1) `silver_dq_setup_job` ‚Äî regular job that creates and populates the `dq_rules` table, (2) `silver_dlt_pipeline` ‚Äî DLT pipeline that reads rules from the table. Setup job MUST run first. |
| **Data Quality Monitoring** | DQ monitoring views created inside the DLT pipeline ‚Äî per-table metrics, referential integrity checks, data freshness. Feeds into observability dashboards in later steps. |

---

## 4Ô∏è‚É£ What Happens Behind the Scenes?

When you paste the prompt, the AI reads `@data_product_accelerator/skills/silver/00-silver-layer-setup/SKILL.md` ‚Äî the **Silver orchestrator skill**. Behind the scenes:

1. **Orchestrator activates** ‚Äî reads the Silver setup workflow with streaming ingestion and DQ rules
2. **Worker skills auto-loaded:**
   - `01-dlt-expectations-patterns` ‚Äî creates portable DQ rules stored in Unity Catalog Delta tables (not hardcoded in notebooks)
   - `02-dqx-patterns` ‚Äî Databricks DQX framework for advanced validation with detailed failure diagnostics
3. **Common skills auto-loaded (8 total):**
   - `databricks-expert-agent` ‚Äî core "Extract, Don''t Generate" principle
   - `databricks-table-properties` ‚Äî ensures proper TBLPROPERTIES (CDF, row tracking, auto-optimize)
   - `databricks-asset-bundles` ‚Äî generates DLT pipeline YAML and DQ setup job YAML
   - `databricks-python-imports` ‚Äî ensures `dq_rules_loader.py` is pure Python (no notebook header)
   - `unity-catalog-constraints` ‚Äî PK constraint on `dq_rules` table: `(table_name, rule_name)`
   - `schema-management-patterns` ‚Äî `CREATE SCHEMA IF NOT EXISTS` with governance metadata
   - `naming-tagging-standards` ‚Äî enterprise naming conventions and dual-purpose COMMENTs
   - `databricks-autonomous-operations` ‚Äî self-healing deploy loop if pipeline fails
4. **Key innovation: Runtime-updateable DQ rules** ‚Äî expectations are stored in a Delta table, not in code. You can update rules without redeploying the pipeline.',
'## Expected Deliverables

### üìÅ Generated Files

```
project_root/
‚îú‚îÄ‚îÄ databricks.yml                        # Updated with Silver resources
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ source_silver/
‚îÇ       ‚îú‚îÄ‚îÄ setup_dq_rules_table.py       # Notebook: Create & populate DQ rules Delta table
‚îÇ       ‚îú‚îÄ‚îÄ dq_rules_loader.py            # Pure Python module (NO notebook header!)
‚îÇ       ‚îú‚îÄ‚îÄ silver_dimensions.py          # DLT notebook: Dimension tables
‚îÇ       ‚îú‚îÄ‚îÄ silver_facts.py               # DLT notebook: Fact tables with quarantine
‚îÇ       ‚îî‚îÄ‚îÄ data_quality_monitoring.py    # DLT notebook: DQ metrics & freshness views
‚îî‚îÄ‚îÄ resources/
    ‚îî‚îÄ‚îÄ silver/
        ‚îú‚îÄ‚îÄ silver_dq_setup_job.yml       # Job: Creates dq_rules table (run FIRST)
        ‚îî‚îÄ‚îÄ silver_dlt_pipeline.yml       # DLT pipeline configuration
```

> **Key file note:** `dq_rules_loader.py` must be a **pure Python module** (no `# Databricks notebook source` header). This is because DLT notebooks import it as a regular module. If it has a notebook header, imports break.

---

### üîÑ Silver Layer Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        SILVER LAYER (SDP Pipeline)                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ   Bronze    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  SDP Pipeline    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Silver Tables     ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ   Tables    ‚îÇ    ‚îÇ  (Serverless)    ‚îÇ    ‚îÇ   (Cleaned Data)    ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ   (CDF)     ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ                     ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  ‚Ä¢ Read CDF      ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                     ‚îÇ  ‚Ä¢ Apply DQ      ‚îÇ              ‚îÇ                     ‚îÇ
‚îÇ                     ‚îÇ  ‚Ä¢ Transform     ‚îÇ              ‚ñº                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ  ‚Ä¢ Deduplicate   ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ  DQ Rules   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ                  ‚îÇ    ‚îÇ  Quarantine Table   ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  (Delta)    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  (Failed Records)   ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### üìä Tables Created

| Table Type | Tables | Description |
|------------|--------|-------------|
| **Silver Dimensions** | `silver_amenities`, `silver_destinations`, ... | Cleaned dimension data mirroring Bronze schema |
| **Silver Facts** | `silver_bookings`, `silver_payments`, ... | Transformed fact data with DQ expectations applied |
| **DQ Rules** | `dq_rules` | Centralized rule definitions (PK: `table_name, rule_name`) |
| **Quarantine** | `quarantine_*` | Records that failed `expect_all_or_drop` critical rules |
| **DQ Monitoring Views** | `dq_metrics_*`, `data_freshness_*` | Per-table quality metrics and freshness tracking |

---

### ‚úÖ Data Quality Framework

| Quality Dimension | Example Rules |
|-------------------|---------------|
| **Completeness** | Required fields not null |
| **Validity** | Values within expected ranges |
| **Uniqueness** | No duplicates on key columns |
| **Consistency** | Cross-field validations |
| **Referential** | Foreign keys exist in parent tables |

---

### üñºÔ∏è Visual Validation in Databricks

**1. DLT Pipeline - Data Quality Tab:**

Shows Expectations results with Pass/Warn/Fail counts for each rule.

**2. Unity Catalog - Silver Schema:**

All Silver tables visible with proper metadata and lineage.

**3. DQ Rules Table:**

```sql
SELECT rule_name, rule_type, expectation, action 
FROM {lakehouse_default_catalog}.{user_schema_prefix}_silver.dq_rules;
```

---

### ‚úÖ Success Criteria Checklist

**Deployment:**
- [ ] Bundle validates with no errors
- [ ] Bundle deploys successfully
- [ ] DQ rules setup job runs and creates `dq_rules` table (**must run FIRST**)
- [ ] DLT pipeline runs without failures

**Data Quality:**
- [ ] DQ rules loaded into centralized Delta table
- [ ] Expectations show in pipeline event log (Data Quality tab)
- [ ] Quarantine table captures failed records (not silently dropped)

**Tables & Properties:**
- [ ] Silver tables populated with cleaned data
- [ ] Silver column names match Bronze (schema cloning)
- [ ] Row tracking enabled (`delta.enableRowTracking = true`)
- [ ] CDF enabled (`delta.enableChangeDataFeed = true`)
- [ ] `cluster_by_auto=True` on every table

**Pipeline Configuration:**
- [ ] `serverless: true` in pipeline YAML
- [ ] `photon: true` in pipeline YAML
- [ ] `edition: ADVANCED` in pipeline YAML
- [ ] `dq_rules_loader.py` has NO notebook header (pure Python)
- [ ] Incremental processing working (only new/changed rows)',
true, 1, true, current_timestamp(), current_timestamp(), current_user());

-- Step 12: Gold Layer Pipeline (YAML-Driven Implementation) - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(9, 'gold_layer_pipeline',
'Implement the Gold layer using @data_product_accelerator/skills/gold/01-gold-layer-setup/SKILL.md

This will involve the following steps:

- **Read YAML schemas** ‚Äî use the Gold layer design YAML files (from Step 9) as the single source of truth for all table definitions, columns, and constraints
- **Create Gold tables** ‚Äî generate CREATE TABLE DDL from YAML, add PRIMARY KEY constraints, then add FOREIGN KEY constraints (NOT ENFORCED) in dependency order
- **Merge data from Silver** ‚Äî deduplicate Silver records before MERGE, map columns using YAML lineage metadata, merge dimensions first (SCD1/SCD2) then facts (FK dependency order)
- **Deploy 2-job architecture** ‚Äî gold_setup_job (2 tasks: create tables + add FK constraints) and gold_merge_job (populate data from Silver)
- **Validate results** ‚Äî verify table creation, PK/FK constraints, row counts, SCD2 history, and fact-dimension joins

Use the gold layer design YAML files as the target destination, and the silver layer tables as source.

Limit pipelines to only 5 core tables for purposes of this exercise.

IMPORTANT: Use the EXISTING catalog `{lakehouse_default_catalog}` -- do NOT create a new catalog. Create the Gold schema `{user_schema_prefix}_gold` and all Gold tables inside this catalog.

NOTE: Before creating the schema, check if `{lakehouse_default_catalog}.{user_schema_prefix}_gold` already exists. If it does, DROP the schema with CASCADE and recreate it from scratch. These are user-specific schemas so dropping is safe.',
'',
'Gold Layer Pipeline (YAML-Driven)',
'Build Gold layer by reading YAML schemas, creating tables with PK/FK constraints (NOT ENFORCED), and merging from Silver with deduplication',
12,
'## 1Ô∏è‚É£ How To Apply

Copy the prompt above, start a **new Agent chat** in Cursor, and paste it. The AI will read YAML files and generate implementation code.

### Prerequisite

**Run this in your cloned Template Repository** (see Prerequisites in Step 0).

Ensure you have:
- ‚úÖ Gold Layer Design completed (Step 9) with YAML files in `gold_layer_design/yaml/`
- ‚úÖ Column lineage documentation in `gold_layer_design/COLUMN_LINEAGE.csv` (Silver‚ÜíGold column mappings)
- ‚úÖ Silver Layer populated (Step 11) with data in Silver tables
- ‚úÖ `data_product_accelerator/skills/gold/01-gold-layer-setup/SKILL.md` ‚Äî The Gold implementation orchestrator (auto-loads 7 worker + 8 common skills)

### Steps to Apply

**Step 1: Start New Agent Thread** ‚Äî Open Cursor and start a new Agent thread for clean context.

**Step 2: Copy and Paste the Prompt** ‚Äî Copy the prompt using the copy button, paste it into Cursor. The AI will read YAML files and generate implementation code.

**Step 3: Review Generated Code** ‚Äî The AI will create:
- `setup_tables.py` ‚Äî reads YAML ‚Üí CREATE TABLE + PKs
- `add_fk_constraints.py` ‚Äî reads YAML ‚Üí ALTER TABLE ADD FK (NOT ENFORCED)
- `merge_gold_tables.py` ‚Äî dedup Silver ‚Üí map columns ‚Üí MERGE (SCD1/SCD2/fact)
- `gold_setup_job.yml` ‚Äî 2-task job (setup ‚Üí FK via `depends_on`)
- `gold_merge_job.yml` ‚Äî merge job (scheduled, PAUSED in dev)

**Step 4: Validate the Bundle**

```bash
# Validate bundle configuration
databricks bundle validate -t dev

# Expected: No errors, all resources validated
```

**Step 5: Deploy the Bundle**

```bash
# Deploy to Databricks workspace
databricks bundle deploy -t dev

# Expected: Jobs created successfully
```

**Step 6: Run the Gold Setup Job (Tables + PKs + FKs)**

```bash
# Run Gold setup (creates tables, adds PKs, then adds FKs)
databricks bundle run -t dev gold_setup_job

# This job has TWO tasks:
#   Task 1: setup_tables (creates tables from YAML + adds PKs)
#   Task 2: add_fk_constraints (depends_on Task 1)
#
# FKs are added here (before data) because UC constraints are
# NOT ENFORCED ‚Äî they''re informational only, no data validation needed.
```

**Step 7: Run the Gold Merge Job**

```bash
# Run Gold merge (populates tables from Silver)
databricks bundle run -t dev gold_merge_job

# Merges dimensions FIRST (SCD1/SCD2), then facts (FK dependency order)
```

**Step 8: Verify in Databricks UI**

After all jobs complete:

```sql
-- 1. List Gold tables
SHOW TABLES IN {lakehouse_default_catalog}.{user_schema_prefix}_gold;

-- 2. Check Primary Key constraints
SELECT * FROM information_schema.table_constraints 
WHERE table_schema = ''{user_schema_prefix}_gold'' AND constraint_type = ''PRIMARY KEY'';

-- 3. Check Foreign Key constraints
SELECT * FROM information_schema.table_constraints 
WHERE table_schema = ''{user_schema_prefix}_gold'' AND constraint_type = ''FOREIGN KEY'';

-- 4. Verify row counts
SELECT ''dim_property'' as tbl, COUNT(*) as cnt FROM {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_property
UNION ALL SELECT ''dim_destination'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_destination
UNION ALL SELECT ''dim_user'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_user
UNION ALL SELECT ''dim_host'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_host
UNION ALL SELECT ''fact_booking_detail'', COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_gold.fact_booking_detail;

-- 5. Preview fact with dimension lookups
SELECT 
    f.booking_id,
    p.property_name,
    d.destination_name,
    u.first_name || '' '' || u.last_name as guest_name,
    f.total_amount
FROM {lakehouse_default_catalog}.{user_schema_prefix}_gold.fact_booking_detail f
JOIN {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_property p ON f.property_id = p.property_id AND p.is_current = true
JOIN {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_destination d ON f.destination_id = d.destination_id
JOIN {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_user u ON f.user_id = u.user_id AND u.is_current = true
LIMIT 5;
```

---

## 2Ô∏è‚É£ What Are We Building?

### üìö What is the Gold Layer Pipeline?

The Gold Layer Pipeline **implements** the Gold Layer Design by:
1. Reading YAML schema files (single source of truth)
2. Creating dimension and fact tables with proper constraints
3. Merging data incrementally from Silver layer

### Design vs Implementation

| Step | What Happens | Output |
|------|--------------|--------|
| **Step 9: Design** | Define schemas, ERDs, lineage | `gold_layer_design/` folder |
| **Step 12: Implementation** | Create tables, run merges | Populated Gold tables |

### üéØ Core Philosophy: Extract, Don''t Generate

**ALWAYS prefer scripting techniques to extract names from existing source files over generating them from scratch.**

| Approach | Result |
|----------|--------|
| ‚ùå **Generate from scratch** | Hallucinations, typos, schema mismatches |
| ‚úÖ **Extract from YAML** | 100% accuracy, consistency, no hallucinations |

### What "Extract" Means

```python
# ‚ùå WRONG: Hardcode table names (might be wrong!)
tables = ["dim_property", "dim_destination", "fact_booking"]

# ‚úÖ CORRECT: Extract from YAML files
import yaml
from pathlib import Path

def get_gold_table_names():
    yaml_dir = Path("gold_layer_design/yaml")
    tables = []
    for yaml_file in yaml_dir.rglob("*.yaml"):
        with open(yaml_file) as f:
            config = yaml.safe_load(f)
            tables.append(config[''table_name''])
    return tables
```

**Benefits:**
- ‚úÖ 100% accuracy (names come from actual schemas)
- ‚úÖ No hallucinations (only existing entities referenced)
- ‚úÖ Consistency across layers
- ‚úÖ Immediate detection of schema changes

### üèóÔ∏è Gold Layer Pipeline Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      GOLD LAYER PIPELINE FLOW (2 Jobs)                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  INPUTS                          PROCESS                       OUTPUT      ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Gold Layer      ‚îÇ   ‚îÇ gold_setup_job                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Design YAML     ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ                                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (Schema Source) ‚îÇ   ‚îÇ  Task 1: setup_tables.py                     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ    ‚Ä¢ CREATE TABLE from YAML                  ‚îÇ  ‚îÇ
‚îÇ                         ‚îÇ    ‚Ä¢ ALTER TABLE ADD PRIMARY KEY              ‚îÇ  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ         ‚Üì depends_on                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ COLUMN_LINEAGE  ‚îÇ   ‚îÇ  Task 2: add_fk_constraints.py               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ .csv            ‚îÇ   ‚îÇ    ‚Ä¢ ALTER TABLE ADD FOREIGN KEY (NOT ENFORCED)‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                          ‚Üì                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Silver Layer    ‚îÇ   ‚îÇ gold_merge_job                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Tables          ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ                                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (Data Source)   ‚îÇ   ‚îÇ  1. Deduplicate Silver (business_key)         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ  2. Map columns (YAML lineage / CSV)         ‚îÇ  ‚îÇ
‚îÇ                         ‚îÇ  3. Merge dims first (SCD1/SCD2)             ‚îÇ  ‚îÇ
‚îÇ                         ‚îÇ  4. Merge facts last (FK order)              ‚îÇ  ‚îÇ
‚îÇ                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üéØ Workshop Scope: 5 Tables

For this exercise, we limit to **5 key tables**:

| Table | Type | Description |
|-------|------|-------------|
| `dim_property` | Dimension (SCD2) | Vacation rental property details |
| `dim_destination` | Dimension (SCD1) | Travel destinations/locations |
| `dim_user` | Dimension (SCD2) | Platform users (guests) |
| `dim_host` | Dimension (SCD2) | Property host profiles |
| `fact_booking_detail` | Fact | Individual booking transactions |

**Why 5 tables?**
- ‚úÖ Demonstrates all patterns (SCD1, SCD2, Fact)
- ‚úÖ Shows FK relationships (Fact ‚Üí Dimensions)
- ‚úÖ Completes in reasonable time for workshop
- ‚úÖ Full pattern coverage without complexity overload

### üîë Constraint Application Order

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  gold_setup_job (2 tasks)                                  ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ  Task 1: setup_tables.py                                   ‚îÇ
‚îÇ    ‚Ä¢ CREATE OR REPLACE TABLE ... (from YAML)               ‚îÇ
‚îÇ    ‚Ä¢ ALTER TABLE ... ADD CONSTRAINT pk_ PRIMARY KEY        ‚îÇ
‚îÇ           ‚Üì (depends_on)                                   ‚îÇ
‚îÇ  Task 2: add_fk_constraints.py                             ‚îÇ
‚îÇ    ‚Ä¢ ALTER TABLE ... ADD CONSTRAINT fk_ FOREIGN KEY        ‚îÇ
‚îÇ    ‚Ä¢ FK references PK ‚Üí PK must exist first                ‚îÇ
‚îÇ                                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  gold_merge_job                                            ‚îÇ
‚îÇ    ‚Ä¢ Merge dimensions first (SCD1/SCD2)                    ‚îÇ
‚îÇ    ‚Ä¢ Merge facts last (FK dependency order)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### ‚ö†Ô∏è Why FKs BEFORE Data?

Unity Catalog constraints are **NOT ENFORCED** ‚Äî they are **informational only**:
- They do NOT reject invalid data on INSERT/MERGE
- They DO tell BI tools (Genie, Power BI, Tableau) how tables relate
- They DO improve query optimizer join planning
- Data does NOT need to exist for constraints to be applied

This is a key Databricks concept: PK/FK in Unity Catalog are for **metadata enrichment and BI tool discovery**, not data integrity enforcement.

---

## 3Ô∏è‚É£ Why Are We Building It This Way? (Databricks Best Practices)

| Practice | How It''s Used Here |
|----------|-------------------|
| **Surrogate Keys as PRIMARY KEYs** | Dimensions use surrogate BIGINT keys (not business keys) as PKs ‚Äî informational constraints in Unity Catalog for BI tool discovery |
| **FOREIGN KEY Constraints** | Fact tables declare FK relationships to dimensions ‚Äî enables Genie, Power BI, and Tableau to auto-discover joins |
| **SCD Type 1 (Overwrite)** | Reference dimensions like `dim_destination` use SCD1 ‚Äî MERGE replaces old values with current values |
| **SCD Type 2 (Versioned History)** | Tracking dimensions like `dim_property`, `dim_host` use SCD2 ‚Äî `is_current`, `valid_from`, `valid_to` columns preserve history |
| **Delta MERGE with Deduplication** | Pre-deduplicates source rows before MERGE to prevent `DELTA_MULTIPLE_SOURCE_ROW_MATCHING_TARGET_ROW_IN_MERGE` errors. Dedup key = `business_key` from YAML. |
| **2-Job Architecture** | `gold_setup_job` (2 tasks: create tables + add FKs) ‚Üí `gold_merge_job` (populate data). FKs applied before data because constraints are NOT ENFORCED. |
| **NOT ENFORCED Constraints** | UC PK/FK are informational ‚Äî they help BI tools discover relationships and improve query planning, but don''t reject invalid data |
| **Dual-Purpose COMMENTs** | Every table and column has a COMMENT serving both business users ("Property name for display") and technical users/Genie ("FK to dim_property.property_sk") |
| **Row Tracking** | `delta.enableRowTracking = true` on every Gold table ‚Äî required for downstream Materialized View incremental refresh |
| **CLUSTER BY AUTO** | Gold tables use automatic liquid clustering ‚Äî Databricks chooses optimal columns based on actual query patterns |
| **Predictive Optimization Ready** | Gold tables are structured for Databricks Predictive Optimization ‚Äî auto-OPTIMIZE, auto-VACUUM, auto-ANALYZE |
| **YAML as Single Source of Truth** | Table schemas live in version-controlled YAML files, not in scattered SQL scripts ‚Äî enables schema diff reviews in PRs |
| **PyYAML + YAML Sync** | `pyyaml>=6.0` in job environment; YAML files synced in `databricks.yml` ‚Äî without sync, `setup_tables.py` can''t find schemas in workspace |
| **Variable Shadowing Prevention** | Never name variables `count`, `sum`, `min`, `max` ‚Äî shadows PySpark functions. Use `spark_sum = F.sum`, `record_count = df.count()` |
| **Column Mapping from Lineage** | Silver‚ÜíGold column renames extracted from YAML `lineage.source_column` or `COLUMN_LINEAGE.csv` ‚Äî never guessed or assumed |

---

## 4Ô∏è‚É£ What Happens Behind the Scenes?

When you paste the prompt, the AI reads `@data_product_accelerator/skills/gold/01-gold-layer-setup/SKILL.md` ‚Äî the **Gold implementation orchestrator**. Behind the scenes:

1. **YAML-driven approach** ‚Äî the orchestrator reads your `gold_layer_design/yaml/` files (from Step 9) as the **single source of truth**. Table names, columns, types, PKs, FKs are all extracted from YAML ‚Äî never generated from scratch.
2. **Pipeline worker skills auto-loaded:**
   - `01-yaml-table-setup` ‚Äî reads YAML schemas and generates CREATE TABLE DDL with PKs
   - `02-merge-patterns` ‚Äî SCD Type 1/2 dimensions, fact table MERGE operations
   - `03-deduplication` ‚Äî prevents DELTA_MULTIPLE_SOURCE_ROW_MATCHING errors by deduplicating Silver before MERGE
   - `04-grain-validation` ‚Äî validates grain before populating fact tables
   - `05-schema-validation` ‚Äî validates schemas before deployment
3. **Common skills auto-loaded (8 total):**
   - `databricks-expert-agent` ‚Äî core "Extract, Don''t Generate" principle applied to EVERY YAML read
   - `databricks-asset-bundles` ‚Äî generates 2 jobs (setup+FK combined, merge separate), `notebook_task` + `base_parameters`
   - `databricks-table-properties` ‚Äî Gold TBLPROPERTIES (CDF, row tracking, auto-optimize, `layer=gold`)
   - `unity-catalog-constraints` ‚Äî surrogate keys as PKs (NOT NULL), FK via ALTER TABLE (NOT ENFORCED)
   - `schema-management-patterns` ‚Äî `CREATE SCHEMA IF NOT EXISTS` with governance metadata
   - `databricks-python-imports` ‚Äî pure Python modules for shared config (avoids `sys.path` issues)
   - `naming-tagging-standards` ‚Äî enterprise naming and dual-purpose COMMENTs
   - `databricks-autonomous-operations` ‚Äî self-healing deploy loop if jobs fail

**Key principle: "Extract, Don''t Generate"** ‚Äî every table name, column name, and type comes from YAML. The AI never hallucinates schema elements.',
'## Expected Deliverables

### üìÅ Generated Asset Bundle Structure

```
project_root/
‚îú‚îÄ‚îÄ databricks.yml                          # Bundle config (MUST sync YAML files!)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ source_gold/
‚îÇ       ‚îú‚îÄ‚îÄ setup_tables.py                 # Creates Gold tables from YAML + adds PKs
‚îÇ       ‚îú‚îÄ‚îÄ add_fk_constraints.py           # Adds FK constraints (separate script)
‚îÇ       ‚îî‚îÄ‚îÄ merge_gold_tables.py            # Merges Silver ‚Üí Gold (dedup + map + merge)
‚îú‚îÄ‚îÄ resources/
‚îÇ   ‚îî‚îÄ‚îÄ gold/
‚îÇ       ‚îú‚îÄ‚îÄ gold_setup_job.yml              # 2 tasks: setup_tables ‚Üí add_fk_constraints
‚îÇ       ‚îî‚îÄ‚îÄ gold_merge_job.yml              # Merge job (scheduled, PAUSED in dev)
‚îî‚îÄ‚îÄ gold_layer_design/                      # Source of truth (from Step 9 design)
    ‚îú‚îÄ‚îÄ COLUMN_LINEAGE.csv                  # Silver‚ÜíGold column mappings
    ‚îî‚îÄ‚îÄ yaml/
        ‚îú‚îÄ‚îÄ property/
        ‚îÇ   ‚îú‚îÄ‚îÄ dim_property.yaml
        ‚îÇ   ‚îî‚îÄ‚îÄ dim_destination.yaml
        ‚îú‚îÄ‚îÄ user/
        ‚îÇ   ‚îú‚îÄ‚îÄ dim_user.yaml
        ‚îÇ   ‚îî‚îÄ‚îÄ dim_host.yaml
        ‚îî‚îÄ‚îÄ booking/
            ‚îî‚îÄ‚îÄ fact_booking_detail.yaml
```

> **Critical:** `databricks.yml` must include a sync rule for `gold_layer_design/yaml/**/*.yaml` ‚Äî without it, the scripts can''t find YAML schemas in the workspace. The environment must also include `pyyaml>=6.0`.

---

### üîÑ What Each Script Does

#### `setup_tables.py` - Table Creation

```python
# Reads YAML ‚Üí Generates DDL ‚Üí Creates Tables
for yaml_file in gold_yaml_files:
    config = yaml.safe_load(yaml_file)
    
    # Extract schema from YAML (don''t hardcode!)
    table_name = config[''table_name'']
    columns = config[''columns'']
    primary_key = config[''primary_key'']
    
    # Generate and execute DDL
    ddl = generate_create_table(table_name, columns, primary_key)
    spark.sql(ddl)
```

#### `merge_gold_tables.py` - Data Population

```python
# For each table: Deduplicate ‚Üí Map Columns ‚Üí Validate ‚Üí Merge
for table_name, meta in inventory.items():
    silver_df = spark.table(f"{lakehouse_default_catalog}.{user_schema_prefix}_silver.{meta[''source_table'']}")
    
    # 1. ALWAYS deduplicate Silver before MERGE (mandatory!)
    deduped_df = silver_df.orderBy(col("processed_timestamp").desc()) \
                          .dropDuplicates(meta["business_key"])
    
    # 2. Map columns (Silver names ‚Üí Gold names from COLUMN_LINEAGE.csv)
    for gold_col, silver_col in meta["column_mappings"].items():
        deduped_df = deduped_df.withColumn(gold_col, col(silver_col))
    
    # 3. Merge (SCD1 or SCD2 based on YAML scd_type)
    merge_condition = build_merge_condition(meta["pk_columns"])
    merge_into_gold(deduped_df, table_name, merge_condition, meta)

# Note: uses spark_sum = F.sum (never shadow Python builtins)
```

#### `add_fk_constraints.py` - Foreign Keys

```python
# Reads FK definitions from YAML ‚Üí Adds constraints
for yaml_file in gold_yaml_files:
    config = yaml.safe_load(yaml_file)
    
    for fk in config.get(''foreign_keys'', []):
        # Add FK constraint (NOT ENFORCED for performance)
        spark.sql(f"""
            ALTER TABLE {table_name}
            ADD CONSTRAINT fk_{table}_{ref_table}
            FOREIGN KEY ({fk_columns})
            REFERENCES {ref_table}({ref_columns})
            NOT ENFORCED
        """)
```

---

### üìä Tables Created with Constraints

| Table | Type | Primary Key | Foreign Keys |
|-------|------|-------------|--------------|
| `dim_property` | Dimension (SCD2) | `property_key` | None |
| `dim_destination` | Dimension (SCD1) | `destination_id` | None |
| `dim_user` | Dimension (SCD2) | `user_key` | None |
| `dim_host` | Dimension (SCD2) | `host_key` | None |
| `fact_booking_detail` | Fact | `booking_id` | ‚Üí dim_property, dim_destination, dim_user, dim_host |

---

### üîÄ Merge Strategies by Table Type

| Table Type | Merge Strategy | What Happens |
|------------|----------------|--------------|
| **Dimension (SCD1)** | Overwrite | Old values replaced with new |
| **Dimension (SCD2)** | Track History | Old record marked `is_current=false`, new record inserted |
| **Fact** | Upsert | INSERT new, UPDATE existing on PK match |

---

### ‚úÖ Verification Queries

After all jobs complete:

```sql
-- 1. Verify table creation
SHOW TABLES IN {lakehouse_default_catalog}.{user_schema_prefix}_gold;

-- 2. Verify Primary Key constraints
SHOW CONSTRAINTS ON {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_property;

-- 3. Verify Foreign Key constraints
SHOW CONSTRAINTS ON {lakehouse_default_catalog}.{user_schema_prefix}_gold.fact_booking_detail;

-- 4. Verify SCD2 history (multiple versions for same entity)
SELECT property_id, is_current, effective_from, effective_to
FROM {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_property
WHERE property_id = 123
ORDER BY effective_from;

-- 5. Verify non-negotiable table properties
SHOW TBLPROPERTIES {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_property;
-- Look for: delta.enableChangeDataFeed=true, delta.enableRowTracking=true,
--           delta.autoOptimize.autoCompact=true, layer=gold

-- 6. Verify SCD2: exactly one is_current=true per business key
SELECT property_id, COUNT(*) as current_versions
FROM {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_property
WHERE is_current = true
GROUP BY property_id
HAVING COUNT(*) > 1;
-- Expected: ZERO rows (any results = SCD2 bug)

-- 7. Verify fact-dimension joins work (no orphan records)
SELECT 
    f.booking_id,
    p.property_name,
    h.host_name,
    f.total_amount
FROM {lakehouse_default_catalog}.{user_schema_prefix}_gold.fact_booking_detail f
JOIN {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_property p 
    ON f.property_id = p.property_id AND p.is_current = true
JOIN {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_host h 
    ON f.host_id = h.host_id AND h.is_current = true
LIMIT 10;
```

---

### ‚úÖ Success Criteria Checklist

**Bundle Deployment:**
- [ ] `databricks bundle validate -t dev` passes (no errors)
- [ ] `databricks bundle deploy -t dev` completes
- [ ] 2 jobs appear in Workflows UI (`gold_setup_job`, `gold_merge_job`)
- [ ] YAML files synced to workspace (verify `gold_layer_design/yaml/` exists)
- [ ] PyYAML dependency present in job environment (`pyyaml>=6.0`)

**Gold Setup Job (2 tasks):**
- [ ] Task 1: All 5 tables created from YAML (no hardcoded DDL)
- [ ] Primary keys added to dimension tables (via `ALTER TABLE`)
- [ ] Task 2: Foreign key constraints added (runs after Task 1 via `depends_on`)
- [ ] Constraints visible in `information_schema.table_constraints`

**Table Properties (non-negotiable):**
- [ ] `CLUSTER BY AUTO` on every table (never specific columns)
- [ ] `delta.enableChangeDataFeed = true` (required for incremental propagation)
- [ ] `delta.enableRowTracking = true` (required for downstream MV refresh)
- [ ] `delta.autoOptimize.autoCompact = true`
- [ ] `delta.autoOptimize.optimizeWrite = true`
- [ ] `layer = gold` in TBLPROPERTIES

**Gold Merge Job:**
- [ ] Dimensions merged BEFORE facts (FK dependency order)
- [ ] Every MERGE deduplicates Silver first (key from YAML `business_key`)
- [ ] Column mappings extracted from YAML/`COLUMN_LINEAGE.csv` (not hardcoded)
- [ ] No variable names shadow PySpark functions (`count`, `sum`, etc.)
- [ ] Row counts match expectations
- [ ] SCD2 dimensions: exactly one `is_current = true` per business key
- [ ] Fact-to-dimension joins resolve correctly (no orphan records)

**Job Configuration:**
- [ ] Jobs use `notebook_task` (never `python_task`)
- [ ] Parameters use `base_parameters` dict (never CLI-style `parameters`)
- [ ] Serverless: `environments` block with `environment_version: "4"`
- [ ] Tags applied: `environment`, `layer=gold`, `job_type`',
true, 1, true, current_timestamp(), current_timestamp(), current_user());

-- Step 13: Create Use-Case Plan (Operationalization Planning) - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(10, 'usecase_plan',
'Perform project planning using @data_product_accelerator/skills/planning/00-project-planning/SKILL.md with planning_mode: workshop

This will involve the following steps:

- **Analyze Gold layer** ‚Äî examine your completed Gold tables to identify natural business domains, key relationships, and analytical questions
- **Generate use-case plans** ‚Äî create structured plans organized as Phase 1 addendums (1.2 TVFs, 1.3 Metric Views, 1.4 Monitors, 1.5 Dashboards, 1.6 Genie Spaces, 1.7 Alerts, 1.1 ML Models)
- **Produce YAML manifests** ‚Äî generate 4 machine-readable manifest files (semantic-layer, observability, ML, GenAI agents) as contracts for downstream implementation stages
- **Apply workshop mode caps** ‚Äî enforce hard limits (3-5 TVFs, 1-2 Metric Views, 1 Genie Space) to keep the workshop focused on pattern variety over depth
- **Define deployment order** ‚Äî establish build sequence: TVFs ‚Üí Metric Views ‚Üí Genie Spaces ‚Üí Dashboards ‚Üí Monitors ‚Üí Alerts ‚Üí Agents

If a PRD exists at @docs/design_prd.md, reference it for business requirements, user personas, and workflows.',
'',
'Create Use-Case Plan',
'Generate implementation plans for operationalizing use cases with supporting artifacts',
13,
'## 1Ô∏è‚É£ How To Apply

Copy the prompt above, start a **new Agent chat** in Cursor, and paste it. The AI will analyze your Gold layer and create use case plans.

### Prerequisite

**Run this in your cloned Template Repository** (see Prerequisites in Step 0).

Ensure you have:
- ‚úÖ Gold Layer Design completed (Step 9)
- ‚úÖ Gold Layer Implementation completed (Step 12)
- ‚úÖ `data_product_accelerator/skills/planning/00-project-planning/SKILL.md` - The project planning skill
- ‚úÖ `docs/design_prd.md` - PRD with business requirements (optional, if available)

### Steps to Apply

1. **Start new Agent thread** ‚Äî Open Cursor and start a new Agent thread for clean context
2. **Copy and paste the prompt** ‚Äî Use the copy button, paste into Cursor; the AI will analyze your Gold layer and create use case plans
3. **Review generated plans** ‚Äî Plans appear in `plans/` folder (Phase addendums, artifact specs, implementation priorities)
4. **Prioritize use cases** ‚Äî Identify highest-value use cases, assign P0/P1/P2, determine implementation order
5. **Prepare for implementation** ‚Äî Use plans to guide Step 14+ (implement artifacts based on plans)

---

## 2Ô∏è‚É£ What Are We Building?

### üìö What is Use-Case Planning?

After building the Gold layer (data foundation), we now plan how to **operationalize** that data through various artifacts that serve different use cases.

### From Data to Value

| Layer | What You Have | What''s Next |
|-------|---------------|--------------|
| **Bronze** | Raw data | ‚úÖ Complete |
| **Silver** | Clean data | ‚úÖ Complete |
| **Gold** | Business-ready data | ‚úÖ Complete |
| **Artifacts** | Operational use cases | üëâ **THIS STEP** |

### üéØ Why Plan Before Building?

**The Goal:** Identify use cases FIRST, then create artifacts to realize them.

| Approach | Result |
|----------|--------|
| ‚ùå Build random artifacts | Unused dashboards, irrelevant metrics |
| ‚úÖ Plan use cases first | Every artifact serves a business need |

### PRD-Driven Planning

If a **Product Requirements Document (PRD)** exists at `docs/design_prd.md`, it provides:

| PRD Element | How It Informs Planning |
|-------------|-------------------------|
| **User Personas** | Who needs what data? |
| **Workflows** | What questions do users ask? |
| **Business Requirements** | What metrics matter most? |
| **Success Criteria** | How do we measure value? |

**PRD ‚Üí Use Cases ‚Üí Artifacts**

### üèóÔ∏è Agent Layer Architecture (How Artifacts Connect)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   FROM GOLD LAYER TO USE CASES                               ‚îÇ
‚îÇ                   (Agent Layer Architecture)                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  USERS (Natural Language)                                                   ‚îÇ
‚îÇ       ‚Üì                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ PHASE 3: Frontend App (Databricks App / Custom UI)                  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                 ‚Üì                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ PHASE 2: AI Agents (orchestrator ‚Üí domain agents)                   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ          Agents query through Genie Spaces ‚Äî NEVER direct SQL       ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                 ‚Üì                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ PHASE 1.6: Genie Spaces (NL-to-SQL interface, ‚â§ 25 assets each)    ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                 ‚Üì                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ PHASE 1 DATA ASSETS (consumed by Genie & Dashboards):               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  1.3 Metric Views ‚îÇ 1.2 TVFs ‚îÇ 1.1 ML Tables ‚îÇ 1.4 Monitors       ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                 ‚Üì                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ GOLD LAYER (Foundation ‚Äî completed in prior steps)                  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  dim_property ‚îÇ dim_destination ‚îÇ dim_user ‚îÇ dim_host ‚îÇ fact_booking ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

> **Key principle:** Each layer consumes the layer below it. Agents never bypass Genie Spaces to query Gold directly. This provides abstraction, query optimization, and built-in guardrails.

### üìã Phase 1 Addendums (Artifact Categories)

All analytics artifacts are organized as Phase 1 addendums:

| # | Addendum | Artifacts | Downstream Manifest |
|---|----------|-----------|---------------------|
| 1.1 | **ML Models** | Prediction models, feature tables | `ml-manifest.yaml` |
| 1.2 | **TVFs** | Parameterized SQL functions for Genie | `semantic-layer-manifest.yaml` |
| 1.3 | **Metric Views** | Semantic measures & dimensions | `semantic-layer-manifest.yaml` |
| 1.4 | **Lakehouse Monitoring** | Data quality monitors, custom metrics | `observability-manifest.yaml` |
| 1.5 | **AI/BI Dashboards** | Lakeview visualizations | `observability-manifest.yaml` |
| 1.6 | **Genie Spaces** | NL query interfaces (‚â§ 25 assets each) | `semantic-layer-manifest.yaml` |
| 1.7 | **Alerting Framework** | SQL Alerts with severity routing | `observability-manifest.yaml` |

> **Workshop default:** 1.2 TVFs, 1.3 Metric Views, and 1.6 Genie Spaces are included by default. Others included if requested.

### üîÑ Planning Methodology

The planning skill organizes work into **3 phases**, with Phase 1 containing **7 addendums** for all analytics artifacts:

### Phase & Addendum Structure

```
Phase 1: Use Cases (ALL analytics artifacts)
‚îú‚îÄ‚îÄ 1.1 ML Models           (demand predictors, pricing optimizers)
‚îú‚îÄ‚îÄ 1.2 TVFs                (parameterized queries for Genie)
‚îú‚îÄ‚îÄ 1.3 Metric Views        (semantic measures & dimensions)
‚îú‚îÄ‚îÄ 1.4 Lakehouse Monitoring (data quality monitors)
‚îú‚îÄ‚îÄ 1.5 AI/BI Dashboards    (Lakeview visualizations)
‚îú‚îÄ‚îÄ 1.6 Genie Spaces        (natural language query interfaces)
‚îî‚îÄ‚îÄ 1.7 Alerting Framework   (SQL alerts with severity routing)

Phase 2: Agent Framework (AI Agents with Genie integration)
‚îî‚îÄ‚îÄ Agents use Genie Spaces as query interface (never direct SQL)

Phase 3: Frontend App (User interface ‚Äî optional)
‚îî‚îÄ‚îÄ Databricks Apps or custom UI consuming Phase 1-2 artifacts
```

> **Key insight:** ALL data artifacts (TVFs, Metric Views, Dashboards, Monitors, Alerts, ML, Genie Spaces) are addendums within Phase 1. Agents (Phase 2) and Frontend (Phase 3) **consume** Phase 1 artifacts ‚Äî they are not separate artifact categories.

### Agent Domain Framework

**Domains emerge from business questions, not fixed categories.** The skill derives domains from your Gold table groupings and stakeholder questions:

| Rule | Why |
|------|-----|
| Domains come from Gold table relationships | Natural boundaries, not arbitrary labels |
| A domain needs ‚â• 3 business questions | Fewer = merge into a neighbor domain |
| Two domains sharing > 70% of Gold tables ‚Üí consolidate | Avoid duplicate artifacts |
| Don''t force a fixed count (2-3 for 5-10 tables is fine) | More domains ‚â† better |

**Example for your source data:**

| Domain | Focus Area | Key Gold Tables |
|--------|------------|----------------|
| üí∞ **Revenue** | Bookings, pricing, revenue trends | `fact_booking_detail`, `dim_property` |
| üè† **Host Performance** | Host activity, ratings, response times | `dim_host`, `fact_review` |
| üë§ **Guest Experience** | Guest behavior, satisfaction, lifetime value | `dim_user`, `fact_booking_detail` |

> **Anti-pattern:** Creating 5+ generic domains (Cost, Performance, Quality, Reliability, Security) that don''t map to your actual Gold tables.

### üí° Use Case Examples for Vacation Rentals

Based on your Gold layer, typical use cases include:

### Revenue Analytics
- "What is our total booking revenue by destination?"
- "Which properties have the highest average nightly rate?"
- "Revenue trend over the past 12 months?"

### Host Performance
- "Who are our top-performing hosts?"
- "Which hosts have the best guest ratings?"
- "Host response time analysis?"

### Guest Insights
- "Customer lifetime value by segment?"
- "Repeat booking rate analysis?"
- "Guest demographics by destination?"

### Property Optimization
- "Property occupancy rates by season?"
- "Which amenities correlate with higher bookings?"
- "Pricing optimization recommendations?"

### Operational Monitoring
- "Data freshness alerts?"
- "Booking anomaly detection?"
- "Revenue target tracking?"

---

## 3Ô∏è‚É£ Why Are We Building It This Way? (Databricks Best Practices)

| Practice | How It''s Used Here |
|----------|-------------------|
| **Agent Domain Framework** | Domains derived from business questions and Gold table groupings (not forced to a fixed count). Each domain maps to a potential Genie Space. |
| **Artifact Rationalization** | Every artifact must trace to a business question. TVFs only when Metric Views can''t answer. No quota-filling. Prevents artifact bloat. |
| **Genie Space 25-Asset Limit** | Hard constraint: each Genie Space holds ‚â§ 25 data assets. Plan calculates total assets ‚Üí determines space count. Under 10 assets = merge. |
| **Deployment Order Discipline** | Build order enforced: Phase 1 addendums (1.2‚Üí1.3‚Üí1.6‚Üí1.5‚Üí1.4‚Üí1.7‚Üí1.1) ‚Üí Phase 2 (Agents). Genie Spaces MUST exist before Agents can use them. |
| **Agent Layer Architecture** | AI Agents (Phase 2) query data through Genie Spaces (Phase 1.6), never direct SQL. Provides abstraction, optimization, and guardrails. |
| **Serverless-First Architecture** | Every artifact designed for serverless execution ‚Äî SQL warehouses for queries, serverless jobs for ETL, serverless DLT for pipelines |
| **Lakehouse Monitoring Integration** | Plans include monitor specifications leveraging Databricks Lakehouse Monitoring with custom business metrics (AGGREGATE, DERIVED, DRIFT) |
| **AI/BI Dashboard Planning** | Dashboard specs designed for Databricks AI/BI (Lakeview) ‚Äî native format with widget-query alignment and parameter configuration |
| **Genie Space Optimization Targets** | Plans include benchmark questions with accuracy targets (95%+) and repeatability targets (90%+). General Instructions ‚â§ 20 lines. |
| **YAML Manifests as Contracts** | 4 machine-readable manifests bridge planning and implementation. Downstream skills parse manifests (not prose). `planning_mode: workshop` prevents expansion. |
| **Workshop Mode Hard Caps** | When `planning_mode: workshop` is active, artifact counts are capped (3-5 TVFs, 1-2 MVs, 1 Genie Space). Manifests propagate this ceiling to all downstream skills. |

---

## 4Ô∏è‚É£ What Happens Behind the Scenes?

When you paste the prompt, the AI reads `@data_product_accelerator/skills/planning/00-project-planning/SKILL.md` ‚Äî the **Project Planning orchestrator**. Behind the scenes:

1. **Workshop mode detection** ‚Äî `planning_mode: workshop` activates the workshop profile, which produces a **minimal representative plan** (3-5 TVFs, 1-2 Metric Views, 1 Genie Space) designed for hands-on workshops. The first line of output confirms: `**Planning Mode:** Workshop (explicit opt-in ‚Äî artifact caps active)`.
2. **Interactive quick start** ‚Äî the skill asks key decisions before generating plans:
   - Which domains to include (derived from business questions and Gold table groupings)
   - Which Phase 1 addendums to generate (1.1 ML through 1.7 Alerting)
   - Whether to include Phase 2 (Agents) and Phase 3 (Frontend)
   - Agent-to-Genie Space mapping strategy
3. **Artifact Rationalization** ‚Äî the skill applies rigorous rules to prevent artifact bloat:
   - Every artifact must trace to a business question (no quota-filling)
   - TVFs only where Metric Views can''t answer the question
   - Genie Spaces sized by total asset count (25-asset hard limit per space)
   - Domains consolidated when overlap exceeds 70% of Gold tables
4. **YAML manifest contracts** ‚Äî 4 machine-readable manifests generated for downstream stages:
   - `semantic-layer-manifest.yaml` (TVFs + Metric Views + Genie Spaces)
   - `observability-manifest.yaml` (Monitors + Dashboards + Alerts)
   - `ml-manifest.yaml` and `genai-agents-manifest.yaml`
5. **Common skills auto-loaded:**
   - `databricks-expert-agent` ‚Äî "Extract, Don''t Generate" applied to plan-to-implementation handoff
   - `naming-tagging-standards` ‚Äî enterprise naming conventions for all planned artifacts

**Key concept: Agent Layer Architecture** ‚Äî Agents (Phase 2) use Genie Spaces (Phase 1.6) as their query interface, NOT direct SQL. This means Genie Spaces must be deployed before agents can consume them.',
'## Expected Deliverables

### üìÅ Generated Plan Files

```
plans/
‚îú‚îÄ‚îÄ README.md                               # Plan index and navigation
‚îú‚îÄ‚îÄ prerequisites.md                        # Bronze/Silver/Gold summary
‚îú‚îÄ‚îÄ phase1-use-cases.md                     # Phase 1 master (all analytics artifacts)
‚îÇ   ‚îú‚îÄ‚îÄ phase1-addendum-1.1-ml-models.md        # ML model specifications
‚îÇ   ‚îú‚îÄ‚îÄ phase1-addendum-1.2-tvfs.md             # TVF definitions
‚îÇ   ‚îú‚îÄ‚îÄ phase1-addendum-1.3-metric-views.md     # Metric view specifications
‚îÇ   ‚îú‚îÄ‚îÄ phase1-addendum-1.4-lakehouse-monitoring.md  # Monitor configurations
‚îÇ   ‚îú‚îÄ‚îÄ phase1-addendum-1.5-aibi-dashboards.md  # Dashboard specifications
‚îÇ   ‚îú‚îÄ‚îÄ phase1-addendum-1.6-genie-spaces.md     # Genie Space setups
‚îÇ   ‚îî‚îÄ‚îÄ phase1-addendum-1.7-alerting.md         # Alert configurations
‚îú‚îÄ‚îÄ phase2-agent-framework.md               # AI agent specifications (optional)
‚îú‚îÄ‚îÄ phase3-frontend-app.md                  # App integration plans (optional)
‚îî‚îÄ‚îÄ manifests/                              # ‚≠ê Machine-readable contracts
    ‚îú‚îÄ‚îÄ semantic-layer-manifest.yaml        # TVFs + Metric Views + Genie Spaces
    ‚îú‚îÄ‚îÄ observability-manifest.yaml         # Monitors + Dashboards + Alerts
    ‚îú‚îÄ‚îÄ ml-manifest.yaml                    # Feature Tables + Models + Experiments
    ‚îî‚îÄ‚îÄ genai-agents-manifest.yaml          # Agents + Tools + Eval Datasets
```

> **Key innovation: Plan-as-Contract.** The 4 YAML manifests serve as **contracts** between planning and implementation. When downstream skills (semantic layer, monitoring, ML, GenAI) run, they read their manifest to know exactly what to build ‚Äî enforcing "Extract, Don''t Generate" across the planning-to-implementation handoff. In workshop mode, manifests include `planning_mode: workshop` to prevent downstream skills from expanding beyond listed artifacts.

---

### üìä Plan Document Structure

Each plan document includes:

```markdown
# Artifact Category Plan

## Overview
- Business objectives
- Target users
- Success metrics

## Artifact Specifications

### Artifact 1: [Name]
- **Agent Domain:** [Derived from your business questions]
- **Description:** [What it does]
- **Source Gold Tables:** [Gold tables used]
- **Business Questions Answered:** [Which stakeholder questions does this serve?]
- **Implementation Priority:** [P0/P1/P2]

### Artifact 2: [Name]
...

## Implementation Timeline
- Sprint assignments
- Dependencies
- Milestones

## Validation Criteria
- How to verify success
- Expected outcomes
```

---

### üéØ Workshop Mode Artifact Caps

This workshop uses `planning_mode: workshop` ‚Äî hard caps prevent artifact bloat:

| Category | Workshop Cap | Selection Criteria | Acceleration (default) |
|----------|-------------|-------------------|----------------------|
| **Domains** | 1-2 max | Richest Gold table relationships | Derived from business questions |
| **TVFs** | 3-5 total | One per parameter pattern (date-range, entity-filter, top-N) | ~1-2 per Gold table |
| **Metric Views** | 1-2 total | One per fact table (pick richest joins) | One per distinct grain |
| **Genie Spaces** | 1 unified | All workshop assets in one space (< 15 assets) | Based on 25-asset limit |
| **Dashboards** | 0-1 | Optional if time permits | 5-8 |
| **Monitors** | 1-2 | One fact + one dimension | 10-15 |
| **Alerts** | 2-3 | One CRITICAL + one WARNING (severity demo) | 10-15 |
| **ML Models** | 0-1 | Skip unless explicitly requested | 5-7 |
| **Phase 2 (Agents)** | Skip | Excluded by default in workshop | Full agent framework |
| **Phase 3 (Frontend)** | Skip | Excluded by default in workshop | Databricks App |

> **Selection principle:** Pick the **most representative** artifact for each pattern type. Prefer **variety of patterns** (date-range TVF, entity-filter TVF, top-N TVF) over depth in a single domain. The goal is to teach the full pattern vocabulary with minimum artifacts.

---

### üîÄ Deployment Order (Critical!)

**Phase 1 addendums must be deployed in this order:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  PHASE 1 DEPLOYMENT ORDER                                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  1.2 TVFs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ 1.3 Metric Views ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ 1.6 Genie Spaces       ‚îÇ
‚îÇ  (parameterized         (semantic measures           (NL-to-SQL using       ‚îÇ
‚îÇ   queries)               & dimensions)                TVFs + MVs + tables)  ‚îÇ
‚îÇ                                                            ‚îÇ                ‚îÇ
‚îÇ  1.4 Monitors ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ 1.7 Alerts                          ‚îÇ                ‚îÇ
‚îÇ  (data quality          (threshold/anomaly                 ‚îÇ                ‚îÇ
‚îÇ   profiling)             notifications)                    ‚îÇ                ‚îÇ
‚îÇ                                                            ‚îÇ                ‚îÇ
‚îÇ  1.5 Dashboards                                            ‚îÇ                ‚îÇ
‚îÇ  (visualizes Metric Views + TVFs + Monitors)               ‚îÇ                ‚îÇ
‚îÇ                                                            ‚îÇ                ‚îÇ
‚îÇ  1.1 ML Models                                             ‚îÇ                ‚îÇ
‚îÇ  (predictions feed into Genie Spaces as tables)            ‚îÇ                ‚îÇ
‚îÇ                                                            ‚ñº                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ PHASE 2: AI Agents (consume Genie Spaces ‚Äî deploy AFTER Phase 1.6) ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                          ‚Üì                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ PHASE 3: Frontend App (consumes Agents + Dashboards ‚Äî optional)     ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

> **Why order matters:** Genie Spaces need TVFs and Metric Views to exist before they can be added as assets. Agents need Genie Spaces to exist before they can query through them. Violating this order causes deployment failures.

---

### ‚úÖ Success Criteria Checklist

**Plan Structure:**
- [ ] First line confirms mode: `**Planning Mode:** Workshop (explicit opt-in)`
- [ ] `plans/README.md` provides navigation with links to all documents
- [ ] Phase 1 master + selected addendums (1.2 TVFs, 1.3 MVs, 1.6 Genie included by default in workshop)
- [ ] Each plan document follows standard template (Overview, Specs, Timeline, Validation)

**Agent Domain Framework:**
- [ ] Domains derived from business questions and Gold table groupings
- [ ] Each domain has ‚â• 3 business questions (or merged)
- [ ] No two domains share > 70% of Gold tables (or consolidated)
- [ ] Domain count justified (2-3 for 5-10 Gold tables)

**Artifact Rationalization (Prevent Bloat):**
- [ ] Every artifact traces to a business question
- [ ] No TVF duplicates what a Metric View already provides
- [ ] Each Genie Space has ‚â§ 25 data assets and ‚â• 10 assets
- [ ] Genie Space count based on total asset volume (not domain count)
- [ ] Workshop caps respected: 3-5 TVFs, 1-2 MVs, 1 Genie Space

**YAML Manifests (Plan-as-Contract):**
- [ ] 4 manifests generated in `plans/manifests/`
- [ ] `planning_mode: workshop` present in all manifests
- [ ] All table/column references validated against Gold YAML
- [ ] Artifact counts in manifests match plan addendum counts

**Deployment Order:**
- [ ] Phase 1 addendum dependencies documented
- [ ] Genie Spaces listed as deployed AFTER TVFs + Metric Views
- [ ] Agents (Phase 2) listed as deployed AFTER Genie Spaces (if included)

**Use Case Coverage:**
- [ ] Key business questions documented per domain (‚â• 3 each)
- [ ] All artifacts tagged with Agent Domain
- [ ] LLM-friendly comments specified for all artifacts
- [ ] Source Gold tables identified for each artifact',
true, 1, true, current_timestamp(), current_timestamp(), current_user());

-- Step 14: Build AI/BI Dashboard (moved to step 16, order_number 14) - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(12, 'aibi_dashboard',
'Build an AI/BI (Lakeview) Dashboard using @data_product_accelerator/skills/monitoring/02-databricks-aibi-dashboards/SKILL.md

This will involve the following end-to-end workflow:

- **Build Lakeview dashboard** ‚Äî create a complete `.lvdash.json` configuration with KPI counters, charts, data tables, and filters for business self-service analytics
- **Use 6-column grid layout** ‚Äî position all widgets on a 6-column grid (NOT 12!) with correct widget versions (KPIs=v2, Charts=v3, Tables=v1, Filters=v2)
- **Query Metric Views** ‚Äî write dataset queries using `MEASURE()` function against Metric Views with `${catalog}.${gold_schema}` variable substitution
- **Validate SQL and widget alignment** ‚Äî run pre-deployment validation ensuring every widget `fieldName` matches its SQL alias exactly (90% reduction in dev loop time)
- **Deploy via UPDATE-or-CREATE** ‚Äî use Workspace Import API with `overwrite: true` to preserve dashboard URLs and viewer permissions

Reference the dashboard plan at @data_product_accelerator/plans/phase1-addendum-1.1-dashboards.md

The skill provides:
- Dashboard JSON structure with **6-column grid** layout (NOT 12!)
- Widget patterns: KPI counters (v2), charts (v3), tables (v1), filters (v2)
- Query patterns from Metric Views using `MEASURE()` function
- Pre-deployment SQL validation (90% reduction in dev loop time)
- UPDATE-or-CREATE deployment pattern (preserves URLs and permissions)
- Variable substitution (`${catalog}`, `${gold_schema}`) ‚Äî no hardcoded schemas
- Monitoring table query patterns (window structs, CASE pivots) if Lakehouse Monitors exist

Build the dashboard in this order:
1. Plan layout (KPIs, filters, charts, tables)
2. Create datasets (validated SQL queries)
3. Build widgets with correct version specs
4. Configure parameters (DATE type, not DATETIME)
5. Add Global Filters page
6. Deploy via Workspace Import API',
'',
'Build AI/BI Dashboard',
'Create an AI/BI (Lakeview) dashboard with KPI counters, charts, filters, and automated deployment from Gold layer data',
14,
'## 1Ô∏è‚É£ How To Apply

**Copy the prompt above**, start a **new Agent thread** in Cursor, and **paste it**. The AI will build the dashboard in phases.

---

### Prerequisites

**Run this in your cloned Template Repository** (see Prerequisites in Step 0).

Ensure you have:
- ‚úÖ Gold Layer Implementation completed (Step 12) ‚Äî with column COMMENTs
- ‚úÖ Semantic Layer completed (Step 14) ‚Äî Metric Views for dashboard queries
- ‚úÖ Use-Case Plan created (Step 13) ‚Äî with dashboard requirements
- ‚úÖ Plan file exists: `plans/phase1-addendum-1.1-dashboards.md`
- ‚úÖ Gold YAML schemas available for column name validation

---

### Steps to Apply

**Step 1:** Start new Agent thread ‚Äî Open Cursor and start a **new Agent thread** for clean context.

**Step 2:** Copy and paste the prompt ‚Äî Copy the entire prompt using the copy button, paste it into Cursor. The AI will build the dashboard in phases.

**Step 3:** Plan Reading ‚Äî The AI will read dashboard plan (`plans/phase1-addendum-1.1-dashboards.md`), extract KPI requirements, chart types, filter dimensions, and identify data sources (Metric Views preferred over raw Gold tables).

**Step 4:** Dataset Creation ‚Äî The AI will create SQL queries for each widget (using `${catalog}` substitution), use `MEASURE()` function for Metric View queries, include "All" option for filter datasets, and handle NULLs with `COALESCE()` and SCD2 with `is_current = true`.

**Step 5:** Widget and Layout Creation ‚Äî The AI will build KPI counters (version 2) for top-line metrics, build charts (version 3) for trends and comparisons, build data tables (version 1) for drill-down, and position using 6-column grid (widths 1-6, NOT 12!).

**Step 6:** Parameter and Filter Configuration ‚Äî The AI will add DATE parameters with static defaults (not DATETIME), create Global Filters page (`PAGE_TYPE_GLOBAL_FILTERS`), and link filter widgets to dataset parameters.

**Step 7:** Validate and Deploy

```bash
# Pre-deployment validation
python scripts/validate_dashboard_queries.py
python scripts/validate_widget_encodings.py

# Deploy via Asset Bundle or API
databricks bundle deploy -t dev
```

```sql
-- Verify Gold tables have COMMENTs (prerequisite for good queries)
SELECT table_name, comment FROM information_schema.tables 
WHERE table_schema = ''{user_schema_prefix}_gold'' AND comment IS NOT NULL;
```

---

## 2Ô∏è‚É£ What Are We Building?

### What is an AI/BI (Lakeview) Dashboard?

**AI/BI Dashboards** (formerly Lakeview) provide **visual, self-service analytics** for business users ‚Äî no SQL required. They are built from JSON configuration files that define datasets, widgets, pages, and parameters.

**Core Philosophy: Self-Service Analytics**
- ‚úÖ Visual insights for non-technical users
- ‚úÖ Consistent metrics across the organization (via Metric Views)
- ‚úÖ Professional, branded appearance with auto-refresh
- ‚úÖ Automated deployment with validation
- ‚ùå NOT a code editor ‚Äî business users interact through UI only

### Lakeview Dashboard Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     AI/BI (LAKEVIEW) DASHBOARD                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                    DASHBOARD JSON                                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ              (.lvdash.json configuration file)                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                    PAGES                                     ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Page 1: Overview    ‚îÇ  Page 2: Details   ‚îÇ  Global Filters ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                  WIDGETS (6-Column Grid)                       ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                                                               ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ KPI  ‚îÇ ‚îÇ KPI  ‚îÇ ‚îÇ KPI  ‚îÇ ‚îÇ KPI  ‚îÇ ‚îÇ KPI  ‚îÇ ‚îÇ KPI  ‚îÇ    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ (v2) ‚îÇ ‚îÇ (v2) ‚îÇ ‚îÇ (v2) ‚îÇ ‚îÇ (v2) ‚îÇ ‚îÇ (v2) ‚îÇ ‚îÇ (v2) ‚îÇ    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  Line Chart (v3)‚îÇ ‚îÇ  Bar Chart (v3) ‚îÇ                    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ  Trend over time‚îÇ ‚îÇ  By dimension   ‚îÇ                    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ         Data Table (v1)              ‚îÇ                    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ         Detailed drill-down          ‚îÇ                    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                    DATASETS                                    ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  SQL queries ‚Üí Metric Views / Gold tables / Monitoring tables ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Parameters: DATE type (not DATETIME), variable substitution  ‚îÇ ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ  ‚îÇ   DEPLOY via API     ‚îÇ  ‚îÇ   VALIDATE before     ‚îÇ                       ‚îÇ
‚îÇ  ‚îÇ   UPDATE-or-CREATE   ‚îÇ  ‚îÇ   deploy (SQL + widget)‚îÇ                       ‚îÇ
‚îÇ  ‚îÇ   Preserves URLs     ‚îÇ  ‚îÇ   90% faster dev loop  ‚îÇ                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Key Concepts

| Concept | What It Means | Why It Matters |
|---------|--------------|----------------|
| **Lakeview JSON** | Dashboards are defined as `.lvdash.json` files | Version-controlled, deployable via API |
| **6-Column Grid** | Widget positions use columns 0-5 (NOT 12!) | #1 cause of widget snapping issues |
| **Widget Versions** | KPIs=v2, Charts=v3, Tables=v1, Filters=v2 | Wrong version causes rendering errors |
| **DATE Parameters** | Use DATE type (not DATETIME) with static defaults | DATETIME with dynamic expressions won''t work |
| **`dataset_catalog`/`dataset_schema`** | Variable substitution for environment portability | Never hardcode catalog/schema in queries |
| **Widget-Query Alignment** | Widget `fieldName` MUST match query output alias | #1 cause of "no fields to visualize" errors |
| **Number Formatting** | Return raw numbers; widgets format them | `FORMAT_NUMBER()` or string concat breaks widgets |
| **Global Filters Page** | Dedicated page for cross-dashboard filtering | Required for consistent filter behavior |

---

### Dashboard Components

#### Widget Type Reference

| Widget Type | Version | Use Case | Grid Size |
|-------------|---------|----------|-----------|
| **KPI Counter** | v2 | Single metric display (revenue, count) | width: 1-2, height: 2 |
| **Bar Chart** | v3 | Category comparisons (revenue by destination) | width: 3, height: 6 |
| **Line Chart** | v3 | Trends over time (daily revenue) | width: 3, height: 6 |
| **Pie Chart** | v3 | Distribution (booking share by type) | width: 3, height: 6 |
| **Area Chart** | v3 | Stacked trends (revenue by category over time) | width: 3-6, height: 6 |
| **Data Table** | v1 | Detailed drill-down data | width: 6, height: 6+ |
| **Filter** | v2 | Single-select / multi-select / date range | width: 2, height: 2 |

#### Chart Scale Rules (Encoding Requirements)

```
Pie Charts:   color.scale = categorical, angle.scale = quantitative
Bar Charts:   x.scale = categorical, y.scale = quantitative
Line Charts:  x.scale = temporal, y.scale = quantitative
Area Charts:  x.scale = temporal, y.scale = quantitative, y.stack = "zero"
```

> **Missing `scale` in encodings** is the #2 cause of "unable to render visualization" errors.

#### Standard Dashboard Layout (6-Column Grid)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Page 1: Overview                                     ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ Row 0 (height 2): Filters                           ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ ‚îÇ Date (w2)‚îÇ ‚îÇ Filter(w2‚îÇ ‚îÇ Filter(w2‚îÇ            ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ Row 2 (height 2): KPI Counters                      ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ ‚îÇKPI ‚îÇ ‚îÇKPI ‚îÇ ‚îÇKPI ‚îÇ ‚îÇKPI ‚îÇ ‚îÇKPI ‚îÇ ‚îÇKPI ‚îÇ       ‚îÇ
‚îÇ ‚îÇw=1 ‚îÇ ‚îÇw=1 ‚îÇ ‚îÇw=1 ‚îÇ ‚îÇw=1 ‚îÇ ‚îÇw=1 ‚îÇ ‚îÇw=1 ‚îÇ       ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ Row 4 (height 6): Charts                            ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ ‚îÇ  Line Chart (w3) ‚îÇ ‚îÇ  Bar Chart  (w3) ‚îÇ          ‚îÇ
‚îÇ ‚îÇ  Revenue Trend   ‚îÇ ‚îÇ  By Destination  ‚îÇ          ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ Row 10 (height 6): Detail Table                     ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ ‚îÇ         Data Table (w6)              ‚îÇ            ‚îÇ
‚îÇ ‚îÇ         Full-width drill-down        ‚îÇ            ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Page: Global Filters                                 ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ ‚îÇ Date (w2)‚îÇ ‚îÇ Dim  (w2)‚îÇ ‚îÇ Dim  (w2)‚îÇ            ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ pageType: PAGE_TYPE_GLOBAL_FILTERS                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Query Pattern Best Practices

#### Use Metric Views (Preferred)

```sql
-- ‚úÖ PREFERRED: Query Metric View with MEASURE()
SELECT 
  destination,
  MEASURE(total_revenue) as revenue,
  MEASURE(booking_count) as bookings
FROM ${catalog}.${gold_schema}.revenue_analytics_metrics
WHERE booking_date BETWEEN :start_date AND :end_date
GROUP BY destination
ORDER BY revenue DESC
```

#### Direct Gold Table Query (Fallback)

```sql
-- When no Metric View exists for the data
SELECT 
  d.destination_name as destination,
  SUM(f.total_amount) as revenue,
  COUNT(*) as bookings
FROM ${catalog}.${gold_schema}.fact_booking_detail f
JOIN ${catalog}.${gold_schema}.dim_destination d 
  ON f.destination_id = d.destination_id
WHERE f.booking_date BETWEEN :start_date AND :end_date
GROUP BY d.destination_name
ORDER BY revenue DESC
```

#### Number Formatting Rules

| Return This | Widget Displays | Format Type |
|-------------|-----------------|-------------|
| `0.85` | `85%` | `number-percent` |
| `1234.56` | `$1,234.56` | `number-currency` |
| `1234` | `1,234` | `number-plain` |

> **NEVER** use `FORMAT_NUMBER()`, `CONCAT(''$'', ...)`, or `CONCAT(..., ''%'')` in queries. Return raw numbers; let widgets format them.

---

## 3Ô∏è‚É£ Why Are We Building It This Way? (Databricks Best Practices)

| Practice | How It''s Used Here |
|----------|-------------------|
| **6-Column Grid (NOT 12!)** | Widget widths use 1-6 columns. `width: 6` = full width, `width: 3` = half. This is the #1 cause of layout issues ‚Äî most platforms use 12 columns, Lakeview uses 6. |
| **Widget Version Specs** | KPI Counters = version 2, Charts (bar/line/pie/area) = version 3, Tables = version 1, Filters = version 2. Wrong version causes rendering failures. |
| **Widget-Query Column Alignment** | Every widget `fieldName` MUST exactly match the SQL alias in its dataset query. Mismatch = "no fields to visualize" error. |
| **Raw Number Formatting** | Queries return raw numbers (e.g., `0.85` for 85%). Widgets apply formatting (`number-percent`, `number-currency`, `number-plain`). NEVER use `FORMAT_NUMBER()` or string concatenation. |
| **DATE Parameters (Not DATETIME)** | Dashboard parameters use `DATE` type with static default values. `DATETIME` with dynamic expressions like `now-30d/d` does NOT work. |
| **Variable Substitution** | All queries use `${catalog}.${gold_schema}` ‚Äî never hardcoded catalog/schema. Substitution done in Python at deployment time. |
| **Global Filters Page** | Every dashboard includes a `PAGE_TYPE_GLOBAL_FILTERS` page for cross-dashboard date range and dimension filtering. |
| **Metric View Queries** | Dashboards query Metric Views using `MEASURE()` function for consistent metric definitions. Metric Views are preferred over raw Gold tables. |
| **UPDATE-or-CREATE Deployment** | Workspace Import API with `overwrite: true` ‚Äî single code path for create and update. Preserves dashboard URLs and viewer permissions. |
| **Pre-Deployment SQL Validation** | All dataset queries validated with `SELECT ... LIMIT 1` before dashboard import. Catches UNRESOLVED_COLUMN, TABLE_NOT_FOUND, UNBOUND_PARAMETER errors. |
| **SCD2 Handling in Queries** | Dimension queries use `QUALIFY ROW_NUMBER() OVER(PARTITION BY id ORDER BY change_time DESC) = 1` or `WHERE is_current = true` |
| **"All" Option for Filters** | Filter datasets include `SELECT ''All'' UNION ALL SELECT DISTINCT ...` so users can clear filters |

---

## 4Ô∏è‚É£ What Happens Behind the Scenes?

When you paste the prompt, the AI reads `@data_product_accelerator/skills/monitoring/02-databricks-aibi-dashboards/SKILL.md` ‚Äî the **AI/BI Dashboard worker skill**. Behind the scenes:

1. **Plan reading** ‚Äî the skill reads your dashboard plan (`plans/phase1-addendum-1.1-dashboards.md`) to extract: KPIs, charts, filters, layout requirements
2. **Dashboard skill loaded** ‚Äî provides complete JSON templates, widget specs, grid layout rules, query patterns, validation scripts, and deployment workflows
3. **5 Common skills auto-loaded:**
   - `databricks-expert-agent` ‚Äî "Extract, Don''t Generate" for table/column names
   - `databricks-asset-bundles` ‚Äî dashboard resource deployment
   - `databricks-python-imports` ‚Äî deployment script module patterns
   - `naming-tagging-standards` ‚Äî dashboard and file naming conventions
   - `databricks-autonomous-operations` ‚Äî self-healing deploy loop
4. **Query pattern selection:** Metric Views ‚Üí Gold tables ‚Üí Monitoring tables (priority order)
5. **Pre-deployment validation** ‚Äî SQL validation + widget-encoding alignment check before import (catches 90% of errors before deploy)
6. **UPDATE-or-CREATE deployment** ‚Äî Workspace Import API with `overwrite: true` preserves URLs and permissions

**Key principle:** The AI reads your plan to **extract** KPI/chart requirements. Dashboard queries use `${catalog}` and `${gold_schema}` variable substitution ‚Äî never hardcoded schemas.

> **Note:** For the full observability stack (Lakehouse Monitoring + Dashboards + SQL Alerts), use the orchestrator at `@data_product_accelerator/skills/monitoring/00-observability-setup/SKILL.md`. This step focuses specifically on the dashboard.',
'## Expected Deliverables

### üìÅ Dashboard Files Created

```
docs/dashboards/
‚îú‚îÄ‚îÄ analytics_dashboard.lvdash.json   # Dashboard JSON config
‚îî‚îÄ‚îÄ README.md                                      # Dashboard documentation

scripts/
‚îú‚îÄ‚îÄ deploy_dashboard.py                            # UPDATE-or-CREATE deployment
‚îú‚îÄ‚îÄ validate_dashboard_queries.py                  # Pre-deploy SQL validation
‚îî‚îÄ‚îÄ validate_widget_encodings.py                   # Widget-query alignment check

resources/monitoring/
‚îî‚îÄ‚îÄ dashboard_deploy_job.yml                       # Asset Bundle deployment job
```

> **Key:** The `.lvdash.json` file IS the dashboard. It contains all datasets, pages, widgets, parameters, and theme settings. Version-control this file.

---

### üìä Dashboard Configuration Summary (Workshop Scope)

| Element | Value |
|---------|-------|
| **Dashboard Name** | Analytics Dashboard |
| **Pages** | 2 (Overview + Global Filters) |
| **KPI Counters** | 3-6 top-line metrics (total revenue, bookings, avg rate) |
| **Charts** | 2-4 visualizations (trend line, bar comparison, pie distribution) |
| **Data Tables** | 1 drill-down table |
| **Filters** | Date range + 1-2 dimension filters |
| **Data Sources** | Metric Views (preferred) + Gold tables (fallback) |
| **Parameters** | DATE type with static defaults |
| **Deployment** | UPDATE-or-CREATE via Workspace Import API |

---

### üìä What Each Widget Does

| Widget | Type | Version | Data Source | Insight |
|--------|------|---------|-------------|---------|
| Total Revenue | KPI Counter | v2 | `revenue_analytics_metrics` | Top-line revenue figure |
| Booking Count | KPI Counter | v2 | `revenue_analytics_metrics` | Total bookings in period |
| Avg Nightly Rate | KPI Counter | v2 | `revenue_analytics_metrics` | Average price metric |
| Revenue Trend | Line Chart | v3 | `fact_booking_detail` | Revenue over time |
| Revenue by Destination | Bar Chart | v3 | `revenue_analytics_metrics` | Geographic breakdown |
| Booking Details | Data Table | v1 | `fact_booking_detail + dims` | Drill-down for analysis |
| Date Range | Filter | v2 | Parameter | Cross-page date filtering |
| Destination | Filter | v2 | `dim_destination` | Geographic filtering |

---

### üìä 6-Column Grid Layout (Critical)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Grid columns: 0  1  2  3  4  5          ‚îÇ
‚îÇ                                          ‚îÇ
‚îÇ width: 1 = one column (1/6 of page)     ‚îÇ
‚îÇ width: 2 = two columns (1/3 of page)    ‚îÇ
‚îÇ width: 3 = three columns (1/2 of page)  ‚îÇ
‚îÇ width: 6 = full width (entire page)     ‚îÇ
‚îÇ                                          ‚îÇ
‚îÇ Common layouts:                          ‚îÇ
‚îÇ ‚Ä¢ 6 KPIs: [w1][w1][w1][w1][w1][w1]     ‚îÇ
‚îÇ ‚Ä¢ 3 KPIs: [w2  ][w2  ][w2  ]           ‚îÇ
‚îÇ ‚Ä¢ 2 charts: [w3     ][w3     ]          ‚îÇ
‚îÇ ‚Ä¢ Full table: [w6                ]       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

> **#1 mistake:** Using width values from a 12-column grid. In Lakeview, `width: 6` = FULL width, not half!

---

### üìä Dashboard JSON Structure (Simplified)

```json
{
  "datasets": [
    {
      "name": "kpi_totals",
      "query": "SELECT ... FROM ${catalog}.${gold_schema}.metric_view ..."
    }
  ],
  "pages": [
    {
      "name": "page_overview",
      "displayName": "Overview",
      "layout": [ /* widgets with positions */ ]
    },
    {
      "name": "page_global_filters",
      "displayName": "Global Filters",
      "pageType": "PAGE_TYPE_GLOBAL_FILTERS",
      "layout": [ /* filter widgets */ ]
    }
  ],
  "parameters": [
    {
      "keyword": "start_date",
      "dataType": "DATE",
      "defaultSelection": { "values": { "values": [{"value": "2024-01-01"}] } }
    }
  ]
}
```

---

### ‚úÖ Success Criteria Checklist

**Grid and Layout:**
- [ ] All widget widths use 6-column grid (1-6, never 7-12)
- [ ] KPI row uses consistent heights (height: 2)
- [ ] Chart row uses consistent heights (height: 6)
- [ ] Full-width tables use width: 6
- [ ] Global Filters page included (`PAGE_TYPE_GLOBAL_FILTERS`)

**Widget Versions (non-negotiable):**
- [ ] KPI Counters use version 2 (not 3)
- [ ] Bar/Line/Pie/Area Charts use version 3
- [ ] Data Tables use version 1
- [ ] Filters use version 2

**Widget-Query Alignment:**
- [ ] Every widget `fieldName` matches its SQL alias exactly
- [ ] Pie charts have `scale` on both `color` and `angle` encodings
- [ ] Bar charts have `scale` on both `x` and `y` encodings
- [ ] Line charts use `temporal` scale on x-axis

**Number Formatting:**
- [ ] Percentages returned as 0-1 decimal (widget displays as %)
- [ ] Currency returned as raw number (widget displays as $)
- [ ] No `FORMAT_NUMBER()` or string concatenation in queries

**Parameters:**
- [ ] Date parameters use DATE type (never DATETIME)
- [ ] Static default values (never dynamic expressions like `now-30d`)
- [ ] All parameters defined in dataset''s `parameters` array
- [ ] Filters include "All" option via `UNION ALL`

**Data Sources:**
- [ ] Queries use `${catalog}.${gold_schema}` variable substitution
- [ ] No hardcoded catalog or schema names in queries
- [ ] Metric View queries use `MEASURE()` function where applicable
- [ ] SCD2 dimensions filtered with `is_current = true` or `QUALIFY`
- [ ] NULL values handled with `COALESCE()`

**Deployment:**
- [ ] `.lvdash.json` file created and version-controlled
- [ ] `deploy_dashboard.py` uses UPDATE-or-CREATE pattern
- [ ] `validate_dashboard_queries.py` passes all SQL checks
- [ ] `validate_widget_encodings.py` passes all alignment checks
- [ ] `databricks bundle deploy -t dev` succeeds

**Verification:**
```sql
-- Check dashboard exists in workspace
-- Navigate to: Databricks ‚Üí Dashboards ‚Üí find your dashboard

-- Verify data sources are connected
SELECT COUNT(*) FROM ${catalog}.${gold_schema}.fact_booking_detail;

-- Verify Metric Views exist
SELECT table_name, table_type 
FROM information_schema.tables 
WHERE table_schema = ''{user_schema_prefix}_gold'' AND table_type = ''METRIC_VIEW'';
```',
true, 1, true, current_timestamp(), current_timestamp(), current_user());

-- Step 15: Build Genie Space [Metric Views/TVFs] - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(11, 'genie_space',
'Set up the semantic layer using @data_product_accelerator/skills/semantic-layer/00-semantic-layer-setup/SKILL.md

This will involve the following end-to-end workflow:

- **Read plan manifests** ‚Äî extract TVF, Metric View, and Genie Space specifications from the semantic-layer-manifest.yaml (from Step 13 planning)
- **Create Metric Views** ‚Äî build Metric Views using `WITH METRICS LANGUAGE YAML` syntax with dimensions, measures, 3-5 synonyms each, and format specifications
- **Create Table-Valued Functions (TVFs)** ‚Äî write parameterized SQL functions with STRING date params (non-negotiable for Genie), v3.0 bullet-point COMMENTs, and ROW_NUMBER for Top-N patterns
- **Configure Genie Space** ‚Äî set up natural language query interface with data assets (Metric Views ‚Üí TVFs ‚Üí Gold tables priority), General Instructions (‚â§20 lines), and ‚â•10 benchmark questions with exact expected SQL
- **Create JSON exports** ‚Äî export Genie Space configuration as JSON for CI/CD deployment across environments
- **Optimize for accuracy** ‚Äî run benchmark questions via Conversation API and tune 6 control levers until accuracy ‚â•95% and repeatability ‚â•90%

Implement in this order:

1. **Table-Valued Functions (TVFs)** ‚Äî using plan at @data_product_accelerator/plans/phase1-addendum-1.2-tvfs.md
2. **Metric Views** ‚Äî using plan at @data_product_accelerator/plans/phase1-addendum-1.3-metric-views.md
3. **Genie Space** ‚Äî using plan at @data_product_accelerator/plans/phase1-addendum-1.6-genie-spaces.md
4. **Genie JSON Exports** ‚Äî create export/import deployment jobs

The orchestrator skill automatically loads worker skills for TVFs, Metric Views, Genie Space patterns, and export/import API.',
'',
'Build Genie Space [Metric Views/TVFs]',
'Create semantic layer with TVFs, Metric Views, and Genie Space for natural language analytics',
15,
'## 1Ô∏è‚É£ How To Apply

Copy the prompt above, start a **new Agent chat** in Cursor, and paste it. The AI will process all 4 implementation steps in order.

---

### Prerequisite

**Run this in your cloned Template Repository** (see Prerequisites in Step 0).

Ensure you have:
- ‚úÖ Gold Layer Implementation completed (Step 12) ‚Äî with column COMMENTs on all tables
- ‚úÖ Use-Case Plan created (Step 13) ‚Äî with `planning_mode: workshop`
- ‚úÖ Plan manifest exists: `plans/manifests/semantic-layer-manifest.yaml`
- ‚úÖ Plan addendum files exist:
  - `plans/phase1-addendum-1.2-tvfs.md`
  - `plans/phase1-addendum-1.3-metric-views.md`
  - `plans/phase1-addendum-1.6-genie-spaces.md`
- ‚úÖ Gold YAML schemas available in `gold_layer_design/yaml/` (for schema validation)

---

### Steps to Apply

**Step 1: Start New Agent Thread** ‚Äî Open Cursor and start a new Agent thread for clean context.

**Step 2: Copy and Paste the Prompt** ‚Äî Copy the entire prompt using the copy button, paste it into Cursor. The AI will process all 4 implementation steps in order.

**Step 3: Phase 0 ‚Äî Plan Reading** ‚Äî The AI will read `plans/manifests/semantic-layer-manifest.yaml` (implementation checklist), extract exact TVF names, Metric View specs, Genie Space configuration. If no manifest exists, fall back to self-discovery from Gold tables.

**Step 4: Phase 1 ‚Äî Metric Views** ‚Äî The AI will read Metric View plan (`plans/phase1-addendum-1.3-metric-views.md`), create YAML definition files (dimensions, measures, synonyms, formats), create `create_metric_views.py` (reads YAML ‚Üí `CREATE VIEW WITH METRICS LANGUAGE YAML`), create `metric_views_job.yml` for Asset Bundle deployment.

**Step 5: Phase 2 ‚Äî TVFs** ‚Äî The AI will read TVF plan (`plans/phase1-addendum-1.2-tvfs.md`), validate Gold YAML schemas (confirm column names/types exist), create `table_valued_functions.sql` with v3.0 bullet-point COMMENTs, create `tvf_job.yml` (SQL task) for Asset Bundle deployment.

**Step 6: Phase 3 ‚Äî Genie Space** ‚Äî The AI will read Genie Space plan (`plans/phase1-addendum-1.6-genie-spaces.md`), verify ALL Gold tables have column COMMENTs (prerequisite), configure: data assets (MVs ‚Üí TVFs ‚Üí tables), General Instructions (‚â§20 lines), create ‚â•10 benchmark questions with exact expected SQL.

**Step 7: Deploy and Validate**

```bash
# Deploy all semantic layer jobs
databricks bundle deploy -t dev
databricks bundle run tvf_job -t dev
databricks bundle run metric_views_job -t dev
```

```sql
-- Test TVFs (note: STRING date params, not DATE)
SELECT * FROM get_revenue_by_period(''2024-01-01'', ''2024-12-31'');
SELECT * FROM get_top_properties_by_revenue(''2024-01-01'', ''2024-12-31'', 10);

-- Verify Metric View created correctly
SELECT table_name, table_type 
FROM information_schema.tables 
WHERE table_schema = ''{user_schema_prefix}_gold'' AND table_type = ''METRIC_VIEW'';
```

**Step 8: Phase 5 ‚Äî Optimization Loop** ‚Äî After Genie Space is created: run benchmark questions via Conversation API, check accuracy (target: ‚â• 95%) and repeatability (target: ‚â• 90%), apply 6 control levers if targets not met (UC metadata ‚Üí Metric Views ‚Üí TVFs ‚Üí Monitoring ‚Üí ML ‚Üí Genie Instructions), re-test until targets achieved.

---

## 2Ô∏è‚É£ What Are We Building?

### What is the Semantic Layer?

The **Semantic Layer** sits between your Gold data and end users, providing:
- **Natural language** access to data
- **Standardized metrics** with business definitions
- **Reusable query patterns** via functions

### Semantic Layer Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          SEMANTIC LAYER STACK                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                   GENIE SPACE (Phase 3)                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ              Natural Language Interface                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   "What is our total revenue this month by destination?"            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Serverless SQL Warehouse ‚îÇ ‚â§20-line Instructions ‚îÇ ‚â•10 Benchmarks ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                 ‚îÇ Data Asset Priority:                      ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ
‚îÇ                    ‚îÇ 1st choice   2nd choice ‚îÇ                             ‚îÇ
‚îÇ                    ‚ñº                         ‚ñº                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ  METRIC VIEWS (Phase 1) ‚îÇ   ‚îÇ    TVFs (Phase 2)       ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  WITH METRICS YAML      ‚îÇ   ‚îÇ  STRING date params     ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ                         ‚îÇ   ‚îÇ                         ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Dimensions + Synonyms‚îÇ   ‚îÇ  ‚Ä¢ get_revenue_by_period‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Measures + Formats   ‚îÇ   ‚îÇ  ‚Ä¢ get_top_properties   ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Joins (snowflake)    ‚îÇ   ‚îÇ  ‚Ä¢ get_host_performance ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ v1.1 specification   ‚îÇ   ‚îÇ  ‚Ä¢ v3.0 bullet comments ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ               ‚îÇ                             ‚îÇ                              ‚îÇ
‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ
‚îÇ                              ‚îÇ 3rd choice (raw tables)                     ‚îÇ
‚îÇ                              ‚ñº                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                    GOLD LAYER (prerequisite)                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   dim_property ‚îÇ dim_host ‚îÇ dim_user ‚îÇ fact_booking_detail ‚îÇ ...    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   All tables must have column COMMENTs before Genie Space creation  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                    OPTIMIZATION (Phase 5)                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Benchmark ‚Üí Test ‚Üí Apply 6 Levers ‚Üí Re-test                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Target: Accuracy ‚â• 95%  ‚îÇ  Repeatability ‚â• 90%                   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Why This Order Matters

| Phase | Artifact | Depends On | Enables | Non-Negotiable Rule |
|-------|----------|------------|---------|---------------------|
| 0 | **Read Plan** | Semantic layer manifest | All phases | Extract specs from plan, don''t generate |
| 1 | **Metric Views** | Gold tables + COMMENTs | Genie + Dashboards | `WITH METRICS LANGUAGE YAML` syntax |
| 2 | **TVFs** | Gold YAML schemas | Genie NL queries | All date params STRING (not DATE) |
| 3 | **Genie Space** | MVs + TVFs + COMMENTs | End-user queries | ‚â•10 benchmarks, Serverless warehouse |
| 4 | **JSON Export** | Genie Space | CI/CD deployment | Variable substitution for env portability |
| 5 | **Optimization** | Genie Space deployed | Production readiness | ‚â•95% accuracy, ‚â•90% repeatability |

**Build bottom-up:** Metric Views and TVFs FIRST (both depend only on Gold), then Genie Space (depends on both), then Optimize.

---

### The Three Semantic Components

### 1Ô∏è‚É£ Table-Valued Functions (TVFs)

**What:** Parameterized SQL functions that return tables.

```sql
-- Example TVF (v3.0 bullet-point comment format)
CREATE OR REPLACE FUNCTION get_top_properties_by_revenue(
  start_date STRING COMMENT ''Start date (format: YYYY-MM-DD)'',
  end_date STRING COMMENT ''End date (format: YYYY-MM-DD)'',
  top_n INT DEFAULT 10 COMMENT ''Number of top properties to return''
)
RETURNS TABLE(
  rank INT COMMENT ''Property rank by revenue'',
  property_name STRING COMMENT ''Property display name'',
  destination STRING COMMENT ''Property location'',
  total_revenue DECIMAL(18,2) COMMENT ''Total booking revenue for period''
)
COMMENT ''
‚Ä¢ PURPOSE: Returns top N properties ranked by booking revenue for a date range
‚Ä¢ BEST FOR: "top properties by revenue" | "best performing properties" | "highest earning rentals"
‚Ä¢ RETURNS: Individual property rows (rank, name, destination, revenue)
‚Ä¢ PARAMS: start_date, end_date (YYYY-MM-DD), top_n (default: 10)
‚Ä¢ SYNTAX: SELECT * FROM get_top_properties_by_revenue(''''2024-01-01'''', ''''2024-12-31'''', 10)
''
RETURN
  WITH ranked AS (
    SELECT 
      p.property_name,
      d.destination_name as destination,
      SUM(f.total_amount) as total_revenue,
      ROW_NUMBER() OVER (ORDER BY SUM(f.total_amount) DESC) as rank
    FROM {lakehouse_default_catalog}.{user_schema_prefix}_gold.fact_booking_detail f
    JOIN {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_property p 
      ON f.property_id = p.property_id AND p.is_current = true
    JOIN {lakehouse_default_catalog}.{user_schema_prefix}_gold.dim_destination d 
      ON f.destination_id = d.destination_id
    WHERE f.booking_date BETWEEN CAST(start_date AS DATE) AND CAST(end_date AS DATE)
    GROUP BY p.property_name, d.destination_name
  )
  SELECT rank, property_name, destination, total_revenue
  FROM ranked
  WHERE rank <= top_n;
```

**‚ö†Ô∏è Critical TVF Rules:**
- ‚úÖ **STRING for date params** ‚Äî Genie passes dates as strings. DATE type breaks Genie SQL generation.
- ‚úÖ **ROW_NUMBER + WHERE** for Top N ‚Äî never `LIMIT {param}` (SQL compilation error)
- ‚úÖ **v3.0 bullet-point COMMENT** ‚Äî `‚Ä¢ PURPOSE:`, `‚Ä¢ BEST FOR:`, `‚Ä¢ RETURNS:`, `‚Ä¢ PARAMS:`, `‚Ä¢ SYNTAX:`
- ‚úÖ **SCD2 filter** ‚Äî `AND p.is_current = true` on dimension joins
- ‚úÖ **NULLIF** for all divisions ‚Äî prevents divide-by-zero errors

---

### 2Ô∏è‚É£ Metric Views

**What:** Semantic definitions with dimensions, measures, and synonyms using Databricks'' `WITH METRICS LANGUAGE YAML` syntax.

```sql
-- Metric Views use YAML syntax, NOT regular SQL views:
CREATE OR REPLACE VIEW {lakehouse_default_catalog}.{user_schema_prefix}_gold.revenue_analytics_metrics
WITH METRICS
LANGUAGE YAML
COMMENT ''PURPOSE: Revenue and booking analytics...''
AS $$
version: "1.1"

source: {lakehouse_default_catalog}.{user_schema_prefix}_gold.fact_booking_detail

dimensions:
  - name: destination
    expr: source.destination_name
    comment: Travel destination for geographic analysis
    display_name: Destination
    synonyms: [location, city, travel destination]

measures:
  - name: total_revenue
    expr: SUM(source.total_amount)
    comment: Total booking revenue in USD
    display_name: Total Revenue
    format:
      type: currency
      currency_code: USD
    synonyms: [revenue, earnings, income, amount]

  - name: booking_count
    expr: COUNT(*)
    comment: Number of bookings
    display_name: Booking Count
    synonyms: [bookings, reservations, count]
$$
```

**‚ö†Ô∏è Critical Metric View Rules:**
- ‚úÖ **`WITH METRICS LANGUAGE YAML`** ‚Äî NOT regular `CREATE VIEW` with TBLPROPERTIES
- ‚úÖ **`AS $$ ... $$`** ‚Äî YAML wrapped in dollar-quote delimiters (no SELECT)
- ‚úÖ **`version: "1.1"`** ‚Äî required in every metric view YAML
- ‚úÖ **3-5 synonyms** per dimension/measure ‚Äî dramatically improves Genie NL accuracy
- ‚úÖ **Format specs** ‚Äî currency, percentage, number for proper display

---

### 3Ô∏è‚É£ Genie Space

**What:** Natural language interface to your data, configured with a **7-section deliverable structure.**

**Required sections:**

| # | Section | Requirement |
|---|---------|-------------|
| 1 | **Name & Description** | Domain-specific, descriptive name |
| 2 | **Data Assets** | Priority order: Metric Views ‚Üí TVFs ‚Üí Gold Tables (‚â§ 25 total) |
| 3 | **General Instructions** | ‚â§ 20 lines: table preferences, defaults, disambiguation |
| 4 | **Benchmark Questions** | ‚â• 10 questions with exact expected SQL |
| 5 | **Sample Questions** | 5-10 curated examples shown to users |
| 6 | **Warehouse** | Serverless SQL Warehouse (non-negotiable) |
| 7 | **Column Comments** | Verify ALL Gold tables have COMMENTs before creation |

**Data Asset Priority:** Genie uses Metric Views FIRST (pre-aggregated), then TVFs (parameterized), then raw Gold tables. This priority order maximizes accuracy.

**Example benchmark question (with exact SQL):**
```
Q: "What is our total revenue this month?"
SQL: SELECT MEASURE(total_revenue) FROM revenue_analytics_metrics
     WHERE booking_date >= DATE_TRUNC(''month'', CURRENT_DATE())
```

---

---

## üí° TVF Design Best Practices

### v3.0 Bullet-Point Comment Format (CRITICAL for Genie)

```sql
COMMENT ''
‚Ä¢ PURPOSE: [One-line description of what it returns]
‚Ä¢ BEST FOR: [Question 1] | [Question 2] | [Question 3]
‚Ä¢ RETURNS: [Description of output rows ‚Äî what each row represents]
‚Ä¢ PARAMS: [param1] (required), [param2] (optional, default: X)
‚Ä¢ SYNTAX: SELECT * FROM function_name(''''value1'''', ''''value2'''')
''
```

> **Why bullet format?** Genie''s SQL generation engine parses these structured comments to decide WHEN to invoke a TVF and WHICH parameters to pass. Unstructured prose comments reduce Genie accuracy.

### Parameter Rules (Non-Negotiable)

| Rule | Do This | Never Do This |
|------|---------|---------------|
| **Date params** | `start_date STRING COMMENT ''Format: YYYY-MM-DD''` | ‚ùå `start_date DATE` (breaks Genie) |
| **Param ordering** | Required first, DEFAULT params last | ‚ùå Optional before required |
| **Top N** | `ROW_NUMBER() OVER(...) + WHERE rank <= top_n` | ‚ùå `LIMIT top_n` (SQL error in TVF) |
| **Null safety** | `NULLIF(denominator, 0)` for all divisions | ‚ùå Bare division (divide-by-zero) |
| **SCD2 joins** | `AND dim.is_current = true` | ‚ùå Joining without SCD2 filter (duplicates) |

### Schema Validation BEFORE Writing SQL

**100% of TVF compilation errors are caused by not consulting Gold YAML schemas first.**

```python
# ALWAYS validate before writing SQL:
# 1. Read gold_layer_design/yaml/{domain}/{table}.yaml
# 2. Confirm column names and types exist
# 3. Then write TVF SQL using validated names
```

---

## üí° Metric View Best Practices

### COMMENT Format (on the CREATE VIEW, not inside YAML)

```sql
COMMENT ''PURPOSE: Revenue and booking analytics by property and destination.
BEST FOR: "total revenue" | "bookings by destination" | "average nightly rate"
NOT FOR: Host-level metrics (use host_performance_metrics instead)
DIMENSIONS: destination, property_type, booking_month
MEASURES: total_revenue, booking_count, avg_nightly_rate
SOURCE: fact_booking_detail (bookings domain)''
```

### Schema Validation (100% Error Prevention)

```python
# Before writing YAML, validate column names exist:
# 1. Read gold_layer_design/yaml/bookings/fact_booking_detail.yaml
# 2. Confirm "destination_name", "total_amount", "property_type" exist
# 3. Only THEN write dimension/measure expressions using validated names
```

### Synonym Guidelines (3-5 per field)

```yaml
synonyms:
  - exact_alternative    # "revenue" for "total_revenue"
  - business_term        # "earnings" for "total_revenue"
  - abbreviation         # "qty" for "quantity"
  - common_variation     # "amount" for "total_amount"
  - colloquial           # "income" for "total_revenue"
```

> **Why 3-5?** Fewer synonyms miss natural language variations. More than 5 creates ambiguity where Genie can''t distinguish which measure the user means.

---

## üí° Genie Space Configuration

### General Instructions (‚â§ 20 Lines)

```
-- These instructions tell Genie HOW to query your data:
1. For revenue queries, prefer revenue_analytics_metrics (Metric View) first
2. For parameterized queries (date ranges, top-N), use TVFs
3. For detail-level queries, use Gold tables directly
4. Default date range: last 30 days if not specified
5. Always join dimensions with is_current = true (SCD2)
6. For host queries, use dim_host; for property queries, use dim_property
7. Revenue = SUM(total_amount) from fact_booking_detail
8. When asked "top N", use get_top_properties_by_revenue TVF
```

> **Why ‚â§ 20 lines?** Genie''s instruction processing degrades with too many rules. Focus on table preferences, defaults, and common disambiguation.

### Benchmark Questions (Minimum 10, with Exact SQL)

```
-- Each benchmark includes the question AND the expected SQL:
Q1: "What is total revenue this month?"
SQL: SELECT MEASURE(total_revenue) FROM revenue_analytics_metrics WHERE ...

Q2: "Top 10 properties by revenue last year"
SQL: SELECT * FROM get_top_properties_by_revenue(''2025-01-01'', ''2025-12-31'', 10)

Q3: "How many bookings per destination?"
SQL: SELECT destination, MEASURE(booking_count) FROM revenue_analytics_metrics GROUP BY ...
-- ... (minimum 10 total)
```

> **Why exact SQL?** Benchmark SQL enables automated testing via the Conversation API ‚Äî you can programmatically verify Genie generates correct queries.

---

## 3Ô∏è‚É£ Why Are We Building It This Way? (Databricks Best Practices)

| Practice | How It''s Used Here |
|----------|-------------------|
| **Metric View `WITH METRICS LANGUAGE YAML`** | Metric views use Databricks'' native YAML syntax (`CREATE VIEW ... WITH METRICS LANGUAGE YAML AS $$ ... $$`) ‚Äî NOT regular views with TBLPROPERTIES |
| **TVFs with STRING Parameters** | All TVF date parameters use STRING type ‚Äî non-negotiable for Genie compatibility. Genie passes dates as strings; DATE type breaks SQL generation. |
| **v3.0 Bullet-Point Comments** | `‚Ä¢ PURPOSE:`, `‚Ä¢ BEST FOR:`, `‚Ä¢ RETURNS:`, `‚Ä¢ PARAMS:`, `‚Ä¢ SYNTAX:` ‚Äî Genie parses these structured bullets to decide when to invoke each TVF |
| **Schema Validation Before SQL** | Always read Gold YAML schemas before writing TVF SQL. 100% of compilation errors are caused by referencing non-existent columns. |
| **ROW_NUMBER for Top-N** | Never `LIMIT {param}` in TVFs (SQL compilation error). Use `ROW_NUMBER() OVER(...) + WHERE rank <= top_n` instead. |
| **SCD2 Filter on Dimension Joins** | Every TVF joining dimensions must include `AND dim.is_current = true` ‚Äî omitting this causes row duplication from historical SCD2 records |
| **Genie Space General Instructions** | ‚â§20 lines of focused instructions telling Genie which tables to prefer, default time ranges, and disambiguation rules |
| **Minimum 10 Benchmark Questions** | Each Genie Space requires ‚â• 10 benchmark questions with exact expected SQL ‚Äî enables automated accuracy testing via the Conversation API |
| **Column Comments Required** | All Gold tables must have column COMMENTs BEFORE creating a Genie Space ‚Äî Genie uses these to understand column semantics for SQL generation |
| **Export/Import API for CI/CD** | Genie Space configuration exported as JSON ‚Äî enables version-controlled deployment across dev/staging/prod environments |
| **Optimization Loop (6 Levers)** | Iterative tuning: UC metadata ‚Üí Metric Views ‚Üí TVFs ‚Üí Monitoring tables ‚Üí ML tables ‚Üí Genie Instructions, targeting 95%+ accuracy, 90%+ repeatability |
| **Serverless SQL Warehouse** | Genie Spaces MUST use a Serverless SQL warehouse ‚Äî required for natural language query execution. NEVER Classic or Pro. |
| **Synonym-Rich Definitions** | 3-5 synonyms per dimension/measure (e.g., "revenue" ‚Üí "earnings", "income", "amount") ‚Äî dramatically improves Genie NL understanding |

---

## 4Ô∏è‚É£ What Happens Behind the Scenes?

When you paste the prompt, the AI reads `@data_product_accelerator/skills/semantic-layer/00-semantic-layer-setup/SKILL.md` ‚Äî the **Semantic Layer orchestrator**. Behind the scenes:

1. **Phase 0: Read Plan** ‚Äî the orchestrator first looks for `plans/manifests/semantic-layer-manifest.yaml`. If found, it uses this as the implementation checklist (every TVF, Metric View, and Genie Space pre-defined). If not found, it falls back to self-discovery from Gold tables.
2. **5 Worker skills auto-loaded:**
   - `01-metric-views-patterns` ‚Äî `WITH METRICS LANGUAGE YAML` syntax, schema validation, join patterns (including snowflake schema)
   - `02-databricks-table-valued-functions` ‚Äî STRING parameters (non-negotiable), v3.0 bullet-point comments, Top-N via ROW_NUMBER, SCD2 handling
   - `03-genie-space-patterns` ‚Äî 7-section deliverable structure, General Instructions (‚â§20 lines), minimum 10 benchmark questions
   - `04-genie-space-export-import-api` ‚Äî REST API JSON schema for programmatic Genie Space deployment (CI/CD)
   - `05-genie-space-optimization` ‚Äî iterative 6-lever optimization loop targeting 95%+ accuracy, 90%+ repeatability
3. **5 Common skills auto-loaded:**
   - `databricks-expert-agent` ‚Äî "Extract, Don''t Generate" applied to all schema references
   - `databricks-asset-bundles` ‚Äî SQL task jobs for TVF deployment, Python jobs for Metric Views
   - `databricks-python-imports` ‚Äî pure Python module patterns for Metric View creation scripts
   - `naming-tagging-standards` ‚Äî enterprise naming for all semantic layer artifacts
   - `databricks-autonomous-operations` ‚Äî self-healing deploy loop when jobs fail
4. **Phase-ordered execution:** Metric Views ‚Üí TVFs ‚Üí Genie Space ‚Üí API Export ‚Üí Optimization. Each phase only begins after the previous completes.
5. **Phase 5: Optimization Loop** ‚Äî after Genie Space creation, the orchestrator runs benchmark questions via the Conversation API and tunes 6 control levers (UC metadata, Metric Views, TVFs, Monitoring tables, ML tables, Genie Instructions) until accuracy ‚â•95% and repeatability ‚â•90%.

**Key principle:** The AI reads your plan manifest to **extract** specifications ‚Äî it doesn''t generate them from scratch. This ensures the semantic layer matches your approved plan exactly.',
'## Expected Deliverables

### üìÅ Semantic Layer Files Created

```
src/source_gold/
‚îú‚îÄ‚îÄ table_valued_functions.sql           # All TVFs in one SQL file (3-5 functions)
‚îú‚îÄ‚îÄ semantic/
‚îÇ   ‚îî‚îÄ‚îÄ metric_views/
‚îÇ       ‚îú‚îÄ‚îÄ revenue_analytics_metrics.yaml   # Metric view YAML definition
‚îÇ       ‚îî‚îÄ‚îÄ create_metric_views.py           # Script: reads YAML ‚Üí CREATE VIEW WITH METRICS
‚îú‚îÄ‚îÄ genie/
‚îÇ   ‚îî‚îÄ‚îÄ genie_space_config.json          # Exported Genie Space config (CI/CD)
resources/
‚îú‚îÄ‚îÄ semantic-layer/
‚îÇ   ‚îú‚îÄ‚îÄ tvf_job.yml                      # SQL task to deploy TVFs
‚îÇ   ‚îú‚îÄ‚îÄ metric_views_job.yml             # Python task to deploy Metric Views
‚îÇ   ‚îî‚îÄ‚îÄ genie_deploy_job.yml             # Genie Space import job (optional)
```

**TVF Count:** 3-5 functions (workshop mode) ‚Äî one per parameter pattern (date-range, entity-filter, top-N)

---

### üìä Metric View Deployment Pattern

Each Metric View is created via a Python script that reads YAML and runs:

```python
# create_metric_views.py reads YAML ‚Üí generates DDL
create_sql = f"""
CREATE OR REPLACE VIEW {lakehouse_default_catalog}.{user_schema_prefix}_gold.{view_name}
WITH METRICS
LANGUAGE YAML
COMMENT ''{view_comment}''
AS $$
{yaml_content}
$$
"""
spark.sql(create_sql)
```

> **Key:** Metric Views use `WITH METRICS LANGUAGE YAML` ‚Äî NOT regular views with TBLPROPERTIES. This is a non-negotiable syntax requirement.

**Metric View Count:** 1-2 metric views (workshop mode) ‚Äî one per fact table with richest dimension joins

---

### üìä TVF Summary Table (Workshop Scope: 3-5 TVFs)

| Pattern | Function | Parameters (all STRING for dates) | Returns |
|---------|----------|----------------------------------|---------|
| **Date Range** | `get_revenue_by_period` | start_date STRING, end_date STRING | Revenue aggregates by destination |
| **Top-N** | `get_top_properties_by_revenue` | start_date STRING, end_date STRING, top_n INT | Top N properties ranked by revenue |
| **Entity Filter** | `get_host_performance` | host_id STRING DEFAULT NULL, min_bookings STRING DEFAULT ''5'' | Host performance metrics |

> **Workshop selection:** One per parameter pattern to teach the full TVF vocabulary. Production would add 10-15 more.

---

### üìä Metric View Summary (Workshop Scope: 1-2)

| Metric View | Source | Dimensions | Measures | Synonyms |
|-------------|--------|------------|----------|----------|
| `revenue_analytics_metrics` | fact_booking_detail + dim_property + dim_destination | destination, property_type, booking_month | total_revenue, booking_count, avg_nightly_rate | revenue‚Üíearnings, bookings‚Üíreservations |

> **Workshop selection:** One metric view with richest joins to demonstrate full YAML syntax (dimensions, measures, joins, formats, synonyms).

---

### üîó Genie Space Configuration (1 Unified Space)

| Element | Value |
|---------|-------|
| **Name** | Analytics |
| **Data Assets** | 1 Metric View + 3-5 TVFs + 4 Gold Tables (< 15 total) |
| **General Instructions** | ‚â§ 20 lines (table preferences, defaults, disambiguation) |
| **Benchmark Questions** | ‚â• 10 with exact expected SQL |
| **Sample Questions** | 5-10 curated examples shown to users |
| **Warehouse** | Serverless SQL Warehouse (non-negotiable) |
| **Optimization Target** | Accuracy ‚â• 95%, Repeatability ‚â• 90% |

---

### ‚úÖ Success Criteria Checklist

**TVFs (non-negotiable):**
- [ ] All date parameters use STRING type (never DATE ‚Äî breaks Genie)
- [ ] v3.0 bullet-point COMMENT format on every TVF (`‚Ä¢ PURPOSE:`, `‚Ä¢ BEST FOR:`, etc.)
- [ ] Top-N uses `ROW_NUMBER() + WHERE rank <=` (never `LIMIT {param}`)
- [ ] SCD2 dimension joins include `AND dim.is_current = true`
- [ ] `NULLIF(denominator, 0)` for all divisions
- [ ] Schema validated against Gold YAML before writing SQL
- [ ] 3-5 TVFs created (workshop mode)

**Metric Views (non-negotiable):**
- [ ] Created with `WITH METRICS LANGUAGE YAML` syntax (not regular VIEW)
- [ ] `table_type = ''METRIC_VIEW''` in `information_schema.tables`
- [ ] 3-5 synonyms per dimension/measure
- [ ] Format specifications (currency, percentage) where applicable
- [ ] Source table references validated against Gold YAML
- [ ] 1-2 metric views created (workshop mode)

**Genie Space (non-negotiable):**
- [ ] All Gold tables have column COMMENTs (prerequisite verified)
- [ ] Uses Serverless SQL Warehouse (never Classic or Pro)
- [ ] General Instructions ‚â§ 20 lines
- [ ] ‚â• 10 benchmark questions with exact expected SQL
- [ ] Data assets: Metric Views ‚Üí TVFs ‚Üí Gold Tables (priority order)
- [ ] Total data assets ‚â§ 25 per space (< 15 for workshop)
- [ ] Natural language queries producing correct SQL

**Optimization (target):**
- [ ] Accuracy ‚â• 95% (benchmark questions answered correctly)
- [ ] Repeatability ‚â• 90% (same question ‚Üí same SQL each time)
- [ ] 6-lever optimization applied if targets not met

**Deployment:**
- [ ] `tvf_job.yml` ‚Äî SQL task for TVF deployment
- [ ] `metric_views_job.yml` ‚Äî Python task for Metric View deployment
- [ ] JSON export created for Genie Space CI/CD (optional)
- [ ] `databricks bundle deploy -t dev` succeeds',
true, 1, true, current_timestamp(), current_timestamp(), current_user());

-- Build Agent
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(13, 'agent_framework',
'Build a multi-agent orchestrator for this project by analyzing the PRD document from @docs/design_prd.md and the UI design documents from @docs/ folder.

Refer to the instruction from @vibe-coding-workshop-template/agentic-framework/agents/multi-agent-build-prompt.md to build the agentic framework.
',
'You are a senior full-stack engineer implementing advanced agentic search capabilities.',
'Build Agent',
'Build advanced agentic search with Genie integration, LLM rewrite, and web search fallback',
16,
'
## Steps to Apply

1. **Copy the generated prompt** using the copy button
2. **Paste into Cursor or VS Code** with Copilot
3. Let the AI **analyze the PRD and UI design structure**
4. **Implement** agent orchestrator and tool calling
5. **Test** with mock data
',
'## Expected Agent Deliverables

### Backend Modules

| Module | Purpose |
|--------|---------|
| `genie_client` | Genie Space integration |
| `llm_client` | LLM rewrite functionality |
| `web_search_client` | Web search fallback |

### API Endpoints

- `POST /api/search/standard`
- `POST /api/search/nl`
- `POST /api/search/assistant`

### Configuration

- `.env.example` with all required variables
- Mock mode for local development

### Tests

- Unit tests for LLM parsing
- Unit tests for Genie no-answer detection
- Integration tests in mock mode',
true, 1, TRUE, current_timestamp(), current_timestamp(), current_user());

-- Step 19: Wire UI to Agent - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(113, 'wire_ui_agent',
'## Task: Wire Frontend UI to Agent Serving Endpoint

Connect your web application''s frontend to the Agent serving endpoint built in the previous step (Build Agent). This enables end-to-end natural language search in your application.

Refer to the instruction from @vibe-coding-workshop-template/agentic-framework/agents/agent-ui-wiring-prompt.md to wire the multi-agent system with the UI.',
'',
'Wire UI to Agent',
'Connect frontend UI to the Agent serving endpoint for end-to-end natural language search',
17,
'## What is Wire UI to Agent?

This step connects your frontend application to the AI Agent built in the previous step. 

---

## Steps to Apply

1. Copy the generated prompt using the copy button
2. Paste it into Cursor or VS Code with Copilot
4. Test the Agent''s response.

**Note:** This step requires the Build Agent step to be completed first. The Agent serving endpoint must be deployed and accessible.',
'## Expected Deliverables

- Agent endpoint configured in `apps_lakebase/app.yaml`
- `agent_client.py` module with query function and mock mode
- Backend API route `POST /api/agent/query` responding correctly
- Frontend NL search wired to agent endpoint
- Results rendering inline without page navigation
- Local testing passed in both mock and live modes

**Next Step:** Iterate and enhance your application in Step 20',
true, 1, TRUE, current_timestamp(), current_timestamp(), current_user());

-- Iterate & Enhance App
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, version, is_active, inserted_at, updated_at, created_by)
VALUES
(14, 'iterate_enhance',
'Iterate and enhance the application based on user feedback and business needs.

---

## Potential Enhancements

Review the current application and identify areas for improvement:

### UI/UX Improvements
- Dark mode support
- Better visualizations and charts
- Improved navigation and user flows
- Mobile responsiveness
- Accessibility improvements

### Data Features
- Additional filters and search capabilities
- Data export functionality (CSV, Excel, PDF)
- Saved views and bookmarks
- Custom dashboards per user

### Agent Enhancements
- Additional tools and capabilities
- Conversation history and context
- Multi-turn conversations
- Integration with more data sources

### Performance Optimizations
- Query caching strategies
- Pagination for large datasets
- Lazy loading for UI components
- Database query optimization

### Integration Enhancements
- Additional data source connections
- External API integrations
- Webhook notifications
- SSO/authentication improvements

---

## Iteration Process

### Step 1: Gather User Feedback
- Conduct user interviews
- Review usage analytics
- Collect feature requests
- Identify pain points

### Step 2: Prioritize Enhancements
Use MoSCoW method:
- **Must Have**: Critical for user success
- **Should Have**: Important but not critical
- **Could Have**: Nice to have
- **Won''t Have**: Out of scope for now

### Step 3: Plan Implementation
- Break down into sprints
- Estimate effort for each enhancement
- Identify dependencies
- Create implementation tickets

### Step 4: Implement Changes
- Work on one enhancement at a time
- Write tests for new features
- Document changes
- Review code before merging

### Step 5: Test and Validate
- Unit tests for new functionality
- Integration tests for workflows
- User acceptance testing
- Performance testing

### Step 6: Deploy and Monitor
- Deploy to staging first
- Validate in staging environment
- Deploy to production
- Monitor for issues

---

## Industry Context
Industry: {industry}
Use Case: {use_case}

Review the current implementation and identify enhancements specific to the {industry} {use_case} use case.',
'You are a product manager and developer specializing in iterative application development.
Generate a detailed, actionable prompt for enhancing the application based on user feedback.
Focus on:
- Identifying high-impact improvements
- Prioritizing based on user value
- Breaking down into manageable tasks
- Ensuring quality through testing',
'Iterate & Enhance App',
'Iterate on the application to add new features, update functionality, and improve user experience',
18,
'## Prerequisite

**Run this in your cloned Template Repository** (see Prerequisites in Step 0). These prompts assume you are working in that codebase with a coding assistant (Cursor or Copilot) enabled.

---

## Steps to Iterate and Enhance

### Step 1: Review Current State
```
@codebase What are the main features of this application? 
What areas could be improved?
```

### Step 2: Gather Feedback
- Review user feedback
- Analyze usage patterns
- Identify pain points

### Step 3: Prioritize Enhancements
- Use MoSCoW method
- Consider effort vs impact
- Plan sprint backlog

### Step 4: Implement Changes
- One enhancement at a time
- Write tests
- Document changes

### Step 5: Test and Deploy
- Run all tests
- Deploy to staging
- Validate and deploy to production',
'## Expected Enhancement Outcomes

### UI/UX Improvements
- [ ] Dark mode implemented
- [ ] Better visualizations
- [ ] Improved navigation

### Data Features
- [ ] Export functionality
- [ ] Advanced filters
- [ ] Saved views

### Performance
- [ ] Faster load times
- [ ] Optimized queries
- [ ] Better caching

### Documentation
- [ ] Updated user guide
- [ ] API documentation
- [ ] Release notes',
1, TRUE, current_timestamp(), current_timestamp(), current_user());

-- Step 21: Redeploy & Test Application (Autonomous Operations + Repository Documentation) - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(15, 'redeploy_test',
'Build, deploy, and test the complete application using @data_product_accelerator/skills/common/databricks-autonomous-operations/SKILL.md for self-healing deployment and @data_product_accelerator/skills/common/databricks-asset-bundles/SKILL.md for DAB validation.

After deployment succeeds, document the entire repository using @data_product_accelerator/skills/admin/documentation-organization/SKILL.md in Framework Documentation Authoring mode.

---

## IMPORTANT: Analyze Current Project First

**This step uses your existing project deployment infrastructure.** Before deploying:

1. **Review the current project structure** to identify:
   - Deploy scripts (e.g., `deploy.sh`, `scripts/deploy.py`)
   - Build configurations (`package.json`, `requirements.txt`)
   - DAB configuration (`databricks.yml`)
   - Environment configurations

Use the AI assistant to analyze the project:
```
@codebase What deploy scripts and configurations exist in this project? 
How do I build and deploy this application to Databricks?
```

---

## Deployment Process (Self-Healing Loop)

Follow the autonomous operations skill''s core loop: **Deploy -> Poll -> Diagnose -> Fix -> Redeploy -> Verify** (max 3 iterations before escalation).

### Step 1: Identify Deployment Scripts
Look for existing scripts in your project:
```bash
# Common locations to check:
ls -la deploy.sh
ls -la scripts/
ls -la databricks.yml
cat package.json | grep scripts
```

### Step 2: Build the Application
Based on your project type:

**For React/Node.js frontend:**
```bash
npm install
npm run build
```

**For Python backend:**
```bash
pip install -r requirements.txt
```

### Step 3: Validate the Bundle (Pre-Deploy)
```bash
# Pre-flight validation catches ~80% of errors
databricks bundle validate -t dev
```
If validation fails, read the error, fix the YAML, and re-validate before proceeding.

### Step 4: Deploy Using Project Scripts
Use the deploy scripts found in your project:
```bash
# If deploy.sh exists:
./deploy.sh

# Or if using DAB:
databricks bundle deploy -t dev
```

### Step 5: Deploy DAB Artifacts
If you have Databricks Asset Bundles configured:
```bash
# Authenticate if needed
databricks auth login --host https://e2-demo-field-eng.cloud.databricks.com --profile DEFAULT

# Validate and deploy
databricks bundle validate
databricks bundle deploy -t dev

# Run jobs/pipelines (extract RUN_ID from output URL)
databricks bundle run <job_name> -t dev
```

### Step 6: Poll with Exponential Backoff
After triggering a job run, poll for completion:
```bash
# Poll job status (30s -> 60s -> 120s backoff)
databricks jobs get-run <RUN_ID> --output json | jq -r ''.state.life_cycle_state''
# PENDING -> RUNNING -> TERMINATED

# When TERMINATED, check result:
databricks jobs get-run <RUN_ID> --output json | jq -r ''.state.result_state''
# SUCCESS -> verify    FAILED -> diagnose
```

### Step 7: On Failure ‚Äî Diagnose
```bash
# CRITICAL: Use TASK run_id, NOT parent job run_id
databricks jobs get-run <JOB_RUN_ID> --output json \
  | jq ''.tasks[] | select(.state.result_state == "FAILED") | {task: .task_key, run_id: .run_id, error: .state.state_message}''

# Get detailed output for each failed task
databricks jobs get-run-output <TASK_RUN_ID> --output json \
  | jq -r ''.notebook_output.result // .error // "No output"''
```

### Step 8: Self-Healing Loop (Fix -> Redeploy -> Re-Poll)
1. Read the source file(s) identified from the error
2. Apply the fix
3. Redeploy: `databricks bundle deploy -t dev`
4. Re-run: `databricks bundle run -t dev <job_name>`
5. Return to Step 6 (Poll)

**Maximum 3 iterations.** After 3 failed attempts, escalate to user with all errors, fixes attempted, and run page URLs.

### Step 9: Verify Deployment
Check deployment status:
```bash
# Check app status
databricks apps get <app-name>

# View logs
databricks apps get <app-name> --output json | jq .app_status

# For multi-task jobs, verify all tasks succeeded:
databricks jobs get-run <RUN_ID> --output json \
  | jq ''.tasks[] | {task: .task_key, result: .state.result_state}''
```

---

## Testing Checklist

After deployment, verify:

### Application Health
- [ ] App URL is accessible
- [ ] `/api/health` returns 200 OK
- [ ] No errors in application logs

### Frontend Functionality
- [ ] UI loads without JavaScript errors
- [ ] Navigation works correctly
- [ ] Forms submit successfully
- [ ] Data displays in tables and charts

### Backend Functionality
- [ ] API endpoints respond correctly
- [ ] Database connections work
- [ ] Authentication/authorization works

### Data Pipelines (if DAB deployed)
- [ ] Bronze jobs completed successfully
- [ ] Silver pipeline processed data
- [ ] Gold tables populated correctly
- [ ] Data visible in dashboards/Genie

---

## Debugging Failed Deployments

If deployment fails:
1. Check build logs for errors
2. Verify environment variables are set
3. Check Databricks workspace permissions
4. Review app.yaml configuration
5. Check network connectivity

```bash
# View deployment logs
databricks apps get <app-name>

# Check bundle deployment status
databricks bundle validate
databricks bundle deploy -t dev --verbose
```

---

## Post-Deployment: Document the Entire Repository

**After deployment succeeds**, run this prompt in a new AI assistant thread:

```
Document this entire repository using @data_product_accelerator/skills/admin/documentation-organization/SKILL.md

Use Framework Documentation Authoring mode to create a complete docs/ set:
- Architecture overview with diagrams
- Component deep dives for each major module
- Deployment guide
- Operations guide (health checks, monitoring, alerting)
- Troubleshooting guide (common errors and solutions)

Also run organizational enforcement:
- Audit root directory for stray .md files
- Move any misplaced docs to correct docs/ subdirectory
- Validate all naming uses kebab-case
```

This generates comprehensive project documentation under `docs/{project-name}-design/`.',
'',
'Redeploy & Test Application',
'Use project deploy scripts and DAB to build, deploy, and test the complete application with self-healing operations and full repository documentation',
19,
'## üîÑ What is Redeploy and Test?

**Redeploy & Test** is not "deploy and pray" ‚Äî it is a **systematic, self-healing operational loop** powered by the autonomous operations skill. Every deployment follows a disciplined cycle: validate, deploy, poll, diagnose, fix, and verify. After deployment succeeds, the **entire repository** is documented comprehensively.

### Two Skills Working Together

| Skill | Role | When It Activates |
|-------|------|-------------------|
| **Autonomous Operations** | Self-healing deploy loop with diagnostics | During deployment and troubleshooting |
| **Documentation Organization** | Full repository documentation authoring | After deployment succeeds (explicit prompt) |

### Core Principles

| Principle | Benefit |
|-----------|---------|
| **Self-Healing Loop** | Deploy -> Poll -> Diagnose -> Fix -> Redeploy (max 3 iterations) |
| **Pre-Deploy Validation** | `databricks bundle validate` catches ~80% of errors before deploy |
| **Exponential Backoff** | 30s -> 60s -> 120s polling prevents API rate limits |
| **Task-Level Diagnostics** | Get output from failed tasks, not just the parent job |
| **Documentation as Final Step** | Every project gets architecture, operations, and troubleshooting docs |

---

## üèóÔ∏è Self-Healing Deployment Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AUTONOMOUS DEPLOY-TEST-DOCUMENT LOOP                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ VALIDATE ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  DEPLOY  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  POLL        ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  RESULT?     ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  Bundle  ‚îÇ    ‚îÇ  Bundle  ‚îÇ    ‚îÇ  (Backoff)   ‚îÇ    ‚îÇ              ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  30s‚Üí60s‚Üí120s‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ       ‚ñ≤                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ              ‚îÇ
‚îÇ       ‚îÇ                                                     ‚îÇ              ‚îÇ
‚îÇ       ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§              ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                                        ‚îÇ              ‚îÇ
‚îÇ       ‚îÇ            ‚ñº                                        ‚ñº              ‚îÇ
‚îÇ       ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ    FAILED    ‚îÇ                    ‚îÇ    SUCCESS       ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ              ‚îÇ                    ‚îÇ                  ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ ‚îÇ Diagnose ‚îÇ ‚îÇ                    ‚îÇ ‚îÇ Verify All   ‚îÇ ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ ‚îÇ (task-   ‚îÇ ‚îÇ                    ‚îÇ ‚îÇ Tasks + App  ‚îÇ ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ ‚îÇ  level)  ‚îÇ ‚îÇ                    ‚îÇ ‚îÇ Health       ‚îÇ ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ      ‚îÇ       ‚îÇ                    ‚îÇ        ‚îÇ         ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ      ‚ñº       ‚îÇ                    ‚îÇ        ‚ñº         ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ ‚îÇ   Fix    ‚îÇ ‚îÇ                    ‚îÇ ‚îÇ Document     ‚îÇ ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ ‚îÇ  Source  ‚îÇ ‚îÇ                    ‚îÇ ‚îÇ Entire Repo  ‚îÇ ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                    ‚îÇ ‚îÇ (Framework   ‚îÇ ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îÇ      ‚îÇ       ‚îÇ                    ‚îÇ ‚îÇ  Authoring)  ‚îÇ ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ      ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ                                                      ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                      ‚îÇ
‚îÇ       Redeploy (max 3 iterations)                                         ‚îÇ
‚îÇ                                                                           ‚îÇ
‚îÇ  After 3 failures ‚îÄ‚îÄ‚ñ∂ ESCALATE to user with all errors + run URLs        ‚îÇ
‚îÇ                                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìñ Key Concepts

| Concept | Why It Matters |
|---------|----------------|
| **Self-Healing Loop** | Deploy -> Poll -> Diagnose -> Fix -> Redeploy (max 3 iterations before escalation) |
| **Exponential Backoff** | 30s -> 60s -> 120s polling intervals prevent API rate limits and reduce noise |
| **Task-Level Diagnostics** | `get-run-output` needs the **TASK** `run_id`, not the parent job `run_id` ‚Äî critical for multi-task jobs |
| **Dependency Ordering** | Bronze -> Silver -> Gold -> Semantic -> Monitoring -> Alerts -> Genie |
| **Structured Notebook Exit** | `dbutils.notebook.exit(json.dumps({...}))` enables machine-parseable output retrieval |
| **Partial Success** | >=90% tasks succeeding = OK; fix individual failures without rerunning everything |
| **Full Repo Documentation** | Post-deployment step generates complete `docs/{project}-design/` with architecture, operations, troubleshooting |
| **Framework Doc Authoring** | 4-step workflow: Requirements Gathering -> File Structure -> Fill Templates -> Quality Validation |
| **Root Directory Hygiene** | Only README.md, QUICKSTART.md, CHANGELOG.md allowed in root; all other docs in `docs/` |
| **43-Item Quality Checklist** | Validates organization, naming, structure, content, usability, and maintenance of all documentation |

---

## Prerequisite

**Run this in your cloned Template Repository** (see Prerequisites in Step 0). These prompts assume you are working in that codebase with a coding assistant (Cursor or Copilot) enabled.

**Before this step, you should have completed:**
- Bronze, Silver, and Gold layer setup (tables populated)
- Semantic layer (Metric Views, TVFs, Genie Space)
- Any application code (frontend/backend)
- DAB configuration (`databricks.yml`)

---

## Steps to Apply

### Step 1: Analyze Project
```
@codebase What deploy scripts and configurations exist? How do I build and deploy?
```

### Step 2: Build Application
```bash
npm install && npm run build  # or equivalent for your project
pip install -r requirements.txt  # if Python backend
```

### Step 3: Validate Bundle (Pre-Deploy)
```bash
databricks bundle validate -t dev
# Catches ~80% of errors ‚Äî fix any issues before proceeding
```

### Step 4: Deploy Using Project Scripts
```bash
./deploy.sh  # or your project''''s deploy script
# Or: databricks bundle deploy -t dev
```

### Step 5: Deploy DAB Artifacts
```bash
databricks bundle deploy -t dev
databricks bundle run <job_name> -t dev
# Extract RUN_ID from the output URL
```

### Step 6: Poll with Exponential Backoff
```bash
# Poll: 30s -> 60s -> 120s intervals
databricks jobs get-run <RUN_ID> --output json | jq -r ''.state.life_cycle_state''
# When TERMINATED: check .state.result_state
```

### Step 7: On Failure ‚Äî Diagnose and Fix
```bash
# Get failed tasks (use TASK run_id, not parent)
databricks jobs get-run <RUN_ID> --output json \
  | jq ''.tasks[] | select(.state.result_state == "FAILED") | {task: .task_key, run_id: .run_id}''

# Get task output
databricks jobs get-run-output <TASK_RUN_ID> --output json \
  | jq -r ''.notebook_output.result // .error // "No output"''

# Fix -> Redeploy -> Re-poll (max 3 iterations)
```

### Step 8: Verify All Tasks and Application Health
```bash
# Verify all tasks succeeded
databricks jobs get-run <RUN_ID> --output json \
  | jq ''.tasks[] | {task: .task_key, result: .state.result_state}''

# Check app health
curl -s https://<app-url>/api/health
databricks apps get <app-name> --output json | jq .app_status
```

### Step 9: Run Testing Checklist
- Application health (URL accessible, health endpoint OK)
- Frontend functionality (UI loads, navigation, forms, data display)
- Backend functionality (API endpoints, database, auth)
- Data pipelines (Bronze, Silver, Gold, dashboards, Genie)

### Step 10: Document the Entire Repository
After deployment succeeds, paste this prompt in a **new AI assistant thread**:

```
Document this entire repository using @data_product_accelerator/skills/admin/documentation-organization/SKILL.md

Use Framework Documentation Authoring mode to create a complete docs/ set:
- Architecture overview with diagrams
- Component deep dives for each major module
- Deployment guide
- Operations guide (health checks, monitoring, alerting)
- Troubleshooting guide (common errors and solutions)

Also run organizational enforcement:
- Audit root directory for stray .md files
- Move any misplaced docs to correct docs/ subdirectory
- Validate all naming uses kebab-case
```

This triggers the documentation-organization skill''''s **Mode 2: Framework Documentation Authoring** which:
1. Gathers requirements (framework name, audience, tech stack, components)
2. Generates numbered docs under `docs/{project-name}-design/`
3. Fills templates (index, introduction, architecture, components, implementation, operations, troubleshooting)
4. Validates against the 43-item quality checklist

---

## üîß What Happens Behind the Scenes

When you paste the deployment prompt, the AI reads `@data_product_accelerator/skills/common/databricks-autonomous-operations/SKILL.md` ‚Äî the **autonomous operations skill**. Behind the scenes:

### Autonomous Operations Skill

1. **Bundle Discovery** ‚Äî reads `databricks.yml` to identify all resources (jobs, pipelines, dashboards, alerts)
2. **Pre-Deploy Validation** ‚Äî runs `databricks bundle validate` which catches ~80% of errors before deployment
3. **Deploy and Extract** ‚Äî deploys bundle and extracts RUN_ID from the output URL
4. **Exponential Backoff Polling** ‚Äî polls job status at 30s -> 60s -> 120s intervals until terminal state
5. **On Failure: Task-Level Diagnosis** ‚Äî extracts task-level run_ids (NOT parent job run_id), gets detailed output via `get-run-output`, matches errors against the error-solution matrix
6. **Self-Healing Loop** ‚Äî applies fix, redeploys, re-polls (max 3 iterations before escalation)
7. **On Success: Full Verification** ‚Äî verifies all tasks succeeded, retrieves structured JSON output from notebooks
8. **Common skills auto-loaded**:
   - `databricks-asset-bundles` ‚Äî DAB validation and deployment patterns
   - `databricks-expert-agent` ‚Äî core Databricks best practices
   - `naming-tagging-standards` ‚Äî enterprise naming conventions

### Documentation Organization Skill (Explicit Post-Deployment Trigger)

After deployment succeeds, the user runs a **separate prompt** that triggers the documentation-organization skill''''s **Framework Documentation Authoring mode (Mode 2)** to document the entire repository:

1. **Requirements Gathering** ‚Äî skill determines framework name, audience, tech stack, component count, and documentation depth
2. **File Structure Generation** ‚Äî creates numbered docs under `docs/{project-name}-design/`:
   - `00-index.md` ‚Äî document index with architecture summary and quick start
   - `01-introduction.md` ‚Äî purpose, scope, prerequisites, success criteria
   - `02-architecture-overview.md` ‚Äî Mermaid/ASCII diagrams, data flows, component inventory
   - `03-{component-1}.md` through `NN-{component-N}.md` ‚Äî component deep dives
   - `{N+1}-implementation-guide.md` ‚Äî phased steps with validation
   - `{N+2}-operations-guide.md` ‚Äî health checks, alerting, escalation matrix
   - `appendices/A-code-examples.md`, `B-troubleshooting.md`, `C-references.md`
3. **Quality Validation** ‚Äî runs 43-item checklist (organization, naming, structure, content, usability, maintenance, special cases)
4. **Organizational Enforcement** ‚Äî audits root for stray `.md` files, enforces `kebab-case` naming, routes misplaced docs to correct `docs/` subdirectory

### üèÖ Databricks Best Practices Applied

| Practice | How It''''s Used Here |
|----------|-------------------|
| **Self-Healing Deploy Loop** | Max 3 iterations of deploy-diagnose-fix before escalation to user |
| **Exponential Backoff Polling** | 30s -> 60s -> 120s intervals prevent API rate limiting and reduce noise |
| **Task-Level Diagnostics** | Uses **task** `run_id` (not parent job `run_id`) for `get-run-output` ‚Äî critical for multi-task jobs |
| **Structured Notebook Exit** | JSON output from `dbutils.notebook.exit()` enables machine-parseable result retrieval |
| **Pre-Deploy Validation** | `databricks bundle validate` catches ~80% of errors before any deployment attempt |
| **Dependency-Aware Ordering** | Follows Bronze -> Gold -> Semantic -> Monitoring -> Genie deployment order |
| **Partial Success Handling** | >=90% task success = OK; debug individual failures without rerunning everything |
| **CLI jq Patterns** | Structured JSON parsing for job state, failed tasks, and task output |
| **App Health Verification** | `/api/health` endpoint check + app logs review after deployment |
| **Never Retry Destructive Ops** | No auto-retry of `bundle destroy`, `DROP TABLE`, `DELETE` monitors/alerts |
| **Full Repository Documentation** | Post-deployment prompt triggers Framework Documentation Authoring for entire repo |
| **Numbered Documentation Set** | `docs/{project-name}-design/` with `00-index.md` through `NN-operations-guide.md` |
| **Root Directory Hygiene** | Only README/QUICKSTART/CHANGELOG in root; all other docs in `docs/` hierarchy |
| **Quality Checklist Validation** | 43-item checklist covering organization, naming, structure, content, usability |

---

## ‚ö†Ô∏è Error Troubleshooting Quick Reference

If deployment or jobs fail, check this table first:

| Error | Quick Fix |
|-------|-----------|
| `ModuleNotFoundError` | Add to `%pip install` or DAB environment spec |
| `TABLE_OR_VIEW_NOT_FOUND` | Run setup job first; check 3-part catalog.schema.table path |
| `DELTA_MULTIPLE_SOURCE_ROW_MATCHING` | Deduplicate source before MERGE |
| `Invalid access token (403)` | `databricks auth login --host <url> --profile <name>` |
| `ResourceAlreadyExists` | Delete + recreate (monitors, alerts) |
| `python_task not recognized` | Use `notebook_task` with `notebook_path` |
| `PARSE_SYNTAX_ERROR` | Read failing SQL file, fix syntax, redeploy |
| `Parameter not found` | Use `base_parameters` dict, not CLI-style `parameters` |
| `run_job_task` vs `job_task` | Use `run_job_task` (not `job_task`) |
| Genie `INTERNAL_ERROR` | Deploy semantic layer (TVFs + Metric Views) first |

---

## üìÇ Post-Deployment: Document the Entire Repository

After deployment succeeds, run the documentation-organization skill to create comprehensive project documentation.

### The Documentation Prompt

Paste this in a **new AI assistant thread** after deployment:

```
Document this entire repository using @data_product_accelerator/skills/admin/documentation-organization/SKILL.md

Use Framework Documentation Authoring mode to create a complete docs/ set:
- Architecture overview with diagrams
- Component deep dives for each major module
- Deployment guide
- Operations guide (health checks, monitoring, alerting)
- Troubleshooting guide (common errors and solutions)

Also run organizational enforcement:
- Audit root directory for stray .md files
- Move any misplaced docs to correct docs/ subdirectory
- Validate all naming uses kebab-case
```

### Expected Documentation Structure

```
docs/{project-name}-design/
‚îú‚îÄ‚îÄ 00-index.md                        # Document index, architecture summary
‚îú‚îÄ‚îÄ 01-introduction.md                 # Purpose, scope, prerequisites
‚îú‚îÄ‚îÄ 02-architecture-overview.md        # Diagrams, data flows, components
‚îú‚îÄ‚îÄ 03-{component-1}.md               # Component deep dive
‚îú‚îÄ‚îÄ 04-{component-2}.md               # Component deep dive
‚îú‚îÄ‚îÄ ...                                # Additional components
‚îú‚îÄ‚îÄ {N}-implementation-guide.md        # Phased steps with validation
‚îú‚îÄ‚îÄ {N+1}-operations-guide.md          # Health checks, alerting, escalation
‚îî‚îÄ‚îÄ appendices/
    ‚îú‚îÄ‚îÄ A-code-examples.md             # Code snippets and patterns
    ‚îú‚îÄ‚îÄ B-troubleshooting.md           # Error-solution matrix
    ‚îî‚îÄ‚îÄ C-references.md                # External references and links
```

### Documentation Naming Rules

| Format | Use For | Example |
|--------|---------|---------|
| `kebab-case.md` | All docs | `deployment-guide.md` |
| `NN-descriptive-name.md` | Framework docs (numbered) | `03-data-pipelines.md` |
| `YYYY-MM-DD-description.md` | Historical/dated records | `2026-02-07-initial-deployment.md` |
| NEVER `PascalCase.md` | -- | `DeploymentGuide.md` |
| NEVER `ALL_CAPS.md` | -- | `DEPLOYMENT_GUIDE.md` |

### 4-Step Documentation Workflow

| Step | What Happens | Output |
|------|-------------|--------|
| 1. Requirements Gathering | Skill asks about framework, audience, components, depth | Requirements table |
| 2. File Structure | Creates numbered file tree under `docs/` | Directory structure |
| 3. Fill Templates | Generates each doc from fill-in-the-blank templates | Complete documentation |
| 4. Quality Validation | Runs 43-item checklist across 7 categories | Validation report |',
'## Expected Deliverables

### üîÑ Deployment Process (Self-Healing Loop)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      SELF-HEALING DEPLOYMENT LOOP                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  Iteration 1: Deploy ‚Üí Run ‚Üí Poll ‚Üí [FAIL] ‚Üí Diagnose ‚Üí Fix ‚Üí Redeploy   ‚îÇ
‚îÇ  Iteration 2: Run ‚Üí Poll ‚Üí [FAIL] ‚Üí Diagnose ‚Üí Fix ‚Üí Redeploy             ‚îÇ
‚îÇ  Iteration 3: Run ‚Üí Poll ‚Üí [FAIL] ‚Üí ESCALATE TO USER                      ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  OR: Deploy ‚Üí Run ‚Üí Poll ‚Üí [SUCCESS] ‚Üí Verify ‚Üí Document Repo             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Self-Healing Loop Tracking

| Iteration | Error | Fix Applied | Outcome |
|-----------|-------|-------------|---------|
| 1 | (recorded from diagnosis) | (what was changed) | FAIL / SUCCESS |
| 2 | (recorded from diagnosis) | (what was changed) | FAIL / SUCCESS |
| 3 | (recorded from diagnosis) | (what was changed) | FAIL / ESCALATE |

---

### üìä Deployment Verification Commands

```bash
# 1. Check overall job status
databricks jobs get-run <RUN_ID> --output json | jq ''.state''

# 2. Get summary of all tasks
databricks jobs get-run <RUN_ID> --output json \
  | jq ''.tasks[] | {task: .task_key, run_id: .run_id, result: .state.result_state}''

# 3. Get failed tasks only
databricks jobs get-run <RUN_ID> --output json \
  | jq ''.tasks[] | select(.state.result_state == "FAILED") | {task: .task_key, error: .state.state_message, url: .run_page_url}''

# 4. Get task output (MUST use TASK run_id, not parent job run_id)
databricks jobs get-run-output <TASK_RUN_ID> --output json \
  | jq -r ''.notebook_output.result // "No output"''

# 5. Check app status
databricks apps get <app-name>
databricks apps get <app-name> --output json | jq .app_status

# 6. Check bundle status
databricks bundle validate
databricks bundle summary
```

---

### ‚úÖ Application Health Checks

**Application:**
- [ ] App deployed and accessible at URL
- [ ] Health endpoint (`/api/health`) returns 200 OK
- [ ] UI loads without JavaScript errors
- [ ] Navigation works correctly
- [ ] Forms submit successfully
- [ ] Data displays in tables and charts
- [ ] Authentication/authorization working
- [ ] API endpoints respond correctly
- [ ] Database connections working
- [ ] No errors in application logs

**DAB Artifacts (if deployed):**
- [ ] All jobs visible in Workflows UI
- [ ] All job tasks completed with `SUCCESS` result state
- [ ] Pipelines running successfully
- [ ] Tables created in Unity Catalog
- [ ] Data flowing correctly through Bronze -> Silver -> Gold
- [ ] Data visible in dashboards/Genie

---

### üìÇ Repository Documentation Set Created

After running the documentation-organization prompt, the following structure is generated:

```
docs/{project-name}-design/
‚îú‚îÄ‚îÄ 00-index.md                        # Document index
‚îú‚îÄ‚îÄ 01-introduction.md                 # Purpose, scope, prerequisites
‚îú‚îÄ‚îÄ 02-architecture-overview.md        # Diagrams, data flows
‚îú‚îÄ‚îÄ 03-{component-1}.md               # Component deep dive
‚îú‚îÄ‚îÄ ...                                # Additional components
‚îú‚îÄ‚îÄ {N}-implementation-guide.md        # Build instructions
‚îú‚îÄ‚îÄ {N+1}-operations-guide.md          # Health checks, alerting
‚îî‚îÄ‚îÄ appendices/
    ‚îú‚îÄ‚îÄ A-code-examples.md             # Code patterns
    ‚îú‚îÄ‚îÄ B-troubleshooting.md           # Error-solution matrix
    ‚îî‚îÄ‚îÄ C-references.md                # External references
```

**Organizational Enforcement Results:**
- [ ] Root directory audited (only README, QUICKSTART, CHANGELOG remain)
- [ ] All doc filenames use `kebab-case`
- [ ] Numbered sequence for framework docs (00-, 01-, 02-, ...)
- [ ] No misplaced `.md` files in root or wrong subdirectories

---

### ‚úÖ Success Criteria Checklist

**Bundle Deployment:**
- [ ] `databricks bundle validate` passes with no errors
- [ ] `databricks bundle deploy` completes successfully
- [ ] All resources deployed to target workspace

**Job Execution and Monitoring:**
- [ ] Jobs triggered and RUN_ID captured
- [ ] Polling with exponential backoff (30s -> 60s -> 120s)
- [ ] All tasks reached terminal state (SUCCESS)
- [ ] Task output retrieved via `get-run-output` using task run_id

**Application Health:**
- [ ] App URL is accessible
- [ ] `/api/health` returns 200 OK
- [ ] UI loads without errors
- [ ] API endpoints respond correctly
- [ ] No errors in application logs

**Self-Healing Loop:**
- [ ] If failures occurred: diagnosed using task-level CLI commands
- [ ] If failures occurred: fix applied and redeployed (max 3 iterations)
- [ ] If escalated: all errors, fixes, and run URLs provided to user

**Data Pipeline Verification:**
- [ ] Bronze tables populated with data
- [ ] Silver pipeline processed without errors
- [ ] Gold tables reflect correct aggregations
- [ ] Dashboards and Genie Spaces functional

**Repository Documentation:**
- [ ] `docs/{project-name}-design/` directory exists with numbered docs
- [ ] Architecture overview includes diagrams (Mermaid or ASCII)
- [ ] Component deep dives cover each major module
- [ ] Operations guide includes health checks and alerting procedures
- [ ] Troubleshooting guide includes common errors and solutions
- [ ] No stray `.md` files in root (only README, QUICKSTART, CHANGELOG)
- [ ] All doc filenames use `kebab-case`
- [ ] 43-item quality checklist passed',
true, 1, true, current_timestamp(), current_timestamp(), current_user());

-- Setup Lakebase
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(16, 'setup_lakebase',
'## Create Lakebase Tables from UI Design

**Workspace:** `{workspace_url}`
**Lakebase Instance Name:** `{lakebase_instance_name}`
**Lakebase Host Name:** `{lakebase_host_name}`

**Working directory:** All app and Lakebase assets go under `apps_lakebase/`. Read the UI design from the parent `docs/` folder.

> **‚ö†Ô∏è IMPORTANT NOTE:** The Lakebase Instance Name and Host Name above are configured in the Workshop Parameters. Make sure these match your Databricks workspace Lakebase instance. You can verify and update these values in the Configuration ‚Üí Workshop Parameters tab.

Read `@docs/ui_design.md` (parent folder at repo root) and create the database tables needed to power the UI under `apps_lakebase/`.

---

**Step 1: Authenticate to Databricks**

```bash
databricks auth login --host {workspace_url}
```

**Step 2: Create the DDL file**

Create file `apps_lakebase/db/lakebase/ddl/05_app_tables.sql` with CREATE TABLE statements for ALL entities needed by the UI:
- Use PostgreSQL syntax
- Use `vibe_coding_workshop` as schema placeholder
- Include primary keys, foreign keys, indexes
- Include created_at/updated_at timestamps

**DDL Guidelines for Lakebase:**
- Use `TEXT` instead of `TEXT[]` (ARRAY types) - the SQL parser may not handle ARRAY syntax correctly
- Avoid complex PostgreSQL-specific types that may not be supported

**Step 3: Create the DML seed file**

Create file `apps_lakebase/db/lakebase/dml_seed/04_seed_app_data.sql` with INSERT statements:
- 10-15 realistic records per table for {industry_name} industry
- Use `vibe_coding_workshop` as schema placeholder  
- Insert parent tables before child tables (e.g., hosts before listings, listings before reviews)

**DML Guidelines for Lakebase:**
- Use double single quotes ('''') to escape apostrophes in SQL strings (e.g., ''''chef''''''''s kitchen'''')
- Avoid semicolons (;) inside string values - use pipe (|) or comma as delimiters instead (e.g., ''''Rule 1 | Rule 2'''' not ''''Rule 1; Rule 2'''')
- Ensure FK references match parent table row counts (if you have 10 hosts, listings.host_id must be 1-10, not 1-12)

**Step 4: Get your schema name and instance info**

```bash
# Get your schema name
# Get your username (firstname + first initial of lastname, no dots)
FIRSTNAME=$(databricks current-user me --output json | jq -r ''''.userName'''' | cut -d''''@'''' -f1 | cut -d''''.'''' -f1)
LASTINITIAL=$(databricks current-user me --output json | jq -r ''''.userName'''' | cut -d''''@'''' -f1 | cut -d''''.'''' -f2 | cut -c1)
USERNAME="${FIRSTNAME}_${LASTINITIAL}"
SCHEMA_NAME="${USERNAME}_vibe_coding"
echo "Your schema: $SCHEMA_NAME"

# Get the Lakebase instance DNS (run from apps_lakebase/ or use path)
cd apps_lakebase && python3 scripts/lakebase_manager.py --action instance-info --instance-name {lakebase_instance_name}
```

**Step 5: Deploy to Lakebase**

> **üí° Note:** The `account users` role is a default group role in Databricks Lakebase that allows OAuth-authenticated users to connect without requiring individual role creation. Use this as the LAKEBASE_USER_OVERRIDE value.

```bash
# Set environment overrides (replace <values> with your actual values from Step 4)
export LAKEBASE_HOST_OVERRIDE="<instance-dns-from-step-4>"
export LAKEBASE_DATABASE_OVERRIDE="databricks_postgres"
export LAKEBASE_SCHEMA_OVERRIDE="<your-schema-from-step-4>"
export LAKEBASE_PORT_OVERRIDE="5432"
export LAKEBASE_USER_OVERRIDE="account users"

# Deploy tables (run from apps_lakebase/)
cd apps_lakebase && ./scripts/setup-lakebase.sh --recreate --instance-name {lakebase_instance_name}
```

Type `YES-PRODUCTION` when prompted.

**Step 6: Verify deployment**

```bash
cd apps_lakebase && ./scripts/setup-lakebase.sh --status --instance-name {lakebase_instance_name}
```

All tables must show `‚úì exists` with row counts before proceeding.

**Step 7: Update app.yaml**

Update the env section in `apps_lakebase/app.yaml`:

```yaml
env:
  - name: LAKEBASE_HOST
    value: "<your-instance-dns>"
  - name: LAKEBASE_DATABASE
    value: "databricks_postgres"
  - name: LAKEBASE_SCHEMA
    value: "<your-schema>"
  - name: LAKEBASE_PORT
    value: "5432"
  - name: LAKEBASE_USER
    value: "account users"
```

---

**If deployment fails:** Fix the error in your DDL/DML files and re-run Step 5. Retry up to 3 times.
**If auth fails:** Re-run `databricks auth login --host {workspace_url}`',
'You are a database engineer setting up Lakebase (PostgreSQL) tables for a web application.

Key requirements:
1. Read the UI design to understand what data entities are needed
2. Create DDL with proper PostgreSQL syntax, keys, and indexes
3. Create realistic seed data that matches the industry context
4. Deploy tables and verify they exist with data
5. Configure apps_lakebase/app.yaml so the application can connect

This prompt is returned as-is for direct use in Cursor/Copilot. No LLM processing.

CLI Best Practices:
- Run from apps_lakebase/ or use apps_lakebase/scripts/ for scripts
- Run CLI commands outside the IDE sandbox to avoid SSL/TLS certificate errors',
'Setup Lakebase',
'Create and deploy Lakebase tables from UI Design',
5,
'## How to Use

1. **Copy the generated prompt**
2. **Paste into Cursor or Copilot**
3. The code assistant will execute all steps:
   - Authenticate to Databricks workspace
   - Create DDL and DML files
   - Run deployment commands
   - Verify tables exist
   - Update app.yaml',
'## Expected Output

- DDL file: `apps_lakebase/db/lakebase/ddl/05_app_tables.sql`
- DML file: `apps_lakebase/db/lakebase/dml_seed/04_seed_app_data.sql`
- Tables deployed to {lakebase_instance_name}
- apps_lakebase/app.yaml updated with Lakebase config',
TRUE,
1, TRUE, current_timestamp(), current_timestamp(), current_user());

-- Default Section
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, version, is_active, inserted_at, updated_at, created_by)
VALUES
(99, 'default',
'Generate content for {section_tag} in {industry_name} for {use_case_title}.

Industry: {industry_name}
Use Case: {use_case_title}
Section: {section_tag}

Please provide detailed requirements and specifications for this section.',
'You are an expert Databricks solutions architect.
Generate a detailed, actionable prompt for {section_tag} in a {industry_name} {use_case_title} application.',
'Default Section',
'Default template for unknown sections',
99,
'',
'',
1, TRUE, current_timestamp(), current_timestamp(), current_user());

-- Wire UI to Lakebase
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(108, 'wire_ui_lakebase',
'## Task: Wire Frontend UI to Lakebase Backend

Connect the web application to the Lakebase database so the UI displays real data. This step focuses on **local development and testing**.

**Working directory:** All app code and commands use the `apps_lakebase/` folder.

**Lakebase Instance Name:** `{lakebase_instance_name}`
**Lakebase Host Name:** `{lakebase_host_name}`

> **‚ö†Ô∏è IMPORTANT NOTE:** The Lakebase Instance Name and Host Name above are configured in the Workshop Parameters. Ensure these match your Databricks workspace Lakebase instance before proceeding.

---

## Part A: Install Dependencies (CRITICAL - Prevent "No module named psycopg2" Error)

**Both files must be updated in `apps_lakebase/`** - `apps_lakebase/pyproject.toml` is the source of truth and `apps_lakebase/requirements.txt` may be regenerated from it.

1. **Check if `apps_lakebase/pyproject.toml` exists** - if yes, add `psycopg2-binary` to `[project.dependencies]`:
   ```toml
   [project]
   dependencies = [
       "psycopg2-binary",
       # ... other deps
   ]
   ```

2. **Add to `apps_lakebase/requirements.txt`** - ensure this line exists:
   ```
   psycopg2-binary
   ```

3. **Verify both files have the dependency (from apps_lakebase/):**
   ```bash
   cd apps_lakebase && grep -i psycopg pyproject.toml requirements.txt
   ```
   You should see `psycopg2-binary` in BOTH files.

4. **Test locally** before proceeding (from apps_lakebase/):
   ```bash
   cd apps_lakebase && pip install -r requirements.txt
   cd apps_lakebase && python3 -c "import psycopg2; print(''psycopg2 OK'')"
   ```

5. **Ensure `requirements.txt` is NOT in `apps_lakebase/.gitignore`:**
   ```bash
   grep -q "requirements.txt" apps_lakebase/.gitignore && echo "WARNING: Remove requirements.txt from .gitignore!" || echo "OK"
   ```
   If ignored, Databricks sync will skip it and deployment will fail.

**Why this matters:** Databricks Apps install from `requirements.txt`. If the file is missing or ignored, the deployed app will fail with "No module named ''psycopg2''".

---

## Part B: Configure App Permissions (CRITICAL)

Your app runs as a **service principal**. It cannot connect to Lakebase until you grant permissions.

**Step 1: Get service principal ID**
```bash
cd apps_lakebase && python scripts/lakebase_manager.py --action app-info --app-name $APP_NAME
```
Copy the Service Principal ID from the output.

**Step 2: Grant Lakebase role**
```bash
cd apps_lakebase && python scripts/lakebase_manager.py --action add-lakebase-role --app-name $APP_NAME --instance-name {lakebase_instance_name}
```
Look for: `‚úì Successfully added Lakebase role`

**Step 3: Link Lakebase as App Resource**
```bash
cd apps_lakebase && python scripts/lakebase_manager.py --action link-app-resource --app-name $APP_NAME --instance-name {lakebase_instance_name}
```
Look for: `‚úì Successfully linked Lakebase instance`

**Step 4: Verify permissions were added**
```bash
cd apps_lakebase && python scripts/lakebase_manager.py --action list-lakebase-roles --instance-name {lakebase_instance_name}
```
Your service principal ID must appear with `DATABRICKS_SUPERUSER` role.

**Step 5: Update apps_lakebase/app.yaml**
Set `LAKEBASE_USER` to the service principal ID (not your email).

**‚ö†Ô∏è If you skip these steps, you will see this error:**
```
role "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" does not exist
```
Go back and run steps 2-4 again.

---

## Lakebase Authentication Pattern (CRITICAL)

Databricks Apps inject these env vars for linked database resources:
- `PGHOST`, `PGDATABASE`, `PGUSER`, `PGPORT`, `PGSSLMODE`

**‚ö†Ô∏è PGPASSWORD is NOT injected!** Use OAuth authentication instead:

1. Use `databricks-sdk` to get OAuth token:
   ```python
   from databricks.sdk import WorkspaceClient
   ws = WorkspaceClient()
   headers = ws.config.authenticate()
   token = headers["Authorization"][7:]  # Remove "Bearer "
   ```

2. Use the token as the password for psycopg2:
   ```python
   psycopg2.connect(
       host=os.getenv("PGHOST"),
       user=os.getenv("PGUSER"),
       password=token,  # OAuth token, NOT env var
       database=os.getenv("PGDATABASE"),
       port=os.getenv("PGPORT"),
       sslmode=os.getenv("PGSSLMODE")
   )
   ```

**When updating `@apps_lakebase/src/backend/services/lakebase.py`, ensure it follows this pattern.**

---

## Part C: Wire UI to Backend

### Backend Changes

1. **Review DDL and DML files** to understand the database structure (under apps_lakebase/)
   - Open `@apps_lakebase/db/lakebase/ddl/` and review all table DDL files
   - Open `@apps_lakebase/db/lakebase/dml_seed/` and review the seed data files
   - Understand the table names, column names, data types, and relationships
   - Note which tables power which UI pages/components

2. **Update query functions** in `@apps_lakebase/src/backend/services/lakebase.py`
   - Based on your understanding of the backend Lakebase tables, update the query functions
   - Ensure queries match the actual table and column names from the DDL files
   - Use existing connection code - do NOT create new database connections
   - Ensure OAuth token authentication is used (see pattern above)
   - Functions should return `None` if Lakebase connection fails (for fallback)
   - **Check your work:** Run each query manually to verify it returns expected data without errors

3. **Add INFO logging** to all Lakebase connection code
   - Log when connecting to Lakebase (host, database, schema)
   - Log the query being executed and which page/endpoint triggered it
   - Log success with row count or failure with error details
   - Example:
     ```python
     import logging
     logger = logging.getLogger(__name__)
     
     logger.info(f"[Lakebase] Connecting to {host}/{database}/{schema}")
     logger.info(f"[Lakebase] Executing query for endpoint: /api/your-endpoint")
     logger.info(f"[Lakebase] Query successful - returned {len(results)} rows")
     ```

4. **Add/update API endpoints** in `@apps_lakebase/src/backend/api/routes.py`
   - Health endpoint: `/api/health/lakebase` - returns connection status and any errors
   - Data endpoints should return both `data` AND `source` ("live" or "mock")
   - When Lakebase fails, fall back to mock data

### Frontend Changes

5. **Create a ConnectionStatus indicator component**
   - Shows "üî¥ Live Data" when connected to Lakebase  
   - Shows "üìã Mock Data" when using fallback data
   - Displays error indicator (‚ö†Ô∏è) with tooltip when connection fails
   - **Place at the TOP center of the page** (header area) so users clearly see it immediately
   - **Must appear on ALL pages** that fetch data from Lakebase
   - Show the specific action/data being loaded for that page (e.g., "Loading listings...", "Fetching bookings...")

6. **Update data-fetching components**
   - Handle both live and mock data from backend
   - Track the data source so UI can display it if needed

**Important:** Users must clearly see whether they''re viewing live or mock data on every page.

---

## Part D: Local Build and Test

Run all commands from the `apps_lakebase/` folder.

1. **Build the frontend:**
   ```bash
   cd apps_lakebase && npm install && npm run build
   ls apps_lakebase/dist/index.html || echo "ERROR: Build failed!"
   ```

2. **Test the backend locally:**
   ```bash
   cd apps_lakebase && python3 app.py
   ```

3. **Open `http://localhost:8000` in your browser and verify:**
   - The UI loads correctly
   - Navigation works between pages
   - ConnectionStatus indicator shows data source
   - Backend API endpoints respond (check browser dev tools Network tab)
   - No console errors

4. **Test API endpoints locally:**
   ```bash
   curl -s "http://localhost:8000/api/health/lakebase" | jq .
   ```

**Only proceed to Step 8 (Deploy and Test) after local testing passes.**

---

## Defensive Data Handling

When wiring UI to backend, prevent runtime errors:
- Initialize arrays with `[]`, not `undefined`
- Use optional chaining: `data?.slice()`, `data?.map()`
- Provide fallbacks: `(data ?? []).map(...)` or `data || []`
- Check before rendering: `{data && data.map(...)}`

---

## Route Prefix Reminder

Health endpoints are mounted at root (`/health/*`), not under `/api`. Frontend calls to health checks should use `/health/lakebase`, not `/api/health/lakebase`.

---

## Checklist

- [ ] psycopg2-binary in BOTH pyproject.toml AND requirements.txt
- [ ] requirements.txt NOT in .gitignore (will be skipped by sync!)
- [ ] Tested locally: `python3 -c "import psycopg2"`
- [ ] Service principal ID obtained
- [ ] Lakebase database role granted (add-lakebase-role)
- [ ] Lakebase linked as App Resource (link-app-resource)
- [ ] apps_lakebase/app.yaml LAKEBASE_USER set to service principal ID
- [ ] INFO logging added to all Lakebase connection code
- [ ] Backend APIs return data with source indicator (live/mock)
- [ ] Backend falls back to mock data when Lakebase unavailable
- [ ] ConnectionStatus component shows live vs mock indicator on ALL pages
- [ ] Frontend built successfully: `npm run build` (from apps_lakebase/)
- [ ] Local testing passed at localhost:8000',
'You are a full-stack developer connecting a web app to Lakebase.

Key requirements:
1. Configure service principal permissions (apps don''t run as your user)
2. Grant both Lakebase database role AND link as App Resource (both are required)
3. Backend APIs must indicate data source (live/mock) and fall back gracefully
4. UI must show a clear indicator: "Live Data" vs "Mock Data"
5. Display connection errors so users understand any issues
6. Test locally before deployment (deployment is done in the next step)

This prompt is returned as-is for direct use in Cursor/Copilot. No LLM processing.

CLI Best Practices:
- Run from apps_lakebase/ or use apps_lakebase/scripts/ for deploy and lakebase scripts
- Run CLI commands outside the IDE sandbox to avoid SSL/TLS certificate errors',
'Wire UI to Lakebase',
'Connect frontend UI to Lakebase backend, test locally',
6,
'## Prerequisite

Complete Step 6 (Setup Lakebase) first. Tables must exist.

---

## Steps to Apply

1. Copy the generated prompt
2. Paste into Cursor or Copilot
3. Follow Parts A ‚Üí B ‚Üí C ‚Üí D in order
4. Test locally before proceeding to deployment

**CRITICAL:** The deployed app uses a service principal. Get its ID and grant Lakebase permissions.

**Note:** This step focuses on local development. Deployment to Databricks is done in Step 8.',
'## Expected Deliverables

- Service principal with Lakebase database role granted
- Lakebase linked as App Resource (enables PGPASSWORD injection)
- Backend APIs with fallback to mock data
- ConnectionStatus indicator showing live/mock state
- Frontend built successfully (`npm run build`)
- Local testing passed at localhost:8000

**Next Step:** Deploy to Databricks in Step 8',
TRUE,
1, TRUE, current_timestamp(), current_timestamp(), current_user());

-- Deploy to Databricks App
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(110, 'deploy_databricks_app',
'## Your Task

Deploy the locally-tested web application to Databricks Apps.

**Workspace:** `{workspace_url}`

**Working directory:** All app paths and commands use the `apps_lakebase/` folder.

---

## Deployment Constraints
- Databricks App names must use only lowercase letters, numbers, and dashes (no underscores).
  Use hyphens: `my-app-name` not `my_app_name`.
- The Databricks Apps runtime auto-runs `npm install` and `npm run build` when it
  finds a `package.json`. Ensure `databricks.yml` sync config includes both `dist/**`
  AND `src/**` so the platform build succeeds.

---

### Step 1: Verify Frontend Build (CRITICAL)

Before deploying, verify the `apps_lakebase/dist/` folder exists with the built frontend:

```bash
ls -la apps_lakebase/dist/index.html
```

**If missing or error, build the frontend first (from apps_lakebase/):**
```bash
cd apps_lakebase && npm run build
ls -la apps_lakebase/dist/
# Should show: index.html, assets/, vite.svg
```

**Why this matters:** The app serves the UI from the `dist/` folder inside apps_lakebase. If `dist/` is missing, you will see "No frontend deployed" instead of the web UI.

---

### Step 2: Deploy Application (USE SCRIPT ONLY)

**CRITICAL:** Always use the deployment script from `apps_lakebase/`. Do NOT use ad-hoc `databricks apps` commands.

```bash
cd apps_lakebase && ./scripts/deploy.sh --code-only -t production -p e2-demo-field-eng
```

This script automatically:
1. Builds the frontend (`npm run build` in apps_lakebase)
2. Syncs ALL files including `dist/` to workspace
3. Triggers rolling deployment

**WARNING:** Direct `databricks apps deploy` or `databricks workspace import` commands will SKIP the `dist/` folder because it''s in `.gitignore`. Only the deploy script in apps_lakebase syncs it correctly via `databricks bundle sync` (see apps_lakebase/databricks.yml).

---

### Step 3: Verify UI Loads in Browser

Get the deployed app URL and open it in a browser:

```bash
databricks apps get $APP_NAME --output json | jq -r ''.url''
```

**Expected:** You should see the **web UI with sidebar and content** (the React application).

**If you see JSON with "No frontend deployed"** - see Troubleshooting below.

---

### Step 4: Check Logs and Fix Errors (up to 3 iterations)

1. **Get the app logs** and scan for errors:

```bash
databricks apps logs $APP_NAME --tail 100
```

2. **If errors exist:**
   - Research the issue to understand the root cause
   - Apply the fix to the code in apps_lakebase/
   - Rebuild: `cd apps_lakebase && npm run build`
   - Redeploy: `cd apps_lakebase && ./scripts/deploy.sh --code-only -t production -p e2-demo-field-eng`
   - Check logs again

3. **If no errors:** Deployment successful!

4. **Repeat up to 3 times.** If errors persist after 3 attempts, report them for manual investigation.

**Common error:** "Could not import module" ‚Üí Check `apps_lakebase/app.yaml` command matches your file structure (e.g., `app:app` vs `server.app:app`)

---

### Troubleshooting: "No frontend deployed"

If you see this JSON response instead of the web UI:
```json
{"note": "No frontend deployed. Visit /docs for API documentation."}
```

**Cause:** The `apps_lakebase/dist/` folder (built frontend) was not synced to the workspace.

**Fix:**
1. Verify dist exists locally: `ls apps_lakebase/dist/index.html`
2. If missing, build it: `cd apps_lakebase && npm run build`
3. Redeploy using the script (NOT ad-hoc commands):
   ```bash
   cd apps_lakebase && ./scripts/deploy.sh --code-only -t production -p e2-demo-field-eng
   ```

**Root cause:** The `dist/` folder is in `.gitignore`. Only `databricks bundle sync` (used by deploy.sh in apps_lakebase) includes it via explicit config in apps_lakebase/databricks.yml.

---

### If the Workspace App Limit Is Reached

If deployment fails because the workspace has hit its app limit, do NOT rename your app. Instead, free up a slot by removing the oldest stopped app:

1. Find stopped apps sorted by oldest first:
   ```bash
   databricks apps list -o json | jq -r ''[.[] | select(.compute_status.state == "STOPPED")] | sort_by(.update_time) | .[0] | .name''
   ```
2. Delete it and wait for cleanup to complete:
   ```bash
   databricks apps delete <name-from-above>
   sleep 10
   ```
3. Retry the deployment.

If the limit error persists, repeat with the next oldest stopped app -- but **stop after 3 total attempts** (increase the wait to 20s, then 40s between retries). If it still fails after 3 tries, stop and report the issue for manual workspace cleanup. Never delete apps in RUNNING state.

---

### Summary

Your job is complete when:
- The Databricks App is deployed and running
- **The web UI loads in browser** (React app with sidebar, NOT JSON)
- No "No frontend deployed" message
- No errors in the app logs',
'You are deploying a locally-tested web application to Databricks Apps. Focus on deployment, verification, and troubleshooting.

Your approach:
1. Use existing deployment scripts when available
2. Deploy to Databricks Apps
3. Verify the deployment by checking the app URL
4. Debug and fix any deployment errors

CLI Best Practices:
- Use the deployment script from apps_lakebase/scripts/ (run from apps_lakebase/)
- Run CLI commands outside the IDE sandbox to avoid SSL/TLS certificate errors

This prompt is returned as-is for direct use in Cursor/Copilot. No LLM processing.',
'Deploy to Databricks App',
'Deploy your locally-tested app to Databricks workspace',
4,
'## How to Use

1. **Copy the generated prompt**
2. **Paste into Cursor or Copilot**
3. The code assistant will:
   - Use the deployment script from apps_lakebase/scripts/ folder
   - Deploy your app to Databricks
   - Get the app URL and verify it works
   - Fix any deployment errors

**Note:** Make sure local testing passed before running this step.',
'## Expected Output

- Databricks App deployed and running
- **Web UI loads in browser** (React app with sidebar and content, NOT JSON)
- No "No frontend deployed" error message
- No errors in the app logs',
TRUE,
1, TRUE, current_timestamp(), current_timestamp(), current_user());

-- Register Lakebase in Unity Catalog
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(112, 'sync_from_lakebase',
'Copy and paste this prompt to the AI:

```
## Task: Register Lakebase as a Read-Only Unity Catalog Database Catalog

Register the Lakebase PostgreSQL database as a Unity Catalog database catalog so that all tables are automatically accessible via SQL, notebooks, and ETL pipelines with zero ETL.

### Configuration
- **Catalog name:** {lakebase_uc_catalog_name}
- **Lakebase instance:** {lakebase_instance_name}
- **Database name:** databricks_postgres (standard Lakebase database)
- **SQL Warehouse:** {default_warehouse}

### Step 1: Check if Catalog Already Exists

Run the following CLI command to check whether the catalog has already been registered:

```bash
databricks catalogs get {lakebase_uc_catalog_name}
```

- If the command returns catalog info with **state: ACTIVE**, the catalog is already registered. Print a confirmation message: "Catalog ''{lakebase_uc_catalog_name}'' already exists and is ACTIVE. Skipping creation."
- If the command returns an error (e.g., "CATALOG_DOES_NOT_EXIST" or "not found"), proceed to Step 2.

### Step 2: Create the Database Catalog (only if it does not exist)

Register the Lakebase PostgreSQL database as a read-only Unity Catalog catalog:

```bash
databricks database create-database-catalog {lakebase_uc_catalog_name} {lakebase_instance_name} databricks_postgres
```

After creation, verify the catalog state:

```bash
databricks catalogs get {lakebase_uc_catalog_name}
```

Confirm the output shows **state: ACTIVE**. If the state is not ACTIVE, wait a few seconds and check again.

### Step 3: List All Schemas in the Catalog

Whether the catalog was just created or already existed, always run this final verification step to display all available schemas:

```sql
SELECT schema_name 
FROM {lakebase_uc_catalog_name}.information_schema.schemata 
ORDER BY schema_name;
```

Run this SQL query using the SQL Warehouse **{default_warehouse}**. Display the results to confirm which schemas are available in the registered catalog.

### Expected Result:
- Catalog `{lakebase_uc_catalog_name}` is registered and ACTIVE in Unity Catalog
- All schemas from the Lakebase PostgreSQL database are listed and visible
- Tables within those schemas are now queryable via standard SQL (e.g., `SELECT * FROM {lakebase_uc_catalog_name}.<schema>.<table>`)
```',
'',
'Register Lakebase in Unity Catalog',
'Register Lakebase as a read-only Unity Catalog database catalog',
9,
'## What is a Unity Catalog Database Catalog?

A **Database Catalog** in Unity Catalog allows you to register an external database (such as Lakebase PostgreSQL) as a read-only catalog. Once registered, all tables from the source database appear automatically in Unity Catalog and can be queried via SQL, notebooks, and ETL pipelines -- with zero ETL needed.

This replaces the manual process of syncing individual tables and converting types.

---

## Steps to Apply

1. Copy the generated prompt using the copy button
2. Paste it into Cursor or VS Code with Copilot
3. The AI will check if the catalog already exists
4. If not, it will create the database catalog using the Databricks CLI
5. It will verify the catalog is ACTIVE
6. Finally, it will list all schemas in the catalog as confirmation

**Note:** This is a one-time registration. Once the catalog is created, all current and future tables in the Lakebase database are automatically accessible in Unity Catalog.',
'## Expected Deliverables

- Catalog `{lakebase_uc_catalog_name}` is registered in Unity Catalog with state ACTIVE
- All schemas from the Lakebase PostgreSQL database are listed and displayed to the user
- Tables are queryable via standard SQL (e.g., `SELECT * FROM {lakebase_uc_catalog_name}.<schema>.<table>`)',
TRUE,
1, TRUE, current_timestamp(), current_timestamp(), current_user());

-- =============================================
-- GENIE ACCELERATOR PROMPTS
-- =============================================

-- Step 22: Analyze Silver Metadata (Genie Accelerator) - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(114, 'genie_silver_metadata',
'Extract and analyze comprehensive table and column metadata from your Silver layer schema.

This will:

- **Query table metadata** ‚Äî extract table names, types, and table-level comments from `{chapter_3_lakehouse_catalog}.information_schema.tables`
- **Query column metadata** ‚Äî extract column names, data types, ordinal positions, nullability, defaults, and column-level comments from `{chapter_3_lakehouse_catalog}.information_schema.columns`
- **Query constraints** ‚Äî extract primary key and foreign key constraint definitions from `{chapter_3_lakehouse_catalog}.information_schema.table_constraints` and `constraint_column_usage`
- **Query column tags** ‚Äî extract Unity Catalog tags from `{chapter_3_lakehouse_catalog}.information_schema.column_tags` (if available)
- **Query table tags** ‚Äî extract Unity Catalog tags from `{chapter_3_lakehouse_catalog}.information_schema.table_tags` (if available)
- **Merge and save** ‚Äî combine all results into an enriched metadata CSV
- **Analyze and document** ‚Äî produce a Genie analysis plan based on the metadata

**Source:** `{chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema}` (configured in the Silver Layer panel above)

Copy and paste this prompt to the AI:

```
Run the following SQL queries against {chapter_3_lakehouse_catalog}.{chapter_3_lakehouse_schema} and merge the results into a comprehensive metadata file.

---

**Query 1 ‚Äî Table inventory:**
SELECT table_catalog, table_schema, table_name, table_type, comment
FROM {chapter_3_lakehouse_catalog}.information_schema.tables
WHERE table_schema = ''{chapter_3_lakehouse_schema}''
ORDER BY table_name

**Query 2 ‚Äî Column metadata:**
SELECT table_name, column_name, ordinal_position, data_type, is_nullable, column_default, comment
FROM {chapter_3_lakehouse_catalog}.information_schema.columns
WHERE table_schema = ''{chapter_3_lakehouse_schema}''
ORDER BY table_name, ordinal_position

**Query 3 ‚Äî Table constraints (PKs, FKs):**
SELECT constraint_name, table_name, constraint_type
FROM {chapter_3_lakehouse_catalog}.information_schema.table_constraints
WHERE constraint_schema = ''{chapter_3_lakehouse_schema}''
ORDER BY table_name, constraint_type

**Query 4 ‚Äî Constraint column usage:**
SELECT constraint_name, table_name, column_name
FROM {chapter_3_lakehouse_catalog}.information_schema.constraint_column_usage
WHERE constraint_schema = ''{chapter_3_lakehouse_schema}''
ORDER BY constraint_name, table_name

**Query 5 ‚Äî Column tags (may not exist ‚Äî skip gracefully if error):**
SELECT table_name, column_name, tag_name, tag_value
FROM {chapter_3_lakehouse_catalog}.information_schema.column_tags
WHERE schema_name = ''{chapter_3_lakehouse_schema}''
ORDER BY table_name, column_name

**Query 6 ‚Äî Table tags (may not exist ‚Äî skip gracefully if error):**
SELECT table_name, tag_name, tag_value
FROM {chapter_3_lakehouse_catalog}.information_schema.table_tags
WHERE schema_name = ''{chapter_3_lakehouse_schema}''
ORDER BY table_name

---

**Technical reference (for AI execution):**

1. Get warehouse ID:
   databricks warehouses list --output json | jq ''.[0].id''

2. Execute each SQL query via Statement Execution API:
   databricks api post /api/2.0/sql/statements --json ''{
     "warehouse_id": "<WAREHOUSE_ID>",
     "statement": "<SQL_QUERY>",
     "wait_timeout": "50s",
     "format": "JSON_ARRAY"
   }'' > /tmp/query_N_result.json

3. For queries 5 and 6 (tags), if the table does not exist, skip gracefully and continue.

4. Merge all results into a single enriched CSV with Python:
   - Read each query result JSON
   - Join table metadata (Query 1) with column metadata (Query 2) on table_name
   - Append constraint info (Queries 3-4) as additional columns: constraint_type, constraint_name
   - Append tag info (Queries 5-6) as additional columns: column_tags, table_tags
   - Output columns: table_name, table_type, table_comment, column_name, ordinal_position, data_type, is_nullable, column_default, column_comment, constraint_type, constraint_name, column_tags, table_tags
   - Save to: data_product_accelerator/context/{chapter_3_lakehouse_schema}_Metadata.csv

5. Analyze the metadata and create docs/genie_plan.md with:
   - **Table Inventory**: List each table with its type, row purpose (inferred from table comment and column patterns), and estimated business domain
   - **Column Analysis**: Key columns per table ‚Äî identify likely dimensions, measures, timestamps, and foreign keys based on data types, names, and comments
   - **Relationship Map**: Inferred relationships between tables (from FK constraints and column naming patterns like *_id)
   - **Table Relevance Assessment**: For each table, assess relevance to the use case (High/Medium/Low) with rationale
   - **Recommended Genie Space Structure**: Suggest how tables should be grouped into Genie Spaces (max 25 assets per space)
   - **Metric View Candidates**: Identify numeric columns with business context that could become Metric Views (with suggested dimensions and measures)
   - **TVF Candidates**: Suggest parameterized query patterns based on common access patterns inferred from table structure
   - **Data Lineage Notes**: Document any lineage hints from column comments or naming conventions

Known warehouse ID: <YOUR_WAREHOUSE_ID> (get via: databricks warehouses list --output json | jq ''.[0].id'')
```',
'',
'Analyze Silver Metadata',
'Extract and analyze comprehensive table/column metadata from Silver layer schema including comments, constraints, and tags',
22,
'## 1Ô∏è‚É£ How To Apply

Copy the prompt from the Prompt tab, start a new Agent chat in your IDE, paste it and press Enter.

**Prerequisite:** Run this in your cloned Template Repository (see Prerequisites in Step 0). Ensure Databricks CLI is authenticated.

**Steps:** Copy the prompt ‚Üí paste into Cursor or VS Code with Copilot ‚Üí AI executes 6 SQL queries via Databricks CLI ‚Üí merges results into enriched CSV ‚Üí creates analysis document.

**Note:** The source catalog and schema are shown in the **Silver Layer** panel above this prompt. You can edit them using the Edit button.

---

## 2Ô∏è‚É£ What Are We Building?

This step extracts **comprehensive metadata** from your Silver layer ‚Äî not just column names and types, but also table comments, column comments, constraints, and Unity Catalog tags. This enriched metadata powers the Gold layer design.

### Two Output Files

| File | Purpose |
|------|---------|
| `data_product_accelerator/context/{chapter_3_lakehouse_schema}_Metadata.csv` | Enriched metadata CSV with all table/column/constraint/tag information. Fed into Gold Layer Design. |
| `docs/genie_plan.md` | Analysis document with table relevance, relationship maps, Genie Space recommendations, and metric/TVF candidates. |

### Why Enriched Metadata Matters

| Data Point | What It Tells Us |
|------------|-----------------|
| **Column comments** | Business meaning and context for each field |
| **Table comments** | Purpose and scope of each table |
| **PK/FK constraints** | Explicit relationships between tables |
| **Column tags** | Governance classifications and sensitivity levels |
| **Data types** | Dimension vs. measure classification hints |

---

## 3Ô∏è‚É£ Why Are We Building It This Way? (Databricks Best Practices)

| Practice | How It''''s Used Here |
|----------|-------------------|
| **Unity Catalog information_schema** | Queries the standard UC metadata catalog for comprehensive table/column metadata |
| **Constraint Discovery** | Extracts PK/FK from `table_constraints` to understand explicit relationships |
| **Tag Integration** | Pulls UC tags for governance context and data classification |
| **Graceful Degradation** | Tag queries skip gracefully if the views don''''t exist |
| **Analysis-Driven Design** | The genie_plan.md provides a reasoned assessment before jumping into Gold design |',
'## Expected Deliverables

- `data_product_accelerator/context/{chapter_3_lakehouse_schema}_Metadata.csv` ‚Äî enriched metadata CSV
- `docs/genie_plan.md` ‚Äî analysis with table relevance, relationships, Genie Space recommendations, metric/TVF candidates
- CSV contains: table_name, table_type, table_comment, column_name, data_type, is_nullable, column_comment, constraint_type, constraint_name, column_tags, table_tags',
true, 1, true, current_timestamp(), current_timestamp(), current_user());

-- Gold Layer Design for Genie Accelerator (references _Metadata.csv + PRD) - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(115, 'genie_gold_design',
'I have enriched silver layer metadata at @data_product_accelerator/context/{chapter_3_lakehouse_schema}_Metadata.csv and a metadata analysis at @docs/genie_plan.md.

The business requirements are documented in @docs/design_prd.md.

Please design the Gold layer using @data_product_accelerator/skills/gold/00-gold-layer-design/SKILL.md

This skill will orchestrate the following end-to-end design workflow:

- **Parse the metadata CSV** ‚Äî read the enriched metadata file (includes table comments, column comments, constraints, and tags), classify each table as a dimension, fact, or bridge, and use constraint info to map foreign key relationships
- **Cross-reference with PRD** ‚Äî align the Gold design with business requirements, user personas, and use case workflows documented in the PRD
- **Cross-reference with Genie plan** ‚Äî use the genie_plan.md analysis for table relevance assessments and recommended Genie Space structure
- **Design the dimensional model** ‚Äî identify dimensions (with SCD Type 1/2 decisions), fact tables (with explicit grain definitions), and measures, then assign tables to business domains
- **Create ERD diagrams** ‚Äî generate Mermaid Entity-Relationship Diagrams organized by table count (master ERD always, plus domain and summary ERDs for larger schemas)
- **Generate YAML schema files** ‚Äî produce one YAML file per Gold table with column definitions, PK/FK constraints, table properties, lineage metadata, and dual-purpose descriptions (human + LLM readable)
- **Document column-level lineage** ‚Äî trace every Gold column back through Silver with transformation type (DIRECT_COPY, AGGREGATION, DERIVATION, etc.) in both CSV and Markdown formats
- **Create business documentation** ‚Äî write a Business Onboarding Guide with domain context, real-world scenarios, and role-based getting-started guides
- **Map source tables** ‚Äî produce a Source Table Mapping CSV documenting which source tables are included, excluded, or planned with rationale for each
- **Validate design consistency** ‚Äî cross-check YAML schemas, ERD diagrams, and lineage CSV to ensure all columns, relationships, and constraints are consistent

The orchestrator skill will automatically load its worker skills for merge patterns, deduplication, documentation standards, Mermaid ERDs, schema validation, grain validation, and YAML-driven setup.

IMPORTANT: Use the EXISTING catalog `{lakehouse_default_catalog}` -- do NOT create a new catalog. Create the Gold schema `{user_schema_prefix}_gold` and all Gold tables inside this catalog.

NOTE: Before creating the schema, check if `{lakehouse_default_catalog}.{user_schema_prefix}_gold` already exists. If it does, DROP the schema with CASCADE and recreate it from scratch. These are user-specific schemas so dropping is safe.',
'',
'Gold Layer Design (Genie Accelerator)',
'Design Gold layer from enriched silver metadata and PRD using project skills with YAML definitions and Mermaid ERD',
23,
'## 1Ô∏è‚É£ How To Apply

Copy the prompt from the **Prompt** tab, start a **new Agent chat** in your IDE, paste it, and press Enter.

---

### Prerequisite

**Run this in your cloned Template Repository** (see Prerequisites in Step 0).

Ensure you have:
- ‚úÖ `data_product_accelerator/context/{chapter_3_lakehouse_schema}_Metadata.csv` - Your enriched silver metadata (from Analyze Silver Metadata step)
- ‚úÖ `docs/genie_plan.md` - Metadata analysis with table relevance and Genie recommendations
- ‚úÖ `docs/design_prd.md` - Product Requirements Document (from PRD Generation step)
- ‚úÖ `data_product_accelerator/skills/gold/00-gold-layer-design/SKILL.md` - The Gold layer design orchestrator skill

---

### Steps to Apply

1. **Start new Agent thread** ‚Äî Open Cursor and start a new Agent thread for clean context
2. **Copy and paste the prompt** ‚Äî Use the copy button, paste into Cursor; the AI will read your metadata, PRD, genie plan, and the orchestrator skill
3. **Review generated design** ‚Äî The AI creates `gold_layer_design/` with ERD diagrams, YAML schema files, and lineage documentation
4. **Validate the design** ‚Äî Check grain, SCD type, relationships, and lineage for each fact/dimension
5. **Verify PRD alignment** ‚Äî Ensure the Gold design supports the business requirements from the PRD

---

## 2Ô∏è‚É£ What Are We Building?

This is the **Genie Accelerator variant** of Gold Layer Design. Unlike the standard path that starts from raw schema CSV, this version uses:

| Input | What It Provides |
|-------|-----------------|
| **Enriched Metadata CSV** | Table/column comments, constraints, and tags from the Silver layer |
| **Genie Plan (genie_plan.md)** | Pre-analyzed table relevance, relationship maps, and Genie Space recommendations |
| **PRD (design_prd.md)** | Business requirements, user personas, and success criteria |

This triple-input approach produces a Gold design that is **already optimized for Genie Space consumption** ‚Äî with the right dimensions, measures, and TVF patterns identified upfront.

---

## 3Ô∏è‚É£ Why Are We Building It This Way? (Databricks Best Practices)

| Practice | How It''''s Used Here |
|----------|-------------------|
| **Metadata-Driven Design** | Uses enriched metadata (comments, constraints, tags) instead of raw column lists ‚Äî producing more accurate table classifications |
| **PRD Alignment** | Cross-references business requirements to ensure Gold tables serve actual use cases |
| **Genie-Optimized** | The genie_plan.md pre-identifies metric view and TVF candidates, so the Gold design accounts for downstream semantic layer needs |
| **YAML-Driven Dimensional Modeling** | Gold schemas defined as YAML files ‚Äî reviewable, version-controlled, machine-readable |
| **Dual-Purpose COMMENTs** | Table and column COMMENTs serve both business users AND Genie/LLMs |',
'## Expected Deliverables

- `gold_layer_design/` folder with:
  - ERD diagrams (Mermaid) ‚Äî master + domain ERDs
  - YAML schema files ‚Äî one per Gold table
  - COLUMN_LINEAGE.csv ‚Äî Silver-to-Gold column mappings
  - SOURCE_TABLE_MAPPING.csv ‚Äî table inclusion/exclusion rationale
  - BUSINESS_ONBOARDING_GUIDE.md ‚Äî stakeholder documentation
- Gold design aligned with PRD requirements and Genie plan recommendations',
true, 1, true, current_timestamp(), current_timestamp(), current_user());

-- Step 23: Deploy Lakehouse Assets - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(116, 'deploy_lakehouse_assets',
'Deploy and run all Bronze, Silver, and Gold layer jobs end-to-end using @data_product_accelerator/skills/common/databricks-asset-bundles/SKILL.md and @data_product_accelerator/skills/common/databricks-autonomous-operations/SKILL.md

This is a **deployment checkpoint** ‚Äî it validates and runs the complete Lakehouse pipeline in dependency order.

## Deployment Order (Mandatory)

Run these commands in strict sequence ‚Äî each stage depends on the previous one:

```bash
# 1. Validate the bundle (catches config errors before deploy)
databricks bundle validate -t dev

# 2. Deploy all assets to workspace
databricks bundle deploy -t dev

# 3. Run Bronze clone job (creates Bronze tables from source)
databricks bundle run -t dev bronze_clone_job

# 4. Run Silver DQ setup job FIRST (creates dq_rules table ‚Äî must exist before pipeline)
databricks bundle run -t dev silver_dq_setup_job

# 5. Run Silver DLT pipeline (reads from Bronze via CDF, applies DQ rules)
databricks bundle run -t dev silver_dlt_pipeline

# 6. Run Gold setup job (creates tables from YAML + adds PK/FK constraints)
databricks bundle run -t dev gold_setup_job

# 7. Run Gold merge job (deduplicates Silver ‚Üí merges into Gold)
databricks bundle run -t dev gold_merge_job
```

If any job fails, use the autonomous operations skill to diagnose and fix:
- Get the failed task `run_id` (not the parent job `run_id`)
- Run `databricks runs get-run-output --run-id <TASK_RUN_ID>` to diagnose
- Apply fix and redeploy (max 3 iterations before escalation)

## Verification Queries

After all jobs complete successfully, verify end-to-end:

```sql
-- Bronze: verify tables and CDF
SHOW TABLES IN {lakehouse_default_catalog}.{user_schema_prefix}_bronze;

-- Silver: verify DQ rules and cleaned tables
SELECT COUNT(*) FROM {lakehouse_default_catalog}.{user_schema_prefix}_silver.dq_rules;
SHOW TABLES IN {lakehouse_default_catalog}.{user_schema_prefix}_silver;

-- Gold: verify tables, constraints, and row counts
SHOW TABLES IN {lakehouse_default_catalog}.{user_schema_prefix}_gold;
SELECT * FROM {lakehouse_default_catalog}.information_schema.table_constraints
WHERE table_schema = ''{user_schema_prefix}_gold'';
```

Target catalog: `{lakehouse_default_catalog}`
Target schemas: `{user_schema_prefix}_bronze`, `{user_schema_prefix}_silver`, `{user_schema_prefix}_gold`',
'',
'Deploy Lakehouse Assets (Bronze ‚Üí Silver ‚Üí Gold)',
'Validate, deploy, and run all Bronze, Silver, and Gold layer jobs in dependency order using Asset Bundles with autonomous operations',
23,
'## 1Ô∏è‚É£ How To Apply

Copy the prompt from the **Prompt** tab, start a **new Agent chat** in your IDE, paste it, and press Enter.

---

### Prerequisite

**Run this in your cloned Template Repository** (see Prerequisites in Step 0).

Ensure you have:
- ‚úÖ Bronze layer code generated (Step 10): `src/source_bronze/`, `resources/bronze/`
- ‚úÖ Silver layer code generated (Step 11): `src/source_silver/`, `resources/silver/`
- ‚úÖ Gold layer code generated (Step 12): `src/source_gold/`, `resources/gold/`, `gold_layer_design/yaml/`
- ‚úÖ `databricks.yml` bundle configuration file (created/updated in Steps 10-12)
- ‚úÖ Databricks CLI installed and authenticated (`databricks auth login`)

---

### Steps to Apply

**Step 1: Start New Agent Thread** ‚Äî Open Cursor and start a new Agent thread for clean context.

**Step 2: Copy and Paste the Prompt** ‚Äî Use the copy button, paste it into Cursor. The AI reads both the Asset Bundles skill and the Autonomous Operations skill.

**Step 3: Validate** ‚Äî The AI runs `databricks bundle validate -t dev` to catch config errors before deploying.

**Step 4: Deploy** ‚Äî The AI runs `databricks bundle deploy -t dev` to push all assets to your workspace.

**Step 5: Run Jobs in Dependency Order** ‚Äî The AI runs each job in sequence:

```
Bronze clone job
    ‚Üì
Silver DQ setup job (creates dq_rules table)
    ‚Üì
Silver DLT pipeline (reads Bronze via CDF)
    ‚Üì
Gold setup job (2 tasks: create tables ‚Üí add FK constraints)
    ‚Üì
Gold merge job (dedup Silver ‚Üí merge into Gold)
```

**Step 6: Diagnose Failures (if any)** ‚Äî If a job fails, the autonomous operations skill kicks in:
1. Get failed task `run_id` from the job run
2. Run `databricks runs get-run-output --run-id <TASK_RUN_ID>`
3. Match error against known patterns, apply fix, redeploy
4. Max 3 iterations before escalation

**Step 7: Verify End-to-End** ‚Äî Run the verification queries to confirm all layers are populated.

---

## 2Ô∏è‚É£ What Are We Building?

This is a **deployment checkpoint** that validates the entire Lakehouse pipeline works end-to-end before moving to the Data Intelligence layer.

### Asset Bundle Structure (Built in Steps 10-12)

```
project_root/
‚îú‚îÄ‚îÄ databricks.yml                        # Bundle configuration (all layers)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ source_bronze/                    # Bronze notebooks (clone/generate)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clone_samples.py
‚îÇ   ‚îú‚îÄ‚îÄ source_silver/                    # Silver notebooks (DLT + DQ)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setup_dq_rules_table.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dq_rules_loader.py           # Pure Python (NO notebook header)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver_dimensions.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver_facts.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_quality_monitoring.py
‚îÇ   ‚îî‚îÄ‚îÄ source_gold/                      # Gold notebooks (YAML-driven)
‚îÇ       ‚îú‚îÄ‚îÄ setup_tables.py
‚îÇ       ‚îú‚îÄ‚îÄ add_fk_constraints.py
‚îÇ       ‚îî‚îÄ‚îÄ merge_gold_tables.py
‚îú‚îÄ‚îÄ resources/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bronze_clone_job.yml
‚îÇ   ‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver_dq_setup_job.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ silver_dlt_pipeline.yml
‚îÇ   ‚îî‚îÄ‚îÄ gold/
‚îÇ       ‚îú‚îÄ‚îÄ gold_setup_job.yml
‚îÇ       ‚îî‚îÄ‚îÄ gold_merge_job.yml
‚îî‚îÄ‚îÄ gold_layer_design/yaml/               # YAML schemas (synced to workspace)
```

### Deployment Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LAKEHOUSE DEPLOYMENT CHECKPOINT                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  Step 1: VALIDATE                                                           ‚îÇ
‚îÇ  databricks bundle validate -t dev                                          ‚îÇ
‚îÇ         ‚Üì                                                                   ‚îÇ
‚îÇ  Step 2: DEPLOY                                                             ‚îÇ
‚îÇ  databricks bundle deploy -t dev                                            ‚îÇ
‚îÇ         ‚Üì                                                                   ‚îÇ
‚îÇ  Step 3: RUN IN ORDER                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ   Bronze    ‚îÇ‚Üí ‚îÇ Silver DQ    ‚îÇ‚Üí ‚îÇ Silver DLT  ‚îÇ‚Üí ‚îÇ  Gold Setup  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  clone_job  ‚îÇ  ‚îÇ setup_job    ‚îÇ  ‚îÇ  pipeline   ‚îÇ  ‚îÇ  (2 tasks)   ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                              ‚Üì              ‚îÇ
‚îÇ                                                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ                                                      ‚îÇ  Gold Merge  ‚îÇ      ‚îÇ
‚îÇ                                                      ‚îÇ    job       ‚îÇ      ‚îÇ
‚îÇ                                                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ         ‚Üì                                                                   ‚îÇ
‚îÇ  Step 4: VERIFY                                                             ‚îÇ
‚îÇ  SHOW TABLES / row counts / constraints / CDF checks                       ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ON FAILURE ‚Üí Autonomous Operations (diagnose ‚Üí fix ‚Üí redeploy, max 3x)   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 3Ô∏è‚É£ Why Are We Building It This Way? (Databricks Best Practices)

| Practice | How It''''s Used Here |
|----------|-------------------|
| **Asset Bundles** | Single `databricks.yml` manages all notebooks, pipelines, and jobs as a versioned, deployable unit |
| **Serverless Compute** | Every job uses `environments` with `environment_version: "4"` ‚Äî no cluster management |
| **Dependency-Ordered Execution** | Bronze ‚Üí Silver DQ ‚Üí Silver DLT ‚Üí Gold Setup ‚Üí Gold Merge ‚Äî each stage depends on the previous |
| **Autonomous Operations** | Deploy ‚Üí Poll ‚Üí Diagnose ‚Üí Fix ‚Üí Redeploy loop with max 3 iterations before escalation |
| **Idempotent Deploys** | `databricks bundle deploy` is safe to run multiple times ‚Äî no duplicates |
| **Task-Level Diagnostics** | Failed task `run_id` (not parent job `run_id`) used for `get-run-output` ‚Äî provides actionable error details |
| **notebook_task** | All jobs use `notebook_task` (never `python_task`) with `base_parameters` dict (never CLI-style `parameters`) |
| **Environment Separation** | Bundle targets (`-t dev`, `-t staging`, `-t prod`) for multi-environment deployments from the same config |

---

## 4Ô∏è‚É£ What Happens Behind the Scenes?

When you paste the prompt, the AI reads two skills:

1. **`@data_product_accelerator/skills/common/databricks-asset-bundles/SKILL.md`** ‚Äî validates bundle structure, ensures serverless environments, proper task types, and parameter patterns
2. **`@data_product_accelerator/skills/common/databricks-autonomous-operations/SKILL.md`** ‚Äî provides the deploy-poll-diagnose-fix loop for self-healing when jobs fail

The autonomous operations skill follows this protocol:
1. Run `databricks bundle run` and capture the RUN_ID from the output URL
2. Poll with exponential backoff (30s ‚Üí 60s ‚Üí 120s) until terminal state
3. On SUCCESS: verify all tasks completed, report run URL
4. On FAILURE: get failed task `run_id`, run `get-run-output`, match error pattern, apply fix
5. Redeploy and re-run (max 3 iterations before escalation with full error context)',
'## Expected Deliverables

### ‚úÖ Deployment Verification

**Bundle:**
- [ ] `databricks bundle validate -t dev` passes with no errors
- [ ] `databricks bundle deploy -t dev` completes successfully
- [ ] All 5 jobs appear in Databricks Workflows UI

**Bronze Layer:**
- [ ] `bronze_clone_job` completes successfully
- [ ] All tables visible in `{lakehouse_default_catalog}.{user_schema_prefix}_bronze`
- [ ] CDF enabled on all Bronze tables (`delta.enableChangeDataFeed = true`)

**Silver Layer:**
- [ ] `silver_dq_setup_job` creates `dq_rules` table in Silver schema
- [ ] `silver_dlt_pipeline` completes with Expectations evaluated
- [ ] Silver tables populated with cleaned data
- [ ] Row tracking enabled (`delta.enableRowTracking = true`)

**Gold Layer:**
- [ ] `gold_setup_job` creates all Gold tables with PK constraints (Task 1) and FK constraints (Task 2)
- [ ] `gold_merge_job` populates Gold tables from Silver
- [ ] PK/FK constraints visible in `information_schema.table_constraints`
- [ ] Fact-to-dimension joins resolve correctly (no orphan records)

**End-to-End:**
- [ ] Data flows from Bronze ‚Üí Silver ‚Üí Gold without errors
- [ ] Row counts are reasonable across all layers
- [ ] Ready for Data Intelligence layer (Genie, Dashboards)',
true, 1, true, current_timestamp(), current_timestamp(), current_user());

-- Step 24: Deploy Data Intelligence Assets - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(117, 'deploy_di_assets',
'Deploy all Data Intelligence assets (TVFs, Metric Views, Genie Spaces, and AI/BI Dashboards) using @data_product_accelerator/skills/common/databricks-asset-bundles/SKILL.md and @data_product_accelerator/skills/semantic-layer/04-genie-space-export-import-api/SKILL.md

This is a **semantic layer deployment checkpoint** ‚Äî it deploys and verifies all Data Intelligence assets in the correct order.

## Deployment Order (Mandatory)

Deploy in this sequence ‚Äî each component depends on the previous:

```bash
# 1. Validate the bundle
databricks bundle validate -t dev

# 2. Deploy all assets to workspace
databricks bundle deploy -t dev

# 3. Deploy TVFs (SQL task ‚Äî creates parameterized functions in Gold schema)
databricks bundle run -t dev tvf_job

# 4. Deploy Metric Views (Python task ‚Äî creates WITH METRICS LANGUAGE YAML views)
databricks bundle run -t dev metric_views_job

# 5. Deploy AI/BI Dashboard (if applicable)
databricks bundle run -t dev dashboard_deploy_job

# 6. Deploy Genie Space via Export/Import API
#    Uses UPDATE-or-CREATE pattern with variable substitution
databricks bundle run -t dev genie_deploy_job
```

## Genie Space API Deployment

The Genie Space is deployed programmatically using the Export/Import API skill:
- JSON config exported with `${catalog}` and `${gold_schema}` template variables
- All IDs use `uuid.uuid4().hex` (32-char hex, no dashes)
- `serialized_space` is a JSON string (`json.dumps()`), not a nested object
- Data asset arrays sorted before submission (tables by `table_name`, TVFs by `function_name`)
- Genie Space MUST use a **Serverless SQL Warehouse** (non-negotiable)

## Verification

```sql
-- Verify TVFs are created
SELECT * FROM get_revenue_by_period(''2024-01-01'', ''2024-12-31'');

-- Verify Metric Views exist
SELECT table_name, table_type FROM {lakehouse_default_catalog}.information_schema.tables
WHERE table_schema = ''{user_schema_prefix}_gold'' AND table_type = ''METRIC_VIEW'';

-- Test Metric View queries
SELECT MEASURE(total_revenue) FROM {lakehouse_default_catalog}.{user_schema_prefix}_gold.revenue_analytics_metrics;
```

Target catalog: `{lakehouse_default_catalog}`
Gold schema: `{user_schema_prefix}_gold`',
'',
'Deploy Semantic Layer Assets (TVFs ‚Üí Metric Views ‚Üí Genie ‚Üí Dashboard)',
'Deploy TVFs, Metric Views, Genie Spaces (via Export/Import API), and AI/BI Dashboards in dependency order',
24,
'## 1Ô∏è‚É£ How To Apply

Copy the prompt from the **Prompt** tab, start a **new Agent chat** in your IDE, paste it, and press Enter.

---

### Prerequisite

**Run this in your cloned Template Repository** (see Prerequisites in Step 0).

Ensure you have:
- ‚úÖ Lakehouse Deployment Checkpoint passed (Step 23) ‚Äî Bronze, Silver, Gold tables populated
- ‚úÖ Semantic layer code generated (Step 15): `src/source_gold/table_valued_functions.sql`, `src/source_gold/semantic/`, `src/source_gold/genie/`
- ‚úÖ AI/BI Dashboard generated (Step 14): `docs/dashboards/*.lvdash.json`, `scripts/deploy_dashboard.py`
- ‚úÖ Plan files: `plans/phase1-addendum-1.2-tvfs.md`, `plans/phase1-addendum-1.3-metric-views.md`, `plans/phase1-addendum-1.6-genie-spaces.md`
- ‚úÖ Serverless SQL Warehouse available in your workspace
- ‚úÖ Databricks CLI installed and authenticated

---

### Steps to Apply

**Step 1: Start New Agent Thread** ‚Äî Open Cursor and start a new Agent thread for clean context.

**Step 2: Copy and Paste the Prompt** ‚Äî Use the copy button, paste it into Cursor. The AI reads the Asset Bundles skill and the Genie Space Export/Import API skill.

**Step 3: Deploy TVFs** ‚Äî SQL task creates parameterized functions (STRING date params, v3.0 bullet COMMENTs, ROW_NUMBER for Top-N).

**Step 4: Deploy Metric Views** ‚Äî Python task creates `WITH METRICS LANGUAGE YAML` views with dimensions, measures, and synonyms.

**Step 5: Deploy AI/BI Dashboard** ‚Äî Workspace Import API with `overwrite: true` (UPDATE-or-CREATE pattern preserving URLs and permissions).

**Step 6: Deploy Genie Space** ‚Äî Export/Import API with:
- `${catalog}` / `${gold_schema}` variable substitution
- Data assets in priority order: Metric Views ‚Üí TVFs ‚Üí Gold Tables
- General Instructions (‚â§ 20 lines)
- ‚â• 10 benchmark questions with exact expected SQL
- Serverless SQL Warehouse (non-negotiable)

**Step 7: Verify All Components**

```sql
-- Test TVFs
SELECT * FROM get_revenue_by_period(''2024-01-01'', ''2024-12-31'');

-- Verify Metric Views
SELECT table_name, table_type FROM information_schema.tables
WHERE table_schema = ''{user_schema_prefix}_gold'' AND table_type = ''METRIC_VIEW'';

-- Navigate to Genie Space in Databricks UI and ask a sample question
```

---

## 2Ô∏è‚É£ What Are We Building?

This deployment checkpoint verifies the complete **Data Intelligence layer** ‚Äî everything end users interact with for analytics and natural language queries.

### Semantic Layer Deployment Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 SEMANTIC LAYER DEPLOYMENT CHECKPOINT                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  Step 1: VALIDATE & DEPLOY BUNDLE                                           ‚îÇ
‚îÇ  databricks bundle validate ‚Üí databricks bundle deploy                      ‚îÇ
‚îÇ         ‚Üì                                                                   ‚îÇ
‚îÇ  Step 2: DEPLOY IN ORDER                                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ    TVFs     ‚îÇ‚Üí ‚îÇ Metric Views ‚îÇ‚Üí ‚îÇ  Dashboard  ‚îÇ‚Üí ‚îÇ Genie Space  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  (SQL task) ‚îÇ  ‚îÇ (Python task)‚îÇ  ‚îÇ (Import API)‚îÇ  ‚îÇ(Export/Import)‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  Step 3: VERIFY                                                             ‚îÇ
‚îÇ  TVF execution ‚îÇ Metric View queries ‚îÇ Dashboard renders ‚îÇ Genie NL test   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  Genie Space API Rules:                                                     ‚îÇ
‚îÇ  ‚Ä¢ serialized_space = json.dumps() (string, not nested object)             ‚îÇ
‚îÇ  ‚Ä¢ All IDs = uuid.uuid4().hex (32-char, no dashes)                         ‚îÇ
‚îÇ  ‚Ä¢ All arrays sorted before submission                                      ‚îÇ
‚îÇ  ‚Ä¢ Template vars: ${catalog}, ${gold_schema} (never hardcoded)             ‚îÇ
‚îÇ  ‚Ä¢ Serverless SQL Warehouse ONLY                                            ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Files Deployed

```
src/source_gold/
‚îú‚îÄ‚îÄ table_valued_functions.sql              # TVFs (STRING params, v3.0 comments)
‚îú‚îÄ‚îÄ semantic/
‚îÇ   ‚îî‚îÄ‚îÄ metric_views/
‚îÇ       ‚îú‚îÄ‚îÄ revenue_analytics_metrics.yaml  # Metric View YAML definitions
‚îÇ       ‚îî‚îÄ‚îÄ create_metric_views.py          # Reads YAML ‚Üí CREATE VIEW WITH METRICS
‚îú‚îÄ‚îÄ genie/
‚îÇ   ‚îî‚îÄ‚îÄ genie_space_config.json             # Exported Genie Space (CI/CD)
docs/dashboards/
‚îú‚îÄ‚îÄ analytics_dashboard.lvdash.json         # Dashboard JSON config
scripts/
‚îú‚îÄ‚îÄ deploy_dashboard.py                     # UPDATE-or-CREATE deployment
‚îú‚îÄ‚îÄ deploy_genie_space.py                   # Genie Space API deployment
resources/semantic-layer/
‚îú‚îÄ‚îÄ tvf_job.yml                             # SQL task for TVF deployment
‚îú‚îÄ‚îÄ metric_views_job.yml                    # Python task for Metric Views
‚îî‚îÄ‚îÄ genie_deploy_job.yml                    # Genie Space API deployment job
resources/monitoring/
‚îî‚îÄ‚îÄ dashboard_deploy_job.yml                # Dashboard deployment job
```

---

## 3Ô∏è‚É£ Why Are We Building It This Way? (Databricks Best Practices)

| Practice | How It''''s Used Here |
|----------|-------------------|
| **Dependency-Ordered Deployment** | TVFs ‚Üí Metric Views ‚Üí Dashboard ‚Üí Genie Space ‚Äî each depends on the previous |
| **Genie Space Export/Import API** | Programmatic deployment via REST API with `serialized_space` as JSON string, UUID IDs, sorted arrays, and variable substitution |
| **Serverless SQL Warehouse** | Genie Spaces MUST use Serverless SQL warehouse ‚Äî required for NL query execution (never Classic or Pro) |
| **Variable Substitution** | `${catalog}` and `${gold_schema}` in all queries and configs ‚Äî never hardcoded catalog/schema |
| **UPDATE-or-CREATE Pattern** | Dashboard and Genie Space deploy use idempotent update-or-create ‚Äî preserves URLs and permissions |
| **Asset Bundles for SQL Tasks** | TVFs deployed via `sql_task` in bundle YAML; Metric Views via `notebook_task` |
| **Dual Validation** | Pre-deploy SQL validation for dashboards (`validate_dashboard_queries.py`) and TVF compilation checks |

---

## 4Ô∏è‚É£ What Happens Behind the Scenes?

When you paste the prompt, the AI reads two skills:

1. **`@data_product_accelerator/skills/common/databricks-asset-bundles/SKILL.md`** ‚Äî validates bundle structure, ensures serverless environments, proper task types
2. **`@data_product_accelerator/skills/semantic-layer/04-genie-space-export-import-api/SKILL.md`** ‚Äî provides the JSON schema, ID generation, array sorting, and variable substitution patterns for programmatic Genie Space deployment

The Genie Space deployment follows this protocol:
1. Load `genie_space_config.json` from repo
2. Substitute `${catalog}` ‚Üí actual catalog, `${gold_schema}` ‚Üí actual schema
3. Generate UUIDs for all ID fields (`uuid.uuid4().hex`)
4. Sort all arrays (tables by `table_name`, TVFs by `function_name`, etc.)
5. Serialize as `json.dumps()` for the `serialized_space` field
6. Call Create or PATCH API (update-or-create pattern)
7. Verify with GET API

**Key constraint:** After this checkpoint passes, the Genie Space is live and queryable ‚Äî required before running Genie Optimization (Step 25).',
'## Expected Deliverables

### ‚úÖ Deployment Verification

**TVFs:**
- [ ] All TVFs created in `{lakehouse_default_catalog}.{user_schema_prefix}_gold` schema
- [ ] DATE parameters use STRING type (non-negotiable for Genie)
- [ ] v3.0 bullet-point COMMENTs applied
- [ ] TVF execution returns expected results

**Metric Views:**
- [ ] Created with `WITH METRICS LANGUAGE YAML` syntax
- [ ] `table_type = ''METRIC_VIEW''` in `information_schema.tables`
- [ ] `MEASURE()` queries return correct aggregations
- [ ] 3-5 synonyms per dimension/measure

**AI/BI Dashboard:**
- [ ] `.lvdash.json` deployed via Workspace Import API
- [ ] Dashboard renders correctly in Databricks UI
- [ ] Widget-query alignment verified (fieldName matches SQL alias)
- [ ] 6-column grid layout correct

**Genie Space:**
- [ ] Genie Space accessible in Databricks UI
- [ ] Uses Serverless SQL Warehouse (non-negotiable)
- [ ] Data assets include Metric Views, TVFs, and Gold tables
- [ ] General Instructions present (‚â§ 20 lines)
- [ ] ‚â• 10 benchmark questions with expected SQL
- [ ] Natural language queries produce correct SQL
- [ ] JSON export saved for CI/CD (`genie_space_config.json`)

**End-to-End:**
- [ ] All 4 components deployed and functional
- [ ] Variable substitution working (`${catalog}`, `${gold_schema}`)
- [ ] Ready for Genie Optimization (Step 25)',
true, 1, true, current_timestamp(), current_timestamp(), current_user());

-- Step 25: Optimize Genie - bypass_LLM = TRUE
INSERT INTO ${catalog}.${schema}.section_input_prompts 
(input_id, section_tag, input_template, system_prompt, section_title, section_description, order_number, how_to_apply, expected_output, bypass_llm, version, is_active, inserted_at, updated_at, created_by)
VALUES
(118, 'optimize_genie',
'Optimize your Genie Space for production accuracy using @data_product_accelerator/skills/semantic-layer/05-genie-optimization-orchestrator/SKILL.md

This orchestrator runs a systematic **benchmark ‚Üí evaluate ‚Üí optimize ‚Üí apply ‚Üí re-evaluate** loop with 4 specialized workers and MLflow experiment tracking.

## Optimization Loop

The orchestrator executes up to **5 iterations**, applying 6 control levers in priority order until all quality targets are met:

### Phase 1: Baseline Evaluation
1. Snapshot current Genie Space metadata (instructions, assets, benchmarks)
2. Create MLflow LoggedModel for the Genie Space
3. Run the **Benchmark Generator** ‚Äî create/validate benchmark dataset with ‚â• 10 questions and ground-truth SQL
4. Run the **Benchmark Evaluator** ‚Äî evaluate all benchmarks using 8 quality scorers via `mlflow.genai.evaluate()`
5. Record baseline scores as iteration 0

### Phase 2: Per-Lever Optimization (Levers 1‚Üí5)
For each control lever in priority order:
1. Run the **Metadata Optimizer** ‚Äî analyze evaluation results and propose metadata changes for the current lever
2. Run the **Optimization Applier** ‚Äî apply proposals with **dual persistence** (Genie API + repo files)
3. Wait 30 seconds for Genie to pick up changes
4. Run slice evaluation (affected benchmarks only)
5. If slice passes ‚Üí run P0 gate (full evaluation)
6. If P0 fails ‚Üí **rollback** and move to next lever

### Phase 3: GEPA (Lever 6) ‚Äî Only if Still Below Target
- General-Purpose Architecture changes (add/remove data assets, restructure instructions)
- Applied ONLY after Levers 1-5 have been attempted
- Requires dual persistence verification (`git diff`)

### Phase 4: Deploy and Verify
- Promote best model iteration
- Run held-out evaluation (benchmarks not seen during optimization)
- Post-deploy verification

## 6 Control Levers (Priority Order)

| Lever | Target | What Gets Changed |
|-------|--------|-------------------|
| **1: UC Metadata** | Column/table COMMENTs, tags | Add synonyms, clarify ambiguous columns |
| **2: Metric Views** | YAML definitions, measures | Add missing measures, fix aggregation logic |
| **3: TVFs** | Function signatures, COMMENTs | Fix parameter types, improve BEST FOR guidance |
| **4: Monitoring Tables** | DQ metrics, freshness views | Add monitoring assets to Genie Space |
| **5: ML Tables** | Feature tables, predictions | Add ML outputs as Genie data assets |
| **6: GEPA** | Instructions, data assets | Restructure Genie Space architecture |

## 8 Quality Targets

| Scorer | Target | What It Measures |
|--------|--------|-----------------|
| **Syntax Correctness** | ‚â• 98% | Generated SQL parses without errors |
| **Schema Accuracy** | ‚â• 95% | All tables/columns exist in the catalog |
| **Logical Correctness** | ‚â• 90% | SQL logic matches the question intent |
| **Semantic Equivalence** | ‚â• 90% | Results equivalent to ground-truth SQL |
| **Completeness** | ‚â• 90% | All requested dimensions/measures present |
| **Result Correctness** | ‚â• 85% | Actual query results match expected values |
| **Asset Routing** | ‚â• 95% | Genie uses the right table/view/TVF |
| **Repeatability** | ‚â• 90% | Same question ‚Üí same SQL on repeated runs |

Target catalog: `{lakehouse_default_catalog}`
Gold schema: `{user_schema_prefix}_gold`',
'',
'Optimize Genie Space (Benchmark-Driven)',
'Systematically optimize Genie Space accuracy using 4 workers, 8 quality scorers, 6 control levers, and MLflow experiment tracking',
25,
'## 1Ô∏è‚É£ How To Apply

Copy the prompt from the **Prompt** tab, start a **new Agent chat** in your IDE, paste it, and press Enter.

---

### Prerequisite

**Run this in your cloned Template Repository** (see Prerequisites in Step 0).

Ensure you have:
- ‚úÖ Semantic Layer Deployment Checkpoint passed (Step 24) ‚Äî Genie Space live and queryable
- ‚úÖ Gold layer tables populated with data
- ‚úÖ Serverless SQL Warehouse running
- ‚úÖ Databricks CLI authenticated (profile resolved from `databricks.yml` ‚Üí `workspace.profile`)
- ‚úÖ MLflow access configured (`DATABRICKS_HOST`, `DATABRICKS_TOKEN`, `MLFLOW_TRACKING_URI=databricks`)

**Critical:** The Genie Space MUST be live and queryable before running optimization. This step runs ONLY after the Semantic Layer Deployment Checkpoint (Step 24) has passed.

---

### Steps to Apply

**Step 1: Start New Agent Thread** ‚Äî Open Cursor and start a new Agent thread for clean context.

**Step 2: Copy and Paste the Prompt** ‚Äî Use the copy button, paste it into Cursor. The AI reads the Genie Optimization Orchestrator skill which automatically loads 4 worker skills.

**Step 3: Phase 1 ‚Äî Baseline** ‚Äî The AI:
1. Snapshots current Genie Space metadata
2. Creates MLflow experiment under `/Users/<your-email>/`
3. Creates LoggedModel for the Genie Space
4. Runs **Benchmark Generator** ‚Äî creates/validates ‚â• 10 benchmark questions with ground-truth SQL
5. Runs **Benchmark Evaluator** ‚Äî 8 scorers via `mlflow.genai.evaluate(model_id=...)`, records baseline

**Step 4: Phase 2 ‚Äî Per-Lever Optimization** ‚Äî For each lever (1‚Üí5):
1. **Metadata Optimizer** analyzes evaluation results, proposes changes for current lever
2. **Optimization Applier** applies with dual persistence (API + repo), waits 30s
3. Slice evaluation on affected benchmarks
4. P0 gate (full evaluation) ‚Äî if fails, rollback and try next lever

**Step 5: Phase 3 ‚Äî GEPA (if needed)** ‚Äî Lever 6 applied only if still below target after Levers 1-5.

**Step 6: Phase 4 ‚Äî Deploy and Verify** ‚Äî Promote best model, held-out evaluation, post-deploy check.

**Step 7: Review Results** ‚Äî Check MLflow experiment for iteration scores, view `optimization-progress.json` for session state.

---

## 2Ô∏è‚É£ What Are We Building?

A **production-ready Genie Space** that consistently generates accurate SQL from natural language queries, verified by systematic benchmark evaluation.

### Optimization Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GENIE OPTIMIZATION LOOP                                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  PHASE 1: BASELINE                                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Snapshot ‚Üí LoggedModel ‚Üí Benchmark Generator ‚Üí Evaluator (iter 0)  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                 ‚Üì                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  PHASE 2: PER-LEVER OPTIMIZATION (max 5 iterations)                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  For lever = 1 ‚Üí 5:                                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    Metadata Optimizer ‚Üí Optimization Applier ‚Üí wait 30s             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ         ‚Üì                                                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    Slice eval ‚Üí P0 gate ‚Üí if fail: rollback, next lever            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                         ‚Üí if pass: check all targets                ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                 ‚Üì                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  PHASE 3: GEPA (Lever 6) ‚Äî only if still below target              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Architecture-level changes with dual persistence verification     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                 ‚Üì                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  PHASE 4: DEPLOY & VERIFY                                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Promote best model ‚Üí held-out eval ‚Üí post-deploy check            ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  MLflow Tracking: Every evaluation logged ‚îÇ Session: optimization-progress ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4 Worker Skills (Loaded by Orchestrator)

| Worker | Path | Purpose |
|--------|------|---------|
| **01-genie-benchmark-generator** | `data_product_accelerator/skills/semantic-layer/genie-optimization-workers/01-*/SKILL.md` | Create/validate benchmarks, sync to MLflow dataset |
| **02-genie-benchmark-evaluator** | `data_product_accelerator/skills/semantic-layer/genie-optimization-workers/02-*/SKILL.md` | 8 scorers via `mlflow.genai.evaluate()`, eval scopes (full/slice/P0/held-out) |
| **03-genie-metadata-optimizer** | `data_product_accelerator/skills/semantic-layer/genie-optimization-workers/03-*/SKILL.md` | Lever-aware analysis (L1-L5: targeted, L6: GEPA), 6 control levers |
| **04-genie-optimization-applier** | `data_product_accelerator/skills/semantic-layer/genie-optimization-workers/04-*/SKILL.md` | Apply proposals with dual persistence (Genie API + repo files) |

### Dual Persistence (Non-Negotiable)

Every metadata change must be applied to BOTH:
1. **Genie API** ‚Äî live Genie Space updated via PATCH/Create API
2. **Repo files** ‚Äî `genie_space_config.json` and source SQL files updated

Verify with `git diff` after each apply. If either persistence fails, the optimization is incomplete.

---

## 3Ô∏è‚É£ Why Are We Building It This Way? (Databricks Best Practices)

| Practice | How It''''s Used Here |
|----------|-------------------|
| **MLflow Experiment Tracking** | Every evaluation iteration logged to MLflow with model_id, scores, and metadata ‚Äî enables comparison across iterations |
| **LoggedModel per Genie Space** | `mlflow.genai.evaluate(model_id=...)` ties evaluations to a specific Genie Space version |
| **8 Quality Scorers** | Comprehensive evaluation: syntax, schema, logic, semantics, completeness, results, asset routing, repeatability |
| **6 Control Levers in Priority Order** | Levers 1‚Üí5 applied sequentially (targeted fixes first), Lever 6 (GEPA) only as last resort |
| **Dual Persistence** | Changes applied to BOTH Genie API (live) and repo files (version-controlled) ‚Äî prevents drift |
| **Slice ‚Üí P0 ‚Üí Held-Out Evaluation** | Slice eval (affected benchmarks) ‚Üí P0 gate (full suite) ‚Üí held-out (unseen benchmarks) ‚Äî prevents overfitting |
| **Rollback on Regression** | If P0 gate fails after applying a lever, changes are rolled back before trying the next lever |
| **SQL Sanitization** | All Genie SQL processed through `sanitize_sql()` before `EXPLAIN` or `spark.sql()` ‚Äî handles multi-statement, comments, markdown |
| **Ground-Truth Variable Resolution** | `${catalog}` / `${gold_schema}` in ground-truth SQL resolved via `resolve_sql()` before execution |
| **Max 5 Iterations** | Hard limit prevents infinite optimization loops ‚Äî escalate with context if targets not met |

---

## 4Ô∏è‚É£ What Happens Behind the Scenes?

When you paste the prompt, the AI reads `@data_product_accelerator/skills/semantic-layer/05-genie-optimization-orchestrator/SKILL.md` ‚Äî the **Genie Optimization Orchestrator**. Behind the scenes:

1. **CLI profile resolution** ‚Äî resolves Databricks profile from `databricks.yml` ‚Üí `workspace.profile` before any API call
2. **MLflow setup** ‚Äî creates experiment under `/Users/<email>/`, registers judge prompts to MLflow Prompt Registry
3. **Worker routing table** ‚Äî mandatory routing; every worker invocation reads its SKILL.md
4. **Session state** ‚Äî persisted in `optimization-progress.json` and MLflow experiment tags; enables resume after interruption
5. **Lever-aware optimization** ‚Äî Metadata Optimizer receives the current lever number and only proposes changes within that lever''''s scope
6. **GEPA (Lever 6)** ‚Äî General-Purpose Architecture changes applied ONLY after Levers 1-5; includes add/remove data assets and instruction restructuring
7. **Dual persistence verification** ‚Äî after every apply, verifies both API success AND `git diff` shows expected repo changes

**Key constraint:** The orchestrator MUST start with the Benchmark Generator (never skip to Evaluator) and MUST create a LoggedModel before the first evaluation.',
'## Expected Deliverables

### ‚úÖ Optimization Results

**MLflow Tracking:**
- [ ] MLflow experiment created under `/Users/<email>/`
- [ ] LoggedModel created for the Genie Space
- [ ] Baseline evaluation logged (iteration 0)
- [ ] Each lever iteration logged with scores

**Quality Targets (all must pass):**
- [ ] Syntax Correctness ‚â• 98%
- [ ] Schema Accuracy ‚â• 95%
- [ ] Logical Correctness ‚â• 90%
- [ ] Semantic Equivalence ‚â• 90%
- [ ] Completeness ‚â• 90%
- [ ] Result Correctness ‚â• 85%
- [ ] Asset Routing ‚â• 95%
- [ ] Repeatability ‚â• 90%

**Dual Persistence:**
- [ ] Genie API updated with optimized metadata
- [ ] Repo files updated (`genie_space_config.json`, source SQL)
- [ ] `git diff` confirms expected changes

**Session State:**
- [ ] `optimization-progress.json` records all iterations
- [ ] Best model iteration identified and promoted
- [ ] Held-out evaluation passes (unseen benchmarks)

**Optimization Report:**
- [ ] Summary of levers applied and their impact
- [ ] Before/after scores for each quality dimension
- [ ] Remaining known limitations (if any)
- [ ] Recommendations for future optimization cycles',
true, 1, true, current_timestamp(), current_timestamp(), current_user());
