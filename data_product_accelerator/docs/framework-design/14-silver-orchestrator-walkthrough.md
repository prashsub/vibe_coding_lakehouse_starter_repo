# Silver Orchestrator â€” Visual Walkthrough

How the `00-silver-layer-setup` orchestrator progressively loads 2 worker skills and 6 common skills across 7 phases, building production-grade Spark Declarative Pipelines (SDP/DLT) with Delta table-based data quality rules, quarantine patterns, and monitoring views.

> **Related skills:** [`00-silver-layer-setup`](../../skills/silver/00-silver-layer-setup/SKILL.md), [`01-dlt-expectations-patterns`](../../skills/silver/01-dlt-expectations-patterns/SKILL.md), [`02-dqx-patterns`](../../skills/silver/02-dqx-patterns/SKILL.md)
>
> **Common skills used:** [`databricks-expert-agent`](../../skills/common/databricks-expert-agent/SKILL.md), [`databricks-table-properties`](../../skills/common/databricks-table-properties/SKILL.md), [`databricks-python-imports`](../../skills/common/databricks-python-imports/SKILL.md), [`databricks-asset-bundles`](../../skills/common/databricks-asset-bundles/SKILL.md), [`schema-management-patterns`](../../skills/common/schema-management-patterns/SKILL.md), [`unity-catalog-constraints`](../../skills/common/unity-catalog-constraints/SKILL.md)

---

## The Agent's Journey Through the Silver Orchestrator

### Step 0: Skill Activation (~100 tokens)

When a user says something like *"Create the Silver layer for my project"*, the agent framework matches:

```yaml
name: silver-layer-setup
description: >
  End-to-end orchestrator for creating Silver layer pipelines using Spark Declarative Pipelines
  (SDP, formerly DLT) with Delta table-based data quality rules, quarantine patterns, and monitoring views...
```

Keywords "Silver layer", "SDP", "DLT", "data quality rules" match the user's intent. The agent reads the full SKILL.md (~580 lines).

### Step 1: The Decision Tree â€” Should I Even Be Here?

| Question | Action |
|----------|--------|
| Creating a Silver layer from scratch? | **Use this skill** â€” it orchestrates everything |
| Only need DLT expectations patterns? | Read `silver/01-dlt-expectations-patterns/SKILL.md` directly |
| Need advanced DQX validation? | Read `silver/02-dqx-patterns/SKILL.md` directly |
| Need Asset Bundle configuration? | Read `common/databricks-asset-bundles/SKILL.md` directly |
| Need table properties reference? | Read `common/databricks-table-properties/SKILL.md` directly |

### Step 2: The Guard Rails Lock In

The Silver orchestrator has 6 non-negotiable defaults that apply to every table and pipeline:

| Default | Value | NEVER Do This Instead |
|---------|-------|-----------------------|
| **Serverless** | `serverless: true` | Never set `serverless: false` or define `clusters:` |
| **Auto Liquid Clustering** | `cluster_by_auto=True` | Never use `cluster_by=["col1", "col2"]` |
| **Edition** | `edition: ADVANCED` | Never use CORE or PRO (expectations require ADVANCED) |
| **Photon** | `photon: true` | Never set `photon: false` |
| **Row Tracking** | `delta.enableRowTracking: "true"` | Never omit (breaks downstream MV refresh) |
| **Change Data Feed** | `delta.enableChangeDataFeed: "true"` | Never omit (required for incremental propagation) |

Plus two API constraints:
- **Always use `import dlt`** (legacy API) â€” NOT `from pyspark import pipelines as dp`
- **`dq_rules_loader.py` must be pure Python** â€” NO `# Databricks notebook source` header

### Step 3: The Core Philosophy â€” Schema Cloning

The orchestrator establishes a key design philosophy that the agent must follow throughout:

```
Silver layer = Bronze schema clone + data quality rules

âœ… Same column names as Bronze
âœ… Same data types (minimal conversions)
âœ… Same grain (no aggregation)
âœ… Add: DQ rules, derived flags, business keys, processed_timestamp

âŒ No major schema restructuring
âŒ No aggregations (that's for Gold)
âŒ No complex business logic
âŒ No cross-table joins
```

### Step 4: The Progressive Disclosure Protocol

The Silver orchestrator manages 2 worker skills, 6 common skills, and 3 reference files across 7 phases:

```
Read skills ONLY at the phase where they are needed:
  Phase 1: Read expert-agent + schema-management â†’ work â†’ persist notes
  Phase 2: Read dlt-expectations + table-properties + unity-catalog â†’ work â†’ persist notes
  Phase 3: Read python-imports + dlt-expectations â†’ work â†’ persist notes
  Phase 4: Read table-properties + dlt-expectations â†’ work â†’ persist notes
  Phase 5: No skills needed â€” use reference files
  Phase 6: Read asset-bundles â†’ work â†’ persist notes
  Phase 7 (user-triggered): Read anomaly-detection â†’ work â†’ done
```

**At each phase boundary, the agent's working memory should contain ONLY:**
1. Table list from Phase 1 (persists through all phases)
2. Previous phase's summary note
3. Current phase's skills (read just-in-time)
4. The constraint: `dq_rules_loader.py` must be pure Python (carry through ALL phases)

---

## Phase 1: Requirements & Schema Setup

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PHASE 1                             â”‚
â”‚                                                          â”‚
â”‚  Working Memory:                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ expert-agent SKILL.md        â”‚ â† READ now            â”‚
â”‚  â”‚ schema-management SKILL.md   â”‚ â† READ now            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                          â”‚
â”‚  Steps:                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ 1. Map Bronze tables â†’ Silver tables    â”‚            â”‚
â”‚  â”‚    bronze_transactions â†’ silver_trans.   â”‚            â”‚
â”‚  â”‚    bronze_products â†’ silver_products     â”‚            â”‚
â”‚  â”‚    bronze_stores â†’ silver_stores         â”‚            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚ 2. Define DQ rules per entity            â”‚            â”‚
â”‚  â”‚    â”œâ”€â”€ Critical: NOT NULL on PKs, FKs   â”‚            â”‚
â”‚  â”‚    â”œâ”€â”€ Warning: Range checks, format     â”‚            â”‚
â”‚  â”‚    â””â”€â”€ Quarantine: high-volume facts     â”‚            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚ 3. CREATE SCHEMA IF NOT EXISTS           â”‚            â”‚
â”‚  â”‚    (from schema-management-patterns)     â”‚            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚ 4. Verify Bronze tables exist            â”‚            â”‚
â”‚  â”‚    Extract schemas â€” don't hardcode!     â”‚            â”‚
â”‚  â”‚    (from expert-agent extraction rule)   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                          â”‚
â”‚  ğŸ“ Persist: schema names, table list, DQ strategy       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 2: DQ Rules Table Setup

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PHASE 2                             â”‚
â”‚                                                          â”‚
â”‚  Working Memory:                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ Phase 1 notes (table list)   â”‚ â† compact handoff     â”‚
â”‚  â”‚ 01-dlt-expectations SKILL.md â”‚ â† READ now (worker)   â”‚
â”‚  â”‚ table-properties SKILL.md    â”‚ â† READ now (common)   â”‚
â”‚  â”‚ unity-catalog-constraints    â”‚ â† READ now (common)   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                          â”‚
â”‚  Creates the centralized DQ rules engine:               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚  setup_dq_rules_table.py                â”‚            â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚            â”‚
â”‚  â”‚  â”‚ CREATE TABLE IF NOT EXISTS   â”‚       â”‚            â”‚
â”‚  â”‚  â”‚   dq_rules (                 â”‚       â”‚            â”‚
â”‚  â”‚  â”‚     table_name STRING,       â”‚       â”‚            â”‚
â”‚  â”‚  â”‚     rule_name STRING,        â”‚       â”‚            â”‚
â”‚  â”‚  â”‚     rule_expression STRING,  â”‚       â”‚            â”‚
â”‚  â”‚  â”‚     severity STRING,         â”‚       â”‚            â”‚
â”‚  â”‚  â”‚     ...                      â”‚       â”‚            â”‚
â”‚  â”‚  â”‚   )                          â”‚       â”‚            â”‚
â”‚  â”‚  â”‚   TBLPROPERTIES (...)        â”‚ â† fromâ”‚            â”‚
â”‚  â”‚  â”‚   â† table-properties skill   â”‚       â”‚            â”‚
â”‚  â”‚  â”‚                              â”‚       â”‚            â”‚
â”‚  â”‚  â”‚ ALTER TABLE ADD CONSTRAINT   â”‚       â”‚            â”‚
â”‚  â”‚  â”‚   pk_dq_rules PRIMARY KEY   â”‚ â† fromâ”‚            â”‚
â”‚  â”‚  â”‚   (table_name, rule_name)    â”‚       â”‚            â”‚
â”‚  â”‚  â”‚   NOT ENFORCED               â”‚ unity â”‚            â”‚
â”‚  â”‚  â”‚                              â”‚catalogâ”‚            â”‚
â”‚  â”‚  â”‚ INSERT rules for each table  â”‚       â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                          â”‚
â”‚  ğŸ“ Persist: DQ table path, rule count, severity dist.   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 3: Rules Loader Module

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PHASE 3                             â”‚
â”‚                                                          â”‚
â”‚  Working Memory:                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ Phase 2 notes (DQ table)     â”‚                       â”‚
â”‚  â”‚ python-imports SKILL.md      â”‚ â† READ now            â”‚
â”‚  â”‚ dlt-expectations SKILL.md    â”‚ â† still from Phase 2  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                          â”‚
â”‚  âš ï¸ CRITICAL: This file must be PURE PYTHON             â”‚
â”‚     NO "# Databricks notebook source" header!            â”‚
â”‚                                                          â”‚
â”‚  Creates: dq_rules_loader.py                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚  # NO notebook header! (pure Python)    â”‚            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚  _cache = {}  # Module-level cache       â”‚            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚  def _load_rules(table_name):            â”‚            â”‚
â”‚  â”‚      if table_name not in _cache:        â”‚            â”‚
â”‚  â”‚          df = spark.table("dq_rules")    â”‚            â”‚
â”‚  â”‚          _cache[table_name] =            â”‚            â”‚
â”‚  â”‚              df.filter(...).toPandas()   â”‚            â”‚
â”‚  â”‚                  â† toPandas, NOT collect â”‚            â”‚
â”‚  â”‚      return _cache[table_name]           â”‚            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚  def get_critical_rules_for_table(name): â”‚            â”‚
â”‚  â”‚      ...                                 â”‚            â”‚
â”‚  â”‚  def get_warning_rules_for_table(name):  â”‚            â”‚
â”‚  â”‚      ...                                 â”‚            â”‚
â”‚  â”‚  def get_quarantine_condition(name):     â”‚            â”‚
â”‚  â”‚      ...                                 â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                          â”‚
â”‚  ğŸ“ Persist: loader path, confirm pure Python            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 4: DLT Notebooks â€” The Main Build

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PHASE 4                             â”‚
â”‚                                                          â”‚
â”‚  Working Memory:                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ Phase 1 notes (table list)   â”‚ â† table names         â”‚
â”‚  â”‚ Phase 3 notes (loader path)  â”‚                       â”‚
â”‚  â”‚ table-properties SKILL.md    â”‚ â† READ now            â”‚
â”‚  â”‚ dlt-expectations SKILL.md    â”‚ â† READ now            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                          â”‚
â”‚  For each Silver table:                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚  import dlt                   â† ALWAYS   â”‚            â”‚
â”‚  â”‚  from dq_rules_loader import  ...        â”‚            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚  @dlt.table(                             â”‚            â”‚
â”‚  â”‚    name="silver_transactions",           â”‚            â”‚
â”‚  â”‚    table_properties={                    â”‚            â”‚
â”‚  â”‚      "delta.enableChangeDataFeed": "true"â”‚ ğŸ”´        â”‚
â”‚  â”‚      "delta.enableRowTracking": "true",  â”‚ ğŸ”´        â”‚
â”‚  â”‚      ...from table-properties skill      â”‚            â”‚
â”‚  â”‚    },                                    â”‚            â”‚
â”‚  â”‚    cluster_by_auto=True        ğŸ”´        â”‚            â”‚
â”‚  â”‚  )                                       â”‚            â”‚
â”‚  â”‚  @dlt.expect_all_or_drop(                â”‚            â”‚
â”‚  â”‚    get_critical_rules_for_table(...)      â”‚            â”‚
â”‚  â”‚  )                                       â”‚            â”‚
â”‚  â”‚  @dlt.expect_all(                        â”‚            â”‚
â”‚  â”‚    get_warning_rules_for_table(...)       â”‚            â”‚
â”‚  â”‚  )                                       â”‚            â”‚
â”‚  â”‚  def silver_transactions():              â”‚            â”‚
â”‚  â”‚    return dlt.read_stream(               â”‚            â”‚
â”‚  â”‚      get_bronze_table("bronze_trans...")  â”‚            â”‚
â”‚  â”‚    )                                     â”‚            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚  For high-volume facts: quarantine table â”‚            â”‚
â”‚  â”‚  using get_quarantine_condition()         â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                          â”‚
â”‚  ğŸ“ Persist: notebook paths, expectation counts          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 5: Monitoring Views

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PHASE 5                             â”‚
â”‚                                                          â”‚
â”‚  Working Memory:                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ Phase 4 notes (notebooks)    â”‚                       â”‚
â”‚  â”‚ references/monitoring-        â”‚ â† reference file     â”‚
â”‚  â”‚   patterns.md                 â”‚   (not a skill)      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚  (No new worker skills â€” reference files only)           â”‚
â”‚                                                          â”‚
â”‚  Creates: data_quality_monitoring.py                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Per-table DQ metrics views              â”‚            â”‚
â”‚  â”‚ â”œâ”€â”€ Record counts (total, pass, fail)   â”‚            â”‚
â”‚  â”‚ â”œâ”€â”€ Pass/fail rates by rule             â”‚            â”‚
â”‚  â”‚ â””â”€â”€ Trend over time                     â”‚            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚ Referential integrity checks            â”‚            â”‚
â”‚  â”‚ â”œâ”€â”€ Orphaned records detection          â”‚            â”‚
â”‚  â”‚ â””â”€â”€ Cross-table FK validation           â”‚            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚ Data freshness monitoring               â”‚            â”‚
â”‚  â”‚ â””â”€â”€ Max processed_timestamp per table   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                          â”‚
â”‚  ğŸ“ Persist: monitoring view paths, metric definitions   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 6: Pipeline & Job Configuration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PHASE 6                             â”‚
â”‚                                                          â”‚
â”‚  Working Memory:                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ Phase 4-5 notes (all paths)  â”‚                       â”‚
â”‚  â”‚ asset-bundles SKILL.md       â”‚ â† READ now            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                          â”‚
â”‚  Creates two YAML configurations:                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚  silver_dlt_pipeline.yml                â”‚            â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚            â”‚
â”‚  â”‚  â”‚ serverless: true        ğŸ”´       â”‚   â”‚            â”‚
â”‚  â”‚  â”‚ photon: true            ğŸ”´       â”‚   â”‚            â”‚
â”‚  â”‚  â”‚ edition: ADVANCED       ğŸ”´       â”‚   â”‚            â”‚
â”‚  â”‚  â”‚ catalog: ${var.catalog}          â”‚   â”‚            â”‚
â”‚  â”‚  â”‚ schema: ${var.silver_schema}     â”‚   â”‚            â”‚
â”‚  â”‚  â”‚ libraries:                       â”‚   â”‚            â”‚
â”‚  â”‚  â”‚   - notebook: silver_dims.py     â”‚   â”‚            â”‚
â”‚  â”‚  â”‚   - notebook: silver_trans.py    â”‚   â”‚            â”‚
â”‚  â”‚  â”‚   - notebook: monitoring.py      â”‚   â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚            â”‚
â”‚  â”‚                                          â”‚            â”‚
â”‚  â”‚  silver_dq_setup_job.yml                â”‚            â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚            â”‚
â”‚  â”‚  â”‚ notebook_task:                   â”‚   â”‚            â”‚
â”‚  â”‚  â”‚   notebook_path: setup_dq_rules  â”‚   â”‚            â”‚
â”‚  â”‚  â”‚   base_parameters:               â”‚   â”‚            â”‚
â”‚  â”‚  â”‚     catalog: ${var.catalog}      â”‚   â”‚            â”‚
â”‚  â”‚  â”‚     silver_schema: ${var...}     â”‚   â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                          â”‚
â”‚  ğŸ“ Persist: pipeline YAML path, job YAML path           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## STOP â€” Artifact Creation Complete

After Phase 6, all files have been created. The agent stops and reports what was built:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ğŸ›‘ STOP GATE                           â”‚
â”‚                                                          â”‚
â”‚  Created files:                                          â”‚
â”‚  â”œâ”€â”€ src/{project}_silver/                              â”‚
â”‚  â”‚   â”œâ”€â”€ setup_dq_rules_table.py    (Phase 2)          â”‚
â”‚  â”‚   â”œâ”€â”€ dq_rules_loader.py         (Phase 3, PURE PY) â”‚
â”‚  â”‚   â”œâ”€â”€ silver_dimensions.py       (Phase 4)          â”‚
â”‚  â”‚   â”œâ”€â”€ silver_transactions.py     (Phase 4)          â”‚
â”‚  â”‚   â””â”€â”€ data_quality_monitoring.py (Phase 5)          â”‚
â”‚  â””â”€â”€ resources/                                         â”‚
â”‚      â”œâ”€â”€ silver_dlt_pipeline.yml    (Phase 6)          â”‚
â”‚      â””â”€â”€ silver_dq_setup_job.yml    (Phase 6)          â”‚
â”‚                                                          â”‚
â”‚  âš ï¸ Do NOT deploy unless user explicitly requests it    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Deployment Order (User-Triggered Only)

When the user says "deploy", the order is critical â€” DQ rules table must exist BEFORE the DLT pipeline runs:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DEPLOYMENT                             â”‚
â”‚                                                          â”‚
â”‚  $ databricks bundle deploy -t dev                      â”‚
â”‚                                                          â”‚
â”‚  Step 1: DQ rules setup FIRST                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ silver_dq_setup_job  â”‚ â† Creates dq_rules table     â”‚
â”‚  â”‚ (notebook_task)      â”‚    and populates rules        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚             â”‚ MUST complete before                       â”‚
â”‚             â–¼                                            â”‚
â”‚  Step 2: DLT pipeline SECOND                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ Silver DLT Pipeline  â”‚ â† Reads rules from table     â”‚
â”‚  â”‚ (serverless SDP)     â”‚    via dq_rules_loader.py     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                          â”‚
â”‚  âŒ WRONG ORDER: Pipeline before DQ setup                â”‚
â”‚     â†’ "Table or view not found: dq_rules"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Phase 7 (User-Triggered): Anomaly Detection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PHASE 7                             â”‚
â”‚                                                          â”‚
â”‚  âš ï¸ Only when user explicitly requests it               â”‚
â”‚                                                          â”‚
â”‚  Read: monitoring/04-anomaly-detection/SKILL.md          â”‚
â”‚                                                          â”‚
â”‚  Enable schema-level anomaly detection:                 â”‚
â”‚  â”œâ”€â”€ Freshness monitoring (stale table alerts)          â”‚
â”‚  â”œâ”€â”€ Completeness monitoring (missing data alerts)      â”‚
â”‚  â”œâ”€â”€ Exclude metadata tables (dq_rules)                 â”‚
â”‚  â””â”€â”€ Non-blocking: if fails, Silver still works         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## The Complete Flow â€” Context Budget Over Time

```
Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶

Phase:  â”‚   1   â”‚   2   â”‚  3  â”‚     4      â”‚  5  â”‚  6  â”‚  7  â”‚

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
table   â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
list    â”‚       â”‚       â”‚     â”‚            â”‚     â”‚     â”‚     â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
expert  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚       â”‚     â”‚            â”‚     â”‚     â”‚     â”‚
agent   â”‚       â”‚       â”‚     â”‚            â”‚     â”‚     â”‚     â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
schema  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚       â”‚     â”‚            â”‚     â”‚     â”‚     â”‚
mgmt    â”‚       â”‚disc.  â”‚     â”‚            â”‚     â”‚     â”‚     â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
01-dlt  â”‚       â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚     â”‚     â”‚
expect  â”‚       â”‚       â”‚     â”‚  (re-read) â”‚disc.â”‚     â”‚     â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
table   â”‚       â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚     â”‚     â”‚
props   â”‚       â”‚       â”‚     â”‚  (re-read) â”‚disc.â”‚     â”‚     â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
unity   â”‚       â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚            â”‚     â”‚     â”‚     â”‚
catalog â”‚       â”‚       â”‚disc.â”‚            â”‚     â”‚     â”‚     â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
python  â”‚       â”‚       â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚            â”‚     â”‚     â”‚     â”‚
imports â”‚       â”‚       â”‚     â”‚discarded   â”‚     â”‚     â”‚     â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
asset   â”‚       â”‚       â”‚     â”‚            â”‚     â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚
bundles â”‚       â”‚       â”‚     â”‚            â”‚     â”‚     â”‚disc.â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
anomaly â”‚       â”‚       â”‚     â”‚            â”‚     â”‚     â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
detect  â”‚       â”‚       â”‚     â”‚            â”‚     â”‚     â”‚ opt â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

 â–ˆâ–ˆâ–ˆ = skill loaded    disc. = discarded
```

The Silver orchestrator has a flatter context curve than Gold Design or Semantic Layer â€” most phases load 1-2 common skills plus possibly the `dlt-expectations` worker. The peak is Phase 4 where DLT notebooks are created using patterns from both `dlt-expectations` and `table-properties`.

---

## The Skill Dependency Graph

Unlike the semantic layer's linear chain, the Silver orchestrator's dependencies form a tree where multiple phases re-read the same skills:

```
                    00-silver-layer-setup (orchestrator)
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           â”‚                           â”‚
  Worker Skills   Common Skills            References
        â”‚           â”‚                           â”‚
  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”    â”œâ”€â”€ expert-agent (Ph 1)     â”œâ”€â”€ silver-table-patterns
  â”‚           â”‚    â”œâ”€â”€ schema-mgmt  (Ph 1)     â”œâ”€â”€ monitoring-patterns
  â”‚ 01-dlt-   â”‚    â”œâ”€â”€ table-props  (Ph 2,4)   â””â”€â”€ pipeline-configuration
  â”‚ expect.   â”‚    â”œâ”€â”€ unity-cat.   (Ph 2)
  â”‚ (Ph 2-4)  â”‚    â”œâ”€â”€ python-imp.  (Ph 3)
  â”‚           â”‚    â”œâ”€â”€ asset-bun.   (Ph 6)
  â”‚ 02-dqx   â”‚    â””â”€â”€ autonomous-ops (Ph 4+)
  â”‚ (optional)â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Re-read pattern: dlt-expectations is used in Phases 2, 3, and 4
  (DQ table DDL â†’ loader cache pattern â†’ decorator patterns)
```

---

## Post-Completion: The Audit Trail

| # | Phase | Skill / Reference Read | Type | What It Was Used For |
|---|-------|----------------------|------|---------------------|
| 1 | Phase 1 | `common/databricks-expert-agent/SKILL.md` | Common | Extraction principle for Bronze schema names |
| 2 | Phase 1 | `common/schema-management-patterns/SKILL.md` | Common | CREATE SCHEMA DDL with governance metadata |
| 3 | Phase 2 | `silver/01-dlt-expectations-patterns/SKILL.md` | Worker | DQ rules table DDL, rule population |
| 4 | Phase 2 | `common/databricks-table-properties/SKILL.md` | Common | Metadata table TBLPROPERTIES |
| 5 | Phase 2 | `common/unity-catalog-constraints/SKILL.md` | Common | PK constraint on dq_rules table |
| 6 | Phase 3 | `common/databricks-python-imports/SKILL.md` | Common | Pure Python loader (no notebook header) |
| 7 | Phase 4 | `silver/01-dlt-expectations-patterns/SKILL.md` | Worker | DLT decorator patterns (re-read) |
| 8 | Phase 4 | `common/databricks-table-properties/SKILL.md` | Common | Silver TBLPROPERTIES (re-read) |
| 9 | Phase 6 | `common/databricks-asset-bundles/SKILL.md` | Common | Pipeline YAML, job YAML, serverless config |
| ... | ... | ... | ... | ... |

---

## Key Design Principles at Work

| # | Principle | How It's Applied |
|---|-----------|-----------------|
| 1 | **Schema cloning** | Silver mirrors Bronze column names/types. No restructuring. DQ rules are the value-add. |
| 2 | **DQ rules as data** | Rules stored in a Delta table, not hardcoded in notebooks. Updatable at runtime without redeploying pipelines. |
| 3 | **Deployment ordering** | DQ setup job must run BEFORE DLT pipeline. The orchestrator enforces this with a STOP gate and explicit deployment instructions. |
| 4 | **Pure Python loader** | `dq_rules_loader.py` has no notebook header â€” this enables `import` from DLT notebooks without `%run` or `sys.path` hacks. |
| 5 | **Legacy API by design** | `import dlt` is used because `@dlt.expect_all_or_drop()` decorators are not yet available in the modern `dp` API. |
| 6 | **Serverless everything** | Pipeline: `serverless: true`. Jobs: `environments` block. No cluster definitions anywhere. |
| 7 | **Row tracking for downstream** | Every Silver table has `delta.enableRowTracking: "true"` â€” without it, Gold materialized views cannot do incremental refresh. |

---

## Common Failure Modes

| Error | Root Cause | Fix |
|-------|-----------|-----|
| `Table or view not found: dq_rules` | DLT pipeline ran before DQ setup job | Run `silver_dq_setup_job` first |
| `ModuleNotFoundError: dq_rules_loader` | Loader file has notebook header | Remove `# Databricks notebook source` line |
| `Incompatible schema change` | Streaming table schema evolved | Trigger full refresh: `--full-refresh` |
| Aggregation in Silver notebook | Business logic belongs in Gold | Remove groupBy/agg, keep transaction grain |
| `cluster_by=["col1"]` in DLT table | Manual clustering instead of auto | Replace with `cluster_by_auto=True` |
| `from pyspark import pipelines as dp` | Modern API incompatible with DQ framework | Use `import dlt` |
| Missing row tracking | Downstream MVs do expensive full recompute | Add `delta.enableRowTracking: "true"` |

---

## What Happens Next

After the Silver layer is complete and deployed:

```
Silver Setup (this skill)
    â”‚
    â–¼
Gold Implementation (01-gold-layer-setup)
    â†’ setup_tables.py creates Gold tables from YAML
    â†’ merge_gold_tables.py reads Silver tables
    â†’ FK constraints reference Silver-populated dimensions
```

The Silver tables created here become the direct input for the Gold merge scripts. Column names, types, and data quality guarantees established in this orchestrator carry forward as the contract that Gold implementation depends on.
